{"pages":[{"title":"关于我","text":"一个攻城狮的小小笔记哎，年纪大了脑子不好使， 就拿个Github项目作为免费的记录笔记的地方吧，笔记主要相关的内容包括如下几点， Mysql, Linux, Java, Golang我所有的笔记都会保存在这个项目里面，如发现笔记中有误， 欢迎来我的Github repo提交issue, 如有其他问题也欢迎与我沟通讨论， 我的邮箱为： winjeg@qq.com Linux golang mysql 容器化相关笔记 其他 bigtable阅读笔记 rocksdb 参数优化 redis 集群搭建 zookeeper 集群搭建 etcd集群搭建 本repo所有内容未经允许一律不准转载、抄袭、售卖等, 侵权必究","link":"/about/index.html"}],"posts":[{"title":"使用Hexo创建博客","text":"在操作之前， 安装好了相关的软件如： node 拥有自己的github账号正确设置好SSH KEY 创建一个 username.github.io 的repo 并开启相应的page设置 安装步骤12git clone git@github.com:your_name/your_name.github.io.gitgit checkout -b hexo 安装 hexo12npm install -g hexo-clihexo init 由于Hexo要求必须在空文件夹中执行init操作，所以我们需要在博客文件夹以外的地方新建一个空文件夹，之后点击鼠标右键选择Git bash Here输入以下命令，并将命令执行完成后文件夹中的所有文件复制到your_name.github.io文件夹中 1npm install 本地预览(可省略) 12hexo generatehexo server 远程部署我们已经在本地成功建站，接下来我们要做的就是通过简单的修改配置文件使得Hexo为我们生成的静态页面能够部署到Github Pages上面。 编辑username.github.io文件夹下面的_config.yml（Hexo官方文档中将其称为全局配置文件），找到deploy关键字，将其修改为 1234deploy: type: git repo: git@github.com:your_name/your_name.github.io.git branch: master 为了将完成到Github的远程部署，我们还需要安装一个插件。 1npm install hexo-deployer-git --save 执行以下命令，完成静态页面的远程部署与博客源文件的备份 1234git add .git commit -m \"提交说明\"git push origin hexohexo generate -d 主题设置请自行搜索github, 输入关键字 hexo theme选择自己喜爱的主题，并按照相关文档进行设置 对于其他设备上写博客12345git clone your_repocd your_reponpm install -g hexo-clinpm installnpm install hexo-deployer-git --save 自动脚本编写在源文件分支(hexo分支)上添加如下文件, 如果是windows 命名为publish.cmd 如果是linux 或者mac命名为publish 并加入可执行权限chmod a+x publish 1234git add .git commit -m \"add article\"git push origin hexohexo generate -d 编写完毕，运行脚本 windows, publish 其他 ./publish","link":"/2019/06/13/setup_blog/"},{"title":"ssh 周边知识","text":"SSH 密钥生成与转换 确保你有公钥私钥对，确保安装了ssh如果没有可以用下面的命令生成 1ssh-keygen 由私钥生成公钥 1ssh-keygen -y -f id_rsa &gt; id_rsa.pub open ssh与 windows ppk相互转换关键工具 puttygen.exe ssh免密码登录 要把自己的公钥添加至目标机的1.ssh/authorized_keys 文件中去，authorized_keys 的权限是 600 ssh-copy-id + host会自动加免密到目标机器如1ssh-copy-id user@host ssh 连接管理ssh 自动补全工具 自动补全是：ssh 连接 linux 自动补全需要bash_completion 还需要在 ~/.ssh/config 文件中记录 三个字段如下 123Host alias-of-the-hostUser usernameIp 10.1.1.2 ssh远程管理工具 在 .ssh/config 中配置的各种服务 一些terminal自带的一些ssh管理工具， 如deeepin的terminal remmina 远程管理工具","link":"/2013/09/13/apps/ssh/"},{"title":"多媒体相关的一些知识","text":"BasicsvideosFFmpeg audiosopenshotffmpeg","link":"/2013/09/13/apps/mutimedia/"},{"title":"版本管理工具Git的使用","text":"什么是GitGit 是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理，是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件， Git也是计算机界最重要的软件之一， 被广泛的适用于各大中小公司的各类项目中。 小插曲: 为啥我们不提其他的版本控制系统， 因为对于一般的情况下， 有Git就足够了，它至少能满足99 %的人的需求。 Git 配置SSH配置 生成 RSA 秘钥对， 私钥自己保存， 公钥需要给1ssh-keygen -t rsa -b 4096 -C \"winjeg@qq.com\" 生成过程中使用的密码会更安全一些， 但设置就会更麻烦一些， 关于相关的设置， 希望大家“不厌其烦”， 去搜索引擎自己搜索就好。生成结果一般有两个文件： id_rsa 这是一个绝密的文件， 只有使用者自己知道， 其他人不能知道 id_rsa.pub 这个是一个公开的文件， 是发给外界用来安全通信的一个工具对于Github或者Gitlab而言，均有地方添加 public key， 一般在 用户settings 菜单下 生成完毕之后， 把私钥放到相应的位置： linux/mac ~/.ssh 并设置id_rsa的权限 chmod 600 ~/.ssh/id_rsa windows 用户直接把 id_rsa 放到 用户目录下的 .ssh 文件夹中即可 安装Git windows 下安装由于安装Git 比较简单，只需要去官方网站， 去下载并且按照默认步骤安装即可。因此，此处不做更多详细的介绍。 如果想用gpg签名则比较复杂， 但注意一点， 如果出现 key not avalible 类似的， 尝试设置下gpg的位置 1git config --global gpg.program \"C:\\Program Files (x86)\\GnuPG\\bin\\gpg.exe\" Linux 下安装 1234sudo apt-get install git # debian basedsudo yum install git # redhat basedsudo pacman -S git # archlinux basedsudo emerge git # gentoo based mac 下安装我猜是： 1brew install git Git的基础使用新建Git 项目克隆代码1git clone https://github.com/winjeg/demos-go 新建本地项目，并关联到远程123456git init repo_name # 创建 repo_name 的文件夹， 并创建好相关的 .git 隐藏文件夹等cd repo_name git remote add origin git@github.com:winjeg/repo.git # 设置远端地址(这个关系到推送的地址)git add . # 把当前的项目文件都暂存git commit -m \"Initial commit\" # 把暂存的文件作为一次 commit 提交git push -u origin master # 把commit push 到远程的master分支 经过以上步骤， 一个本地可以用的repo就建立好啦 拉取远端123456# 拉取指定分支的变化git fetch origin master # 拉取所有分支的变化git fetch # 拉取所有分支的变化，并且将远端不存在的分支同步移除【推荐】git fetch -p 查看当前状态1git status 对于当前repo， 增加， 删除，修改等的状态都会被列出来 1234567891011121314HEAD detached from fd07db2Changes not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) deleted: rops.yaml modified: values.yamlUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) a.mdno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 暂存暂存文件是commit这些变更的前提 12345git add file_name # 暂存某文件git add . # 暂存所有变更git reset # 取消暂存git reset --hard # 取消本地所有未提交的更改git checkout file # 取消某文件的更改 提交1git commit -m \"the commit message\" 提交时候，结果仍在本地，但已经属于创建了本地的一个变更集 推送1git push 推送代码到远端 分支管理新建分支12git checkout -b new_branch # 新建git checkout new_branch # 切换到新建的分支 删除分支1234# 删除本地分支，如果本地还有未合并的代码，则不能删除git branch -d qixiu/feature# 强制删除本地分支git branch -D qixiu/feature 推送新建的分支到远端1git push origin new_branch 设置本地分支与远程同步1git branch --set-upstream-to=origin/&lt;branch&gt; hexo 删除远程分支12# 等同于git push origin -d qixiu/feauregit push origin :qixiu/feature 切换分支 切换到本地分支 1git checkout branch_name 切换到远程分支 12git branch -b branch_name origin/branch_name # 设置本地分支与远程分支同步git pull # 拉取远程分支代码 合并分支 无冲突合并 1git merge branch_name 有冲突合并 12git merge branch_namegit mergetool mergetool 的设置， 下面以Kdiff为例， 当然merge工具也有非常多， 如 meld， beyond compare 等。其设置方法都雷同，因此此处仅仅只举出一个例子作为说明。 12git config --global merge.tool kdiff3git config mergetool.kdiff3.path \"/usr/bin/kdiff3\" 设置好mergetool之后，以后有merge冲突的时候， kdiff3 会自动跳出并让你人工merge。 Rebase1234git rebase -i HEAD~4 # 合并提交记录git:(feature1) git rebase master # rebase 到master分支git rebase --continue # 继续rebasegit rebase —abort # 放弃rebase回到原始状态 在合并提交记录的时候会有如下信息打印出来 12345678910111213141516171819202122s cacc52da add: qrcodes f072ef48 update: indexeddb hacks 4e84901a feat: add indexedDB floders 8f33126c feat: add test2.js# Rebase 5f2452b2..8f33126c onto 5f2452b2 (4 commands)## Commands:# p, pick = use commit# r, reword = use commit, but edit the commit message# e, edit = use commit, but stop for amending# s, squash = use commit, but meld into previous commit# f, fixup = like \"squash\", but discard this commit's log message# x, exec = run command (the rest of the line) using shell# d, drop = remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.# 选择其中一种作为合并的方式， 上述是在vim（也可以是其他设置的编辑器）的一个窗口中展示的。如果你异常退出了 vim 窗口，不要紧张： 1git rebase --edit-todo 这时候会一直处在这个编辑的模式里，我们可以回去继续编辑，修改完保存一下： 1git rebase --continue 与其他版本管理工具类似， 下图比较形象的展示了git中 rebase与merge的区别 Git命令别名git是一个比较开放的系统， 与bash类似， git可以自定义别名来取代冗长的命令行输入如可以设置 git st 代替 git status， 使用 git l代替 git log 等等， 这些都被定义在git的配置文件中(~/.gitconfig)， 修改起来非常方便。 Git 的GPG签名设置(Windows)安装gpg4win 如果没有响相应的GPG的KEY， 利用这个工具生成相应的key与配置， 记得备份。如果是已有备份， 可以直接用这个工具导入，非常简单。 然而仅仅这样设置还是不够的， 你需要在Github/Gitlab上添加相应的 PGP PUBLIC KEY BLOCK提交的时候使用如下命令， 则会自动签名。 1234567$ git commit -S -m \"change readme\"ggpg: directory '/c/Users/winjeg/.gnupg' createdigpg: keybox '/c/Users/winjeg/.gnupg/pubring.kbx' createdgpg: skipped \"winjeg &lt;winjeg@qq.com&gt;\": No secret keygpg: signing failed: No secret keyerror: gpg failed to sign the datafatal: failed to write commit object 如上产生的错误则是由于Git默认的寻找签名证书的程序的路径有误。按照下面的方法进行设置。 1git config --global gpg.program \"C:\\Program Files (x86)\\GnuPG\\bin\\gpg.exe\" 设置完毕再次运行， 则可以看到成功签名commit 1234winjeg@gpc MINGW64 /d/projects/go/github.com/winjeg/cloudb (master)$ git commit -S -m \"change readme\"[master eca6b52] change readme 1 file changed, 3 insertions(+), 1 deletion(-) Git 高级用法Git对象接下来，新建一个空文件test.txt。 1touch test.txt 然后，把这个文件加入 Git 仓库，也就是为test.txt的当前内容创建一个副本。 12git hash-object -w test.txte69de29bb2d1d6434b8b29ae775ad8c2e48c5391 上面代码中，git hash-object命令把test.txt的当前内容压缩成二进制文件，存入 Git。压缩后的二进制文件，称为一个 Git 对象，保存在.git/objects目录。 这个命令还会计算当前内容的 SHA1 哈希值（长度40的字符串），作为该对象的文件名。 查看文件对象的内容 12git cat-file -p 3b18e512dba79e4c8300dd08aeb37f8e728b8dadhello world 暂存区 (git add)文件保存成二进制对象以后，还需要通知 Git 哪些文件发生了变动。所有变动的文件，Git 都记录在一个区域，叫做”暂存区”（英文叫做 index 或者 stage）。等到变动告一段落，再统一把暂存区里面的文件写入正式的版本历史。 1234git update-index --add --cacheinfo 100644 \\3b18e512dba79e4c8300dd08aeb37f8e728b8dad test.txtgit ls-files --stage100644 3b18e512dba79e4c8300dd08aeb37f8e728b8dad 0 test.txt Git 快照 （commit）暂存区保留本次变动的文件信息，等到修改了差不多了，就要把这些信息写入历史，这就相当于生成了当前项目的一个快照（snapshot）。 项目的历史就是由不同时点的快照构成。Git 可以将项目恢复到任意一个快照。快照在 Git 里面有一个专门名词，叫做 commit，生成快照又称为完成一次提交。 下文所有提到”快照”的地方，指的就是 commit。 Git分支Git分支其实是指向某个快照节点的指针， 对于Git来说， 分支的创建成本是极其低廉的。另外，Git 有一个特殊指针HEAD， 总是指向当前分支的最近一次快照。另外，Git 还提供简写方式，HEAD^指向 HEAD的前一个快照（父节点），HEAD~6则是HEAD之前的第6个快照。 本文将不对其他内容做过多介绍, 仅仅介绍到此为止","link":"/2019/06/13/apps/git/"},{"title":"Tidb 笔记","text":"简介TIDB是一种分布式高可用实现MySQL协议的可动态扩容的数据库动态扩容是指计算能力与存储空间两个维度的动态扩容tidb 由 tidb server， PDserver 与 TIKV组成基本结构如下图 TIDBTIDB 是一个无状态可以动态扩展的服务， 本身并不存储任何数据 主要功能 接收SQL请求 处理SQL相关的逻辑 与PD Server 通信定位数据实际存储位置 与TIKV实际交换数据 返回数据 PD serverPD Server 负责整个集群信息的管理与通信， 通常要部署奇数台 主要功能 存储集群数据元信息 管理与负载均衡TIKV， 并负责数据迁移 分配全局唯一单调递增ID TIKVTIKV 是一种分布式事务型Key-Value型存储， 它是负责整个TIDB集群所有的实际数据存储的。它使用Raft协议来保证集群数据的一致性与故障恢复，区域是它最基本的存储单元，所有不同节点上的区域副本构成了一个Raft的分组","link":"/2017/09/13/apps/tidb/"},{"title":"CICD 笔记","text":"CICD 笔录、 想法记录CICD是什么什么是CICD呢， 按我理解， CICD 应该做应用生命周期管理的一整套解决方案， 而非仅仅关注与持续集成和持续发布，CICD是应用生命周期的重要组成部分， 但它不是全部。更有一些组织会把CICD强行隔离， 部署和编译没有任何的整合， 我认为这是不科学的。按照个人理解，应用除了自己要关注的业务逻辑部分， 各其他点（中间件， 运维， 甚至中台）等都应该被CICD所关注，最终能达到的效果就是应用可以放心的去写应用， 大家能再这个生命周期里各司其职。虽然这样，我也不得不关注，当下意义上的CI和CD， 因为他们确实是前辈们抽象的最重要的两个概念了。 如果这两个概念没有， 其实这个体系也许就不存在，甚至是有另外一个体系会存在，现在应用交付的方式会发生质的变化。 CI - 持续集成应该是从代码写之前就开始关注， 从项目的创建，业务逻辑的编写， 到打成可部署的包，包的版本管理。我们知道很多公司不仅仅会有一种类型的项目结构，也不止一种的单测或者其他代码质量工具，甚至一些公司可能用多种语言进行开发。 做到通用的同时，又能做到可版本化、代码化、幂等化、简单化那就是一种艺术。 项目生成项目的生成主要是指项目的初始化， 项目初始化其实是非常重要的，它影响了我们以怎么样的模板去部署它。 让业务方去专注于业务， 就必须由基础设施来承担项目生成的职责。当然基础设施方生成项目，也能够使得项目更好的匹配我们的自动化集成与部署。Spring的 initializer 仅仅提供了一个生成非常通用的Java Spring Boot 项目的工具， 但这些还不够， 我们不仅仅需要把项目生成， 更要与公司的规范和基础设相结合， 保证生成的项目可以直接与基础设施无缝衔接， 这样才能赋能业务仅仅需要关注业务开发。 业务逻辑编写在项目生成的基础上，假设项目生成这一步， 我们已经选择了我们所需要的所有的基础设施， 所有的中间件与存储设施等等， 剩下的只有业务架构设计，以及业务逻辑编写了。我们甚至可以规定生成的项目结构， 让公司某一类项目都遵循同一个项目结构标准， 这对公司的快速迭代及基础设施都是有非常大的好处的。 打包 （Packaging）打包其实只是个步骤而已，它主要关注两件事情： 执行打包命令， 生成可以部署的文件 打包过程可复制化， 或者把打成的包存到历史包集里 不同的程序来讲， 打包命令是不一样的，如maven项目的常用打包命令是 mvn clean package, C/C++ 则更多的使用 make 来生成它所需要的文件。打包过程可复制化要求其实还是有一些的， 代版本跟可以用来发布的包关联 总是使用release 版本的依赖(这里我们假定这个release版本的依赖是不会改变的) 测试 SONAR 等代码规范检测 单元测试 性能测试 CI 应该具备的特点 任务清单化/模板化 CI节点无状态，任务随时来随时完成。 基础依赖镜像化， 固化。 打包过程可知化 包与代码对应，幂等性 gitlab的runner 和travis-ci， 等都是比较优秀的一些平台， 而jenkins 更像一个任务平台，而非部署平台。 CD - 持续发布版本控制与回滚一般版本控制会跟代码的版本控制去走， 建立映射关系， 也有一些版本控制会维护自己单独的生命线。无论哪一种无非是想具备出错迅速纠正的能力。 当然能与代码建立关系是最好的， 最好能够具备某个版本打出来的包不论打几次是等价的。 部署前准备部署前的准备主要则是把程序的依赖， 机器的初始化设置等都一一设置好。 顺序部署顺序部署主要为了平滑上线，而不至于服务中断， 也是必须的。 分批部署 健康监测 部署预热 部分部署部署一部分进行功能点验证， 这对业务逻辑有一定要求。 混合部署在容器化时期是不需要的， 因为大家都可以根据自己的资源需求量来安排资源。但在非容器化时期， 能混合部署会节省大量的IT成本。 弹性伸缩 手动弹性伸缩 自动弹性伸缩","link":"/2019/07/30/devops/cicd/"},{"title":"vsftpd的简介和使用","text":"VSFTPD - 一个强大的FTP服务软件 vsftpd是一个非常好用的ftp软件，它非常的安全高效， 可配置性特别高，在非Windows系统很受个人和企业的欢迎。当然本文主要关注的还是教大家如何使用它， 我们下面将从以下几个方面详细讲述vsftpd的安装与使用。 安装安装其实是非常简单的事情， 我见过的大部分主流发行版中， 都可以一句命令安装。 Ubuntu 系列1sudo apt-get install vsftp Fedora/Redhat 系列1sudo yum install vsftp Archlinux 系列1sudo pacman -S vsftp 启动服务1234# 开启 vsftpd 服务开机启动systemctl enable vsftpd.service# 开启 (修改配置完毕， 请使用 `restart` 来确保配置生效)systemctl start vsftpd.service 配置vsftp 配置对于喜欢GUI的人来讲可能不是那么友好， 但其实配置内容是非常易读的。 而且几乎所有配置项均有详细的注释。 配置文件位置：/etc/vsftpd.conf1vim /etc/vsftpd.conf 配置文件详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 是否监听IPV6端口，开启后可以用IPV6的方式访问ftp站点listen_ipv6=YES# 是否开启FTP目录消息dirmessage_enable=YES# 欢迎消息ftpd_banner=Welcome to blah FTP service.# 是否使用本地时间， 如不使用，默认为GMTuse_localtime=YES# 推荐使用一个独立的用户来运行ftp服务nopriv_user=ftpsecure# 是否允许递归列出ls_recurse_enable=YES# 是否允许本地用户切换到本地目录chroot_local_user=YESchroot_local_user=YESchroot_list_enable=YES# (default follows)chroot_list_file=/etc/vsftpd.chroot_list# SSL相关的配置rsa_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pemrsa_private_key_file=/etc/ssl/private/ssl-cert-snakeoil.keyssl_enable=NO# 禁止一些邮件地址deny_email_enable=YES# (default follows)banned_email_file=/etc/vsftpd.banned_emails# ascii 方式上传下载ascii_upload_enable=YESascii_download_enable=YESlocal_enable=YES# 是否允许任意形式的写命令write_enable=YES# 设置本地权限 （022 是Linux基础常识，不知道什么意思可以自行搜索）local_umask=022# 是否允许匿名用户上传， 只在全局写开启的情况下才有效，另外Linux中也要设置好相应路径的权限# 匿名访问相关anonymous_enable=NO# 是否允许本地用户登录anon_upload_enable=YES# 是否允许匿名用户创建目录anon_mkdir_write_enable=YES# 超时时间# 会话idle_session_timeout=600# 数据连接data_connection_timeout=120# 开启上传下载日志 (下一行为日志位置)xferlog_enable=YESxferlog_file=/var/log/vsftpd.logxferlog_std_format=YES# 使用20端口连接connect_from_port_20=YES# 是否把上传的文件更改所有者 （下一行为所有者）chown_uploads=YESchown_username=whoever 一些概念vsftpd的用户权限其实是跟Linux的权限是紧密相关的， chmod 等基础linux指令会直接影响ftp的读写权限等, 所以他会有很多有趣的应用场景： 老师收作业， 权限设置成只能写不能读，学生可以上传， 但下载不到其他学生的结果 下载站点， 只能读不能写， 防止内容被别人篡改 个人文件管理器， 可以多端访问， 读写权限都开放即可 希望大家读完此文能对Linux下的 vsftpd 有一个基本的认知， 知道它怎么配置， 怎么使用。","link":"/2013/11/13/apps/vsftpd/"},{"title":"docker 简易笔记","text":"基本概念三个基本概念： Image, Contrainer, Repository Image只读模板，预装一些东西 Container启动、开始、停止、删除 – 隔离启动时，在镜像上层创建一层可写层 Repository用来存放images Docker 命令1234567891011121314151617181920sudo # 启动docker服务systemctl start docker.service# 列出本地镜像docker images# 搜索镜像docker search +namedocker run -t -i centos /bin/bashdocker rm docker rmidocker psdocker save filenamedocker load --input filenamedocker logs 查看容器的输出信息docker ps 查看容器的状态信息 Docker file1234567891011121314151617181920# 以centos latest 作为基础FROM centos:latest# 维护者MAINTAINER wenwen# 执行命令RUN yum install vi#ADD /home/wenwen/1.txtEXPOSE portCMD nano#docker build -t &quot;&quot; {dir} docker-registry","link":"/2018/04/13/container/docker/"},{"title":"linux 下常用的应用软件","text":"ssh 相关ssh 是linux下最为广泛使用的远程连接工具， 也是每个linux学习者必备的技能ssh最常用的一些操作包括 ssh key转换、免密登录，连接管理 也是使用者最应该知悉的内容 免密码ssh登录的设置 1.确保你有公钥私钥对，确保安装了ssh如果没有可以用下面的命令生成ssh-keygen如果有私钥可以用下面的命令生成公钥ssh-keygen -y -f id_rsa &gt; id_rsa.pub 2.要把自己的公钥添加至目标机的.ssh/authorized_keys文件中去，authorized_keys的权限是600 open ssh与 windows ppk相互转换关键工具 puttygen由私钥生成公钥如上 ssh 连接 linux自动补全需要bash_completion还需要在 .ssh/config 文件中记录 host user ip三个字段另外，如果仅需要补全自己 ssh-copy-id 会自动加免密到目标机器 设置hostname123456789#!/bin/bash# setting up hostenv# @author winjeg@qq.com#### setting hostname nowhostname $1echo $1 &gt; /etc/hostnameecho -e &quot;127.0.0.1\\t$1$2\\t$1&quot; &gt;&gt; /etc/hosts openvpnopenvpn 添加 /etc/openvpn/client/xxxx.conf systemctl enable openvpn-client@jiaxing.service 设置免密在conf中auth-user-pass xxx.txt xxx.txt 中第一行用户名，第二行密码","link":"/2015/03/13/linux/apps/"},{"title":"Linux 快速入门","text":"Linux 学习之路Linux Logo Linux 的LOGO是一只可爱的小企鹅，选择这个Logo也是有一段奇闻轶事的，有兴趣的读者不妨自行去搜索引擎上了解。关于Linux本身的介绍也不必多说， 相信能看到这篇文章的读者肯定是知道一二。 写在前面相信大家读到这个文章的前提是大家对Linux是有一定兴趣爱好的， 能在这里与读者进行沟通我也是非常感激的。相遇即是缘分，无论是面对面，还是在字里行间。因为网上看到很多系列的教程都是教大家如何去系统的学习Linux的，但是文章长的话，很少人愿意花时间去看， 文章短的话，有很难带大家入门， 因此我写了这个系列的一些文章来帮助大家在1-5个小时之内完成Linux的入门。如果大家不能在这么短的时间入门，也不要来打我，觉得有写的不好的地方欢迎 email我 winjeg@qq.com, 并提出宝贵的建议。 本系列的文章会尽量少的介绍自己的价值观的同时会尽可能多的给出初学者最多的能省掉很多弯路的建议。 所以次系列的文章中我会注重列一些客观事实， 而所有的选择权都在读者， 文章全系手码，有些错误在所难免， 欢迎指正。 关于修改本系列文章欢迎任何形式的修改， 但修改完毕后请让笔者有机会使用大家的修改来完善此文， 大家可以通过发邮件的方式，来告知本文的修改， 本文的目的在于让更多的人更小代价的入门Linux，而非营利性目的。 关于使用与版权本系列的文章没有什么特殊的版权要求，如果大家不是很忌讳，请大家在使用本文的时候尽量注明来源，也能让笔者了解到笔者的文章确确实实的帮助到了什么人， 让笔者默默感动一下。","link":"/2014/01/10/linux/linux/"},{"title":"k8s 常用命令笔记","text":"k8s 常用命令12345678910111213141516171819202122232425262728293031323334353637383940kubectl versionkubectl cluster-infokubectl get nodeskubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080# 当完成run的时候就相当于部署了一个应用kubectl get deployments# expose 后相当于 servicekubectl expose deployment hello-node --type=LoadBalancerkubectl get serviceskubectl describe podskubectl get pods -o widekubectl logs $POD_NAMEkubectl exec -ti $POD_NAME bashkubectl exec $POD_NAME envkubectl describe services/kubernetes-bootcampexport NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template=&apos;{{(index .spec.ports 0).nodePort}}&apos;)echo NODE_PORT=$NODE_PORTkubectl scale deployments/kubernetes-bootcamp --replicas=2kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2# 看是否更新完kubectl rollout status deployments/kubernetes-bootcampkubectl rollout undo deployments/kubernetes-bootcampkubectl get eventskubectl config viewkubectl delete service hello-nodekubectl delete deployment hello-nodekubectl create -f https://k8s.io/docs/user-guide/walkthrough/pod-nginx-with-label.yamlkubectl apply -f https://k8s.io/docs/user-guide/walkthrough//deployment-update.yamlkubectl get pods -l app=nginxkubectl delete pod -l app=nginxkubectl create configmap example-redis-config --from-file=https://k8s.io/docs/tutorials/configuration/configmap/redis/redis-configkubectl get configmap example-redis-config -o yaml 在一个k8s集群内部各个pod默认跟其他服务与pod是可见的一个Pod可以包含多个应用， 但一个pod里面的应用应该是强绑定的，共享存储网络，跟一些容器信息","link":"/2018/04/13/container/k8s_commands/"},{"title":"Linux桌面环境对比与介绍","text":"简介桌面环境是什么， 常见的有哪些 桌面环境列表GnomeKdeXfcei3lxdeawesomeflux… 桌面环境也是百花齐放， 萝卜白菜构成桌面环境的要素窗口管理器 （Windows Manager）设置管理器 （Settings Manager）登陆管理器 （Login Manager）文件管理器","link":"/2014/09/13/linux/linux_des/"},{"title":"linux BootLoader Grub","text":"前段时间修复移动硬盘分区表的时候，把本本的分区也重新弄了下，导致ubuntu的grub找不到linux分区（由于root分区uuid改变造成的不能正常启动），切换到Linux系统时，提示错误“unknown filesystem”，进入grub rescue模式。在Google上搜了一下，终于解决了，方法如下： 首先使用set命令，查看当前配置信息； 然后再使用ls命令，遍历一下所有的磁盘； 查找Linux操作系统的”/”分区所在的磁盘，可以使用“ls (hdx,x)/”，这里的hdx代表你的物理磁盘，如果只有一块硬盘，则x的值为0，后面一个x（也肯呢个是msdosx,是具体情况而定）代表“/”分区的编号。执行该命令（注意，ls命令后的“/”不能少，否则会出现“bad filename”错误）后，如果结果为“unknown filesystem”，则说明不是Linux分区，继续查找，知道返回带有“ /boot”目录的分区。 找到“/”挂载点所在的分区后，就可以修改启动分区了： grub rescue &gt;root=(hdx,msdosx) grub rescue &gt;prefix=(hdx,msdosx)/boot/grub grub rescue &gt;insmod normal grub rescue &gt;normal执行完normal命令后，如果normal模块加载成功，那我们就可以看到久违的grub引导菜单了。此时，按“c”切换到grub的命令行模式,修改grub菜单：grub &gt;root=(hdx,msdosx) //设置系统启动分区，在这里指向内核所在分区grub &gt;prefix=(hdx,msdosx)接下来加载Linux.mod模块，并将新的启动信息写入grub：grub &gt;insmod (hdx,msdosx)/boot/gurb/linux.modgrub &gt;linux /boot/vmlinuz-xxx-xxx root=/dev/sdax //里边的xxxx可以按Tab键grub &gt;initrd /boot/initrd.img-xxx-xxx 执行boot命令，启动系统（如果系统不能启动，可以重复1-4步，多试几次）： grub &gt;boot 正常启动系统后，在终端中输入“sudo update-grub”命令，重新生成“grub.ccfg”文件，更新grub信息，屏幕会出现“generating…”的信息。如果没有安装grub-pc软件包，或者grub-legacy，会出现无法找到命令的错误。这时，只需安装一下grub-pc软件包即可。（注意：安装过程中会出现提示要不要新建grub到第一分区，由于我的本本第一分区是Windows系统，所以在此我选择“NO”，而是将grub建立在“/”挂载点所在的分区） 更新完毕之后，重启，问题解决了。如果问题还没解决，重复1-6步的同时，重新建立grub到第一硬盘mbr：sudo grub-install /dev/sda","link":"/2014/05/13/linux/linux_bootloader/"},{"title":"linux 与主流的操作系统对比","text":"LinuxWindowsMac对比表格","link":"/2014/05/13/linux/linux_compare/"},{"title":"关于Linux 发行版","text":"发行版是什么这里我就不去用搜索引擎搜索答案来告诉大家发行版是什么了， 我就谈谈自己对发行版的理解与定义。所有Linux发行版本是同源（使用Linux内核）， 不同发行版只是在内核上构建的软件的包管理器与上层的其他软件不同罢了。不同的发行版的侧重点可能不同， 有的发行版侧重于稳定与安全， 有的发行版侧重于外观， 有的发行版侧重于多媒体功能，有的发行版侧重于滚动更新。 发行版简介学习Linux 发行版选择是一个绕不过的槛， 尤其是你熟悉了Linux之后， 必然会选择一个自己喜爱的发行版， 当然此文不会帮你决定你应该使用哪个发行版， 大家应该各有所爱，选择合适自己的发行版即可。如果不嫌笔者话多，我在后面会推荐几个发行版，同时会说出这些发行版的一些特点。常见的几个发行版如下：如果我没列出其他的，请不要不开心，因为太多我列不完|Archlinux | Debian | Fedora | Redhat | Ubuntu||—-|—–|—–|——|——|| | | | | | 知名发行版排行榜（按排行来划分）Distro Watch 前15（2017-10） Mint Debian Manjaro Ubuntu Antergos openSUSE Solus Fedora elementary Zorin TrueOS deepin CentOS Arch PCLinuxOS 为什么要看这个发行版的排行榜的数据呢？ 发行版排行榜往往是在学习Linux初期就应该去关注的一个东西。因为如果这一步你关注了它能帮你省下不少麻烦，比如你在一开始选择了比较小众的一个发行版， 这个发行版在世界范围内使用用户不足千人，那么你很难及时得到业界其他Linux爱好者，或者其他技术人员的技术支持， 因为不同发行版的DIff还是比较大的， 往往一些发行版有的问题，另一些没有，无法重现。所以在这一步，还是推荐大家去看看发行版这个东西。当然发行版也不是看完排行榜之后直接去选择排名第一的就好，因为风水轮流转，明年指不定是哪个发行版排第一， 所以按笔者的愚见，只要选一个排名前20的总是没错。一个排名靠前的发行版往往会有一些活跃的社区， 或者维护完备的Wiki文档，尤其是那些最近活跃的发行版。 如果你比较重视外观， 或者性能，或者稳定性，或者实时性, 你可以去搜索引擎里搜索这些发行版，最好去他们官网去看相关的介绍， 排行前10的发行版官网一般都是很正式的， 官网一般都会给出许多发行版的相关截图，及一些发行版的特性介绍，这些都可以作为你选择发行版的参考条件。 按系列与来源进行划分这里的系列是指包管理器相同，其他软件有些差异的发行版。个人认为写一个优秀的包管理器还是很花费时间与精力的， 而且很难得到开源社区的认同，这也是很少人会去写包管理器的原因，如果我们假定同一个包管理器是一种发行版， 那么发行版的数量会锐减到10倍不止， 下面我列出了主流的一些发行版（按包管理器来分）。 Debian 系列 Ubuntu Mint Debian Elemantary Os Zorin Os Deepin Os Redhat 系列 Redhat Centos Fedora Slackware 系列 Slackware Opensuse Slax ArchLinux系列 Manjaro Archlinux Liri Os Gentoo系列 Getoo Funtoo Chrome OS 其他系列其他系列的发行版比较少, LFS 不算一个发行版只是一本书简单列一下比较有名的发行版 Android Puppy Solus 为什么我会按照这个维度去再进行一次划分，有些人会认为笔者的文章是为了凑字数，其实恰恰相反， 这个维度去划分，是为了让大家明白包管理器的重要性。一个包管理器下的发行版，往往安装、删除、更新软件、设定软件仓库地址等都大同小异， 比如如果你是个Ubuntu的用户，让你切换到Mint你基本是无感的，因为安装方式都基本相同，使用方式也无太多差异。 当然在使用体验上回造成很大差别的还有一个因素， 就是桌面环境（DE）, 我们会在后续的章节里面详细介绍桌面环境的差异。 小结通过本章的学习，希望大家能对Linux的发行版能有一个大体的概念，能够自主的通过搜索引擎查看官网介绍，来决定自己去选择哪个发行版。不要再选择发行版上浪费过多时间， 如果你是为了外观的话更没有必要。因为Linux是非常自由的，你完全可以使用不同的发行版定义一模一样的界面（UI）。需要强调的一点是，人生苦短，选定一个发行版就好，不用每个发行版都去学，完全没有意义的。","link":"/2014/08/13/linux/linux_distros/"},{"title":"打造属于自己的Linux个性化界面","text":"#极力外观爱好者浏览这一页内容， 因为这一页会详细的讲述如何自定义一个适合自己使用的Linux系统， 包括外观，也包括一些界面美化软件。 #外观定制 选择一种桌面环境（Gnome）设置壁纸与锁屏界面选择自己喜欢的图标 (icon)选择自己喜欢主题 (theme)选择一个Dock软件 (docky/plank/cairodock)选择一个桌面显示软件 （Conky）操作习惯定制快捷键定制鼠标行为定制定制完成后的效果图","link":"/2014/09/13/linux/linux_customize/"},{"title":"Linux 1号进程","text":"sysinitsystemdLinux系的软件的一些设计理念","link":"/2014/06/13/linux/linux_init/"},{"title":"Linux 常见问题","text":"软件安装问题驱动问题配置问题时间问题语言问题字体问题其他问题","link":"/2014/07/13/linux/linux_faq/"},{"title":"Linux 包管理器简介","text":"apt-get常用命令源（软件仓库）设置yum常用命令源（软件仓库）设置pacman常用命令源（软件仓库）设置emerge常用命令源（软件仓库）设置其他简介","link":"/2014/03/13/linux/linux_pack_mgr/"},{"title":"Linux 系统的安装","text":"#多言无益， 不迈过安装系统这个坎你是永远都学习不会Linux的， 更不可能成为它的爱好者。Linux的系统安装并不比Widnows的安装难，也并不比Windows系统的安装简单，选择何种方式安装也是可以 通用安装步骤本文中我会通过使用Virtualbox 这款软件，来演示Majaro Linux系统的安装步骤。VirtualBox 的下载地址我就不贴了，自己去搜索下载就行了。Manjaro Linux 的下载地址也一样， 不过两个都推荐去官网下载，以免中毒找我麻烦。 VirtualBox安装与设置 如果你想直接装Linux到你的电脑上可以从第一节开始看起 1) 下载virtualbox 2) 安装virtualbox 3) 创建Linux虚拟机，在创建虚拟机之前，请在BIOS里面确保开启了Intel VT-X的支持，否则只能安装32位的Linux 创建步骤：如下 由于是Manjaro，是基于Archlinux的，VirtualBox未列出，可以选择Archlinux 如果这个虚拟机你要用起来流畅，请至少设置 2048M内存 创建虚拟硬盘时候选择任何格式都可以，但容量推荐20GB以上 至此新建虚拟机完成，下面需要进行一些设置，大家按图索骥 设置完虚拟机，大家需要设置一下你的安装文件的位置，如下图 准备 U盘， Linux发行版镜像推荐使用一些工具如Rufus等把自己要装的系统DD到U盘上， 具体很简单这里就不给图示了。制作好U盘启动盘之后，可以把U盘插入电脑的USB口，开机按F2/Delete 进入BIOS设置从U盘启动， 具体的型号可能按键不一样，大家如果不清楚可以看开机时的提示语或者自行搜索。 电脑 你要有一个硬件完好的电脑，如果你幻想使用Linux来拯救硬件本身就是坏的电脑，就别想了, 软件是不能修复硬件的，物理的硬件需要用物理的方法解决。 硬盘分区，挂载分区 选定键盘与区域和语言 创建用户 安装Bootloader 安装Xserver 安装桌面环境 安装登陆管理器 安装其他应用软件 图形界面(GUI) 安装的例子命令行（CLI）安装的例子","link":"/2014/02/13/linux/linux_install/"},{"title":"一些Linux系统知识","text":"启动进程sysinitsystemdLinux系的软件的一些设计理念一些基本概念文件权限sudo 用户用户管理","link":"/2014/03/13/linux/linux_sys/"},{"title":"Linux Shell简介","text":"简介背景知识Linux 的Shell 是Linux入门者必须绕不过的一个坎，对于很多初学者Shell是一个噩梦， 但对于很多有经验的用户，一个没有Shell的系统是最不好用的系统。 Linux、Windows、Mac这三个主流操作系统都是由Shell的存在的不管是Windows的Cmd， PowerShell， 或者是Mac的Terminal， 异或是Linux的各种Terminal。但在Linux的Terminal上你几乎可以无所不能，你可以几乎修改任何系统的设置， 也可以完成一系列复杂的操作， 比如处理图片，处理视频，也可以唤起任何图形界面的程序，只要你系统配置好了， 可以这么说，Linux可以没有图形界面（GUI）， 但不能没有Shell。个人理解Linux的Shell只要配置得当可以甩Windows一大截。 shell 的作用用一句话来概括，就是Shell是你与Linux进行交互的主要渠道之一，主要用来操作Linux系统的方方面面。 另外Shell有着自己的一套脚本语言， 有了这个语言，你也几乎可以做任何自动化的事情， 当然此文不会详细讲Shell编程，因为笔者认为你就算不会Shell编程也可以使用Linux， 完全没有什么问题。 常见的Shellshsh是几乎所有发行版必备的一个Shell, 但它可能不是默认的Shell， 但你总能唤起它。 bashbash是几乎所有Linux发行版都会默认的Shell， 是笔者自己会用的Shell zshzsh 是对Shell期望比较高的一帮人搞出来的一个东西， 它有着自动补全，自动纠错，还有一些自动目录跳转的功能，当然他的功能也不仅仅于此， 当你熟悉了Linux你也会对这个Shell工具十分感兴趣 常见的Shell软件gnome-terminalgnome-terminal 是Gnome桌面环境默认自带的terminal，一般的功能都具有，比如支持多彩显示，主题设置，cursor设置， 背景，前景，字体，粗细，字符集等等 xfce4-terminalxfce4-terminal 是xfce4桌面环境自带的terminal， 一般情况下你只要装了xfce4桌面环境套装，这个terminal 就存在了。它的功能与Gnome的termianl很类似 xtermxterm 的历史比较悠久，很多发行版都自带Xterm，作为一个默认的terminal 其他发行版terminal因为Terminal 的软件实在是数不胜数，因此我也在这里无法列出全部的详细的Terminal， 更没有能力进行一一点评， 选择Terminal也是要按照个人的喜好进行选择，主流发行版/桌面环境里面的Terminal一般都还是不错的，功能也都类似， 也不必刻意去找一个更好的取代品。 TODO Gnome terminal 的配置在shell中嵌入二进制文件 (TODO, fix )将二进制文件打包到shell脚本之前因为要用支付宝更新浏览器插件,直接下载了一个aliedit.sh脚本,直接执行脚本,便搞定了插件的安装,正要称赞阿里的开发人员人性化了,转念一下,一个shell脚本就能搞定的安装,岂不是可以直接cat脚本就可得知支付宝监控工具的代码啦.直接cat结果如下: 1123456789101112131415161718192021222324 main(){SetStringsMkdirARCHIVE=`awk &apos;/^__ARCHIVE_BELOW__/ {print NR + 1; exit 0; }&apos; &quot;$0&quot;`tail -n+$ARCHIVE &quot;$0&quot; | tar xzvm -C $TMP_DIR &gt; /dev/null 2&gt;&amp;1 3&gt;&amp;1if [ $? -ne 0 ]thenecho $PACKAGE_BADQuitfiCUR_DIR=`pwd`cd $TMP_DIR./install.sh#cd &quot;$CUR_DIR&quot;rm -rf $TMP_DIRexit 0}main#This line must be the last line of the file__ARCHIVE_BELOW__2�^��^M�.�Ɠ��jz���Y�Zi(�#;S4#C^��?*oX#���`����jW�u��_���#p��#�`&lt;span style=&quot;font-family: &apos;Lohit Hindi&apos;;&quot;&gt;י��#n�UY������,c���d��II��� shell脚本后面跟了一些乱码,莫非是直接加密了shell,通过阅读代码可以看出脚本后的乱码其实一个tar.gz的二进制: 1ARCHIVE=&lt;code&gt;awk &apos;/^__ARCHIVE_BELOW__/ {print NR + 1; exit 0; }&apos; &quot;$0&quot;tail -n+$ARCHIVE &quot;$0&quot; | tar xzvm -C $TMP_DIR &gt; /dev/null 2&gt;&amp;1 3&gt;&amp;1 首先是用awk获取脚本代码的开始行号,使用tail获取所有二进制码(所以脚本才会有如此注释:#This line must be the last line of the file),通过管道传给tar命令解压到制定目录. 1oen@oen ~/code/shell/aliedit/install $ du -a4 ./README12 ./install.sh268 ./lib/libaliedit64.so244 ./lib/libaliedit32.so516 ./lib536 . 如上可以看到,真正执行是通过解压到临时目录的install.sh 实现的,同时真正玄机在libaliedit64下,是看不到了.不得不说的一个偶然出现的问题：脚本执行完成之后会把临时目录删除,通过vi注释掉删除语句,结果提示包错误,但是怀疑是难道是有脚本校验,但脚本包错误提示是因为tar失败发出的,原来是二进制的乱码通过vi编辑后保存,二进制便彻底变成乱码了,所以tar解包失败.具体使用方式如下: 112345678 oen@oen ~/code/shell/aliedit/install/lib $ echo &quot;oenhan.com blog code&quot; &gt; example.shoen@oen ~/code/shell/aliedit/install/lib $ tar -zcvm * &gt;&gt; example.shexample.shlibaliedit32.solibaliedit64.sooen@oen ~/code/shell/aliedit/install/lib $ cat example.sh | head -n 3oenhan.com blog code#A#3Q#��ePO�����-@p�#��kpw��5�#�&lt;span style=&quot;font-family: &apos;Lohit Hindi&apos;;&quot;&gt;ݝ#����u 其实这个脚本就是一个自解压包,同理你可以把很多文件的二进制搞出来,脚本中找个命令接受转义即可.当然还有专门的命令可以搞定在脚本嵌入二进制文件: uuencode1 uuencode /home/oenhan.com.1.tar /home/oenhan.com.2.tar &gt; /home/oenhan.com.txt将 oenhan.com.1.tar编码到 oenhan.com.txt,将来解码到 oenhan.com.2.tar.具体实现:首先需要写一个脚本example.sh的头: 112345 #!/bin/bashuudecode $0cd /home/tar xvf oenhan.com.2.tarexit 然后将自解压代码编码到脚本中 11 uuencode /home/oenhan.com.1.tar /home/oenhan.com.2.tar &gt;&gt; /home/example.sh 如此一个自解压脚本做成了, uuencode和tar解压没有本质区别,uudecode 自己完成了tar找寻二进制代码的过程,看似很自动化却需要用户安装一个包sharutils,从简易度上得不偿失,不如用tar的方式搞定shell的二进制代码嵌入.","link":"/2014/05/13/linux/linux_shell/"},{"title":"Linux 用户相关的一些知识","text":"主要问题都是安全 譬如专门为 mysql tomcat 创建一个用户名 12sudo groupadd mysqlsudo useradd -r -g mysql mysql 好控制权限，只给相关文件的权限 隔离 万一有 bug/被入侵 运气好也许还能靠权限控制避免进一步的恶劣影响。有些服务用户还设置了 nologin。 话说，搭车问下 windows 上面怎么以其他用户身份运行服务啊，现在是 system 身份运行 ftp 服务感觉很有压力 我司（截至 2017 年 6 月）的某个 linux 生产环境上因 struts2 远程执行漏洞种了 DDOS 木马,由于 tomcat 以 root 运行，导致无法清除木马，后直接迁移。 组策略设定允许某用户以服务身份登陆 在服务里面可以自己改服务启动所属的用户不过首先你要自己新建一个，要不是就用 nt 自有的 命令好像也可以改，明天上班去试试","link":"/2014/03/13/linux/linux_user/"},{"title":"关于Linux的一些思考","text":"总结如果你能看到这里， 我想再次表达自己的感激与感动。山不在高，有仙则灵，此系列的文章也不再多，说清就行。看完本系列的文章，我相信大家都能够独立的安装自己喜欢的Linux操作系统，能够没有太大障碍的使用Linux系统，能够自行利用搜索引擎切切实实的去解决自己遇到的问题，接触到很多开源社区。如果是这样，恭喜你，你已经成功入门。 如果你对我的任何一个章节比较感兴趣，仍然， 搜索引擎是你最大的帮助，当然如果你的理解力比较好，或者我的表述足够通俗易懂的话， 你也不需要去专门去挨个查。有很多东西不一定需要当场学会，而是在以后的日积月累中慢慢学会， 知识是一种累计的过程，而非一蹴而就的。 另外笔者相信大家有耐心，也有信心去逐渐更加深入的学习和了解Linux系统。开源世界的用户体验有时候是不比闭源世界的操作系统做的好， 但开源世界给你提供了做的更好的可能性。 路向何方如果你看完本文还是觉得自己对Linux的兴趣有增无减， 那么恭喜你，你是一名Linux爱好者，也是一个爱好自由的人。看完这些， 最多只能够你安装并使用Linux，但不能带你精通Linux，因为笔者水平也是有限，因此更长的路还是需要读者自己去走完。 这里笔者推荐几个精通Linux必备的技能.1234567891011121314151. shell 编程2. python 编程3. 常用Linux命令学习 find, sed, grep, awk, ssh, vim/emacs, ls, cat等的学习4. linux 管道5. linux的用户、组与权限设计6. linux的初始化工具了解 systemd/sysinit7. linux 网络8. linux 内核* 有人会问笔者为什么笔者不去自己去写完精通相关的文章，其原因有二: 第一笔者自认为水平有限， 不是那种精通的类型， 笔者最多敢说自己是熟悉Linux, 因为Linux是一篇森林， 我所知道的只是深林里的一颗小草。 以上推荐的每一块，有很多人会专门出书或者质量更高的文章来教学，希望此文更多的目的在于指明一些方向，让大家少走弯路， 而非如何交大家如何去走路。","link":"/2014/11/13/linux/summary/"},{"title":"关于音频文件的一些小知识","text":"音频相关知识我们常用的音频格式，大部分都是基于音频CD（采样率44.1khz、采样精度16bit，2通道）的192k是一个分水岭那个，192K以下的，音质损伤比较大 比特率 CBR Constants Bit Rate，恒定比特率 VBR Variable Bit Rate，动态比特率VBR的方式是根据音频源文件中声音的具体频率，自动修正一些比特率，以达到在同样比特率效果中，达到更小的文件 采样率中高低品质的采样率与比特率 项目 低品质 高品质 无损品质 Bitrate 128KBit/s 320KBit/s 916KBit/s 采样率 44100Hz 44100Hz 44100Hz 音频文件中的各种格式的对比无损与有损简单的来说，有损压缩就是通过删除一些已有数据中不太重要的数据来达到压缩目的；无损压缩就是通过优化排列方式来达到压缩目的 无损格式APE(Monkey’s audio)、FLAC(Free LosslessAudio Codec)两种。前者拥有更小的比特率，后者则更容易传播，其区别就是，FLAC可以在传播中断后，已传播的数据就可以直接使用。 格式中的转换","link":"/2013/03/13/others/sound/"},{"title":"Big table 笔记","text":"Big Table 是一种稀疏的多维分布式的有序Map Big Table 的基本数据结构 Tablet Server 的服务方式Compaction 往往发生是为了合并一些数据，节省内存空间(memtable)， 与Log文件的空间需要进行Compation 其他一些基础组件 当一些基础的SSTable分开或者合并的时候， 读写仍然可以同步进行","link":"/2015/09/13/storage/big_table/"},{"title":"Java 接入LDAP小知识","text":"1ldap.url=ldap://10.1.3.129 1234567891011121314&lt;!--ldap configuration--&gt;&lt;bean id=\"contextSource\" class=\"org.springframework.ldap.core.support.LdapContextSource\"&gt; &lt;property name=\"url\" value=\"${ldap.url}\"/&gt; &lt;property name=\"base\" value=\"ou=people,dc=qunhe,dc=cc\"/&gt; &lt;property name=\"baseEnvironmentProperties\"&gt; &lt;map&gt; &lt;entry key=\"com.sun.jndi.ldap.connect.timeout\" value=\"5000\"/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"ldapTemplate\" class=\"org.springframework.ldap.core.LdapTemplate\"&gt; &lt;constructor-arg ref=\"contextSource\"/&gt;&lt;/bean&gt;","link":"/2018/12/13/others/%7Fldap/"},{"title":"MongoDb 草记","text":"备份 1mongodump -h dbhost -d dbname -o dbdirectory 恢复 1sudo mongorestore -h host --db database --dir mongodata/database","link":"/2016/07/13/storage/mongodb/"},{"title":"MySQL笔记","text":"Mysql 笔记此文档下将会放mysql 运维， 使用，应用等相关的文档， 也包括一些高级应用的知识 mysql 使用MySQL 应用速成入门 mysql 运维Centos7 安装MysqlCentos7 安装配置 mysql5.6 Mysql 忘记Root密码MySQL密码找回 mysql 规范MySQL使用规范 mysql 优化mysql 应用如何做数据库分库分表1. 基于客户端路由2. 基于代理和HA的服务的路由常见的分库分表的场景","link":"/2015/09/13/storage/mysql/"},{"title":"开放接口的一些小知识","text":"open 简介open api 是什么？Open API即开放API,也称开放接口 所谓的开放API（OpenAPI）是服务型网站常见的一种应用，网站的服务商将自己的网站服务封装成一系列API（Application Programming Interface，应用编程接口）开放出去，供第三方开发者使用，这种行为就叫做开放网站的API，所开放的API就被称作OpenAPI（开放API）。 （摘自百科，主要为了说一下概念） open api 设计常见问题服务提供通信方式设计常见的设计是：必备的有：app key 与 app secret 签名可选的有： 企业ID， 业务ID。 这些可选的东西一般用来区分业务领域或者权限而存在，确保不同调用者领域与权限的隔离。 app key 与app secret 是我见过最常见，也是必然会出现的一个设计， 在我对接过的十多种API（阿里、腾讯、小米）中， 这这些都是必备的。 app key 的作用主要是为了防止请求被篡改以及用户识别。一般一个 app key 是用于做用户识别的， 服务提供方一般会存放调用方的一些信息， 并可以通过这个App key 检索到。 app secret 的作用app secret 主要是用来签名请求的， 签名过的请求如果被修改之后， 则签名就会发生变化，攻击者是不可能知道这个变化的结果的，这样可以有效防止攻击者的攻击。 一般来讲，会对如下领域进行签名： parameter （url 参数） header （请求头） 一般的流程如下： 用户在请求参数信息中加入时间信息 用户使用自己的 app secret 对请求进行签名 用户把签名的结果， app key 与其他参数一起放到请求里面传到服务提供方 服务方根据 app key找到自己存放的 app secret， 并对请求进行签名 比对签名信息是否一致， 不一致，则认认为请求受到了非法修改，直接拒绝服务。 校验时间信息， 确保时间信息是在允许的范围内， 否则拒绝提供服务。 如何颁发？一般是由服务提供方生成这样的一个键值对， 并把键值对安全的递给调用方。以后 app secret 不会出现在网络传输中，只会用于双方的签名校验。 数据格式与文档 服务地址， 需要准确无误的告诉服务提供方的调用地址 环境设置 （一般API提供方会提供线上与线下两种途径一个用来线上使用，一个用来调试） 参数规定 （对于一个Open API一般来讲参数都是固定的， 不能随意变动，否则会引起双方不小的争执） 结果格式 （结果格式，要写清楚所有出现的结果格式的可能性， 让调用者有办法可以提前对任何可能的结果进行处理） 示例代码 （一段示例代码， 对于开发者是很友好的，大部分开发者喜欢看到这个） 注意事项 安全防范 限流 秘钥对设计安全防范1. 存放攻击这个是最常见的一种， 它不需要知道服务提供方与三方的秘钥， 只需要知道，双方的通信方式， 通过分析这种通信方式，来达到窃取信息，或者造成攻击的目的。这种行为的防范方法也是有一些的：在请求里面加上时间信息，对时间信息进行加密签名， 对时间设置可用时间段， 比如1分钟， 过了一分钟，攻击者获取到的信息就不能再用了。 2. 秘钥泄露秘钥泄露对于很多企业来说并不陌生， 一旦泄露对于双方的危害都很大， 而且如果不能及时发现会带来不小的损失， 这种人为泄漏， 很多时候并没有太好的办法。 可以做的事情比较现实的就是更换秘钥。 3. 穿透攻击调用服务方提供的Open API的三方合作者， 有可能对服务提供方造成压力过大的攻击， 这种攻击主要来源于两种途径： 三方合作者应用程序问题导致请求放大， 导致的调用次数过高 三方合作者把相关接口直接暴露到外部， 这样会带来潜在的问题， 在攻击者识别这样的接口之后，疯狂发起攻击， 服务提供方的服务器会承受巨大压力，甚至crash 对于穿透的攻击： 服务提供方能做的最常见的方案就是限流， 另外就是流量识别，在流量异常的时候对API接口本身做一些限制 当然安全防范确实还有很多需要考虑的点，一般都会结合攻击的特点， 对于攻击类型进行定制防范， 对于业界常见的防范措施，一般都是要默认加入的。 个人的开源项目openapi它主要解决的问题是： 简化服务端提供API的流程 封装服务端API签名流程 封装服务端颁发键值对的流程 封装服务端签名校验机制 提供简单的安全防范 这个工具是用golang设计的， 因此只适用于golang的项目。由于这个工具是基于 http.Request 进行设计的， 因此理论上讲兼容所有的web框架， 如 gin, iris, beego等， 我在readme里面提供了iris的示例代码 如果使用mysql来存放app key 和secret信息， 可以建如下的一个表 1234567CREATE TABLE `app` ( `app_key` varchar(32) NOT NULL, `app_secret` varchar(128) NOT NULL, `created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `updated` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`app_key`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 如果已经存在这样一个类似的表也可以不用建立，在代码中指定即可 123456789r, err := openapi.CheckValid(req,// default implementation is via sql, to fetch the secrect openapi.SqlSecretKeeper{ Db: store.GetDb(), // 可以使用的 mysql 连接 TableName: \"app\", // 存放app key 和secrets的表名 KeyCol: \"app_key\", // app key 的列名 SecretCol: \"app_secret\", // app secret的列名 AppKey: k, // 用户使用的 app key}) 当然如果您已经封装好了app key 与secret的逻辑， 也可以自己实现如下接口 1234// the interface to get the secrettype SecretKeeper interface { GetSecret() (string, error)} 对于使用了web 框架的，只需要写一个middleware， 并启用就行了， 示例代码如下： 创建middleware 123456789101112131415161718192021222324252627282930// create a middle ware for irisfunc OpenApiHandler(ctx iris.Context) { //sign header? to prevent header being modified by others // openapi.SignHeader(true) req := ctx.Request() // you can put the key somewhere in the header or url params k := ctx.URLParam(\"app_key\") r, err := openapi.CheckValid(req, // default implementation is via sql, to fetch the secrect openapi.SqlSecretKeeper{ Db: store.GetDb(), TableName: \"app\", // the name of table where you store all your app keys and secretcs KeyCol: \"app_key\", // the column name of the app keys SecretCol: \"app_secret\", // the column name of the app secrets AppKey: k, // the app key that the client used }) logError(err) if r { // verfy success, continue the request ctx.Next() } else { // verify fail, stop the request and return ctx.Text(err.Error()) ctx.StopExecution() return }} 启用middleware 12345678910// use the middle ware somewhere// so all the apis under this group should be// called with signed result and app key openApiGroup := app.Party(\"/open\") openApiGroup.Use(OpenApiHandler) { openApiGroup.Get(\"/app\", func(ctx iris.Context) { ctx.Text(\"success\") }) } 是不是很简单，如果文中有误，或者缺失的内容欢迎各种批评教育。 如果您能读到这里， 我会感觉到十分荣幸，谢谢您的关注。","link":"/2018/12/13/others/openapi/"},{"title":"gorocksdb 的安装与使用","text":"安装rocksdb官方参考安装方法 去下载 rocksdb 最新的发行版如下代码: 1234wget https://github.com/facebook/rocksdb/archive/v5.14.2.tar.gztar xpf v5.14.2.tar.gzcd rocksdb-5.14.2/make shared_lib -j9 如果编译过程中出错如下 123456789util/status.cc: In static member function ‘static const char* rocksdb::Status::CopyState(const char*)’:util/status.cc:28:15: error: ‘char* strncpy(char*, const char*, size_t)’ output truncated before terminating nul copying as many bytes from a string as its length [-Werror=stringop-truncation] std::strncpy(result, state, cch - 1); ~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~util/status.cc:19:18: note: length computed here std::strlen(state) + 1; // +1 for the null terminator ~~~~~~~~~~~^~~~~~~cc1plus: all warnings being treated as errorsmake: *** [Makefile:650: shared-objects/util/status.o] Error 1 需要打开util/status.cc修改第 28行改为 1std::strncpy(result, state, cch); 然后继续编译完成然后执行如下命令进行安装 123456789cd includes# 拷贝头文件到 include目录cp -r rocksdb /usr/lib/sudo sucp librocksdb.so.5.14.2 /usr/lib/cd /usr/libln -sf librocksdb.so.5.14.2 librocksdb.soln -sf librocksdb.so.5.14.2 librocksdb.so.5ln -sf librocksdb.so.5.14.2 librocksdb.so.5.14 安装其他依赖12345zlib - a library for data compression.bzip2 - a library for data compression.lz4 - a library for extremely fast data compression.snappy - a library for fast data compression.zstandard - Fast real-time compression algorithm. 安装 gorocksdb123CGO_CFLAGS=&quot;-I/path/to/rocksdb/include&quot; \\CGO_LDFLAGS=&quot;-L/path/to/rocksdb -lrocksdb -lstdc++ -lm -lz -lbz2 -lsnappy -llz4 -lzstd&quot; \\ go get github.com/tecbot/gorocksdb 测试代码1234567891011121314151617181920package mainimport ( \"github.com/tecbot/gorocksdb\" \"log\")func Test() { opts := gorocksdb.NewDefaultOptions() opts.SetCreateIfMissing(true) opts.SetCompression(gorocksdb.NoCompression) opts.SetWriteBufferSize(671088640) db, err := gorocksdb.OpenDb(opts, \"test\") wopt := gorocksdb.NewDefaultWriteOptions() if err != nil { log.Printf(\"%v\\n\", err) } defer db.Close() db.Put(wopt, []byte(\"data\"), []byte(\"value\"))}","link":"/2018/08/13/storage/rocksdb/"},{"title":"TDDL 常见配置详解","text":"TDDL 简介TDDL 是淘宝开源的一个基于Client做的， 用于如下特性的一个中间件： 读写分离 权重调配 分库分表 流控/限速 数据库信息配置化 查询重写与优化TDDL 最新的版本是个泄露的版本, 有兴趣的同学可以下载源代码研究一下。 TDDL的配置均以Key-Value 形式存储， 拥有 内存-磁盘-远程 三级降级措施, 官方默认存储配置的地方为Diamond服务器。由于TDDL的配置规则相对复杂， 也许很多人并不能入门， 此文档就是给大家一个配置入门的方案。 变量列表说明 ${dbName} 数据库名称 ${envName} 环境名称 ${userName} 访问数据库使用的用户名 ${ip} mysql 服务器的IP地址 ${appName} appName，数据源名称 ${groupName} 分组名称 ${atomName} 物理数据库别名 数据源的拓扑结构com.taobao.tddl.v1_ds-${appName}_topology123456789&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;matrix xmlns=\"https://github.com/tddl/tddl/schema/matrix\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"https://github.com/tddl/tddl/schema/matrix https://raw.github.com/tddl/tddl/master/tddl-common/src/main/resources/META-INF/matrix.xsd\"&gt; &lt;appName&gt;${appName}&lt;/appName&gt; &lt;group name=\"${groupName}\" type=\"mysql_jdbc\"&gt; &lt;atom name=\"${atomName}\" /&gt; &lt;/group&gt;&lt;/matrix&gt; 这份key-value 对， 配置的是数据源的整体的拓扑结构信息， 这里会想尽的写清楚有哪些 数据分组， 哪些物理数据库会被用到.其中 appName 元素只允许存在一个， 但 group 元素允许存在多个， 也允许非mysql段的存在， atom元素只能存在于group 元素之中，但可以存在多个， 每个group 下面的 atom 都是在数据上是等价的（数据集合相同， 可能读写属性不同） 数据源的规则集合com.taobao.tddl.rule.le.${appName}1234567891011121314151617181920212223242526&lt;beans xmlns=\"http://www.springframework.org/schema/beans\"&gt; &lt;bean id=\"designsnapshot_bean\" class=\"com.taobao.tddl.rule.TableRule\"&gt; &lt;property name=\"dbNamePattern\" value=\"designsnapshot_{0000}\"/&gt; &lt;property name=\"dbRuleArray\"&gt; &lt;value&gt;(#designid,1,64#.longValue() % 256).intdiv(64)&lt;/value&gt; &lt;/property&gt; &lt;property name=\"tbNamePattern\" value=\"designsnapshot_{0000}\" /&gt; &lt;property name=\"tbRuleArray\"&gt; &lt;value&gt;#designid,1,64#.longValue() % 64&lt;/value&gt; &lt;/property&gt; &lt;property name=\"allowFullTableScan\" value=\"true\"/&gt; &lt;/bean&gt; &lt;bean id=\"vtabroot\" class=\"com.taobao.tddl.rule.VirtualTableRoot\"&gt; &lt;property name=\"tableRules\"&gt; &lt;map&gt; &lt;entry key=\"design_snapshot\" value-ref=\"designsnapshot_bean\" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"dbIndexMap\"&gt; &lt;map&gt; &lt;entry key=\"designsnapshot_sequence\" value=\"designsnapshot_0000\" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"defaultDbIndex\" value=\"tms-db-dev\" /&gt; &lt;/bean&gt;&lt;/beans&gt; 这是一个表明整个数据源拓扑结构的规则文件， 也是分库分表规则配置的地方. vtabroot 是总的规则入口的bean。下面有几个概念需要详细解释一下。 defaultDbIndex 是默认的数据分组的设定， 当一个数据源（逻辑）有多个数据库（不同的数据库）的时候， 一个查询分发需要知道这个查询所涉及的表在哪个库上， 如果没有特殊指定， 那就会分发到这个东西指定的 group上。 dbIndexMap 用于指定哪些表在哪 group， 具体来说就是表库的映射关系， 优先级比 defaultDbIndex 高， 同样是在用于配置多个不同数据的数据库的时候使用的。 tableRules 用于定义虚拟表与虚拟表对应的分库分表规则的实际物理数据库关系。 key 是虚拟表名， 可以直接 SELECT FROM , value-ref 是 分库分表规则对应的bean 分库分表的bean 用于设置分库分表的规则dbRuleArra 是分库规则。如例所示， 以 designid列 进行分库， 每个库64张表， 共分为 256/64个库，比如designid = 500, 244 / 64 = 3 分到第四个库上。tbRuleArray 是分表规则。进入到每个库上的ID再进行取余64决定最后落到哪个表里。 这两个字段写法并非固定， 都是支持TDDL内置的一些表达式语法的， 具体得去看代码，此处只列出了最常用的取模的表达式。 数据源的读写分离权重优先级等配置com.taobao.tddl.jdbc.group_V2.4.1_${groupName}1${atomName}:rwp0i0,${atomName}:rp1i1 逗号是各个数据库原的分隔符，冒号前面的是atom的名字。rw 是指读写, r 是指只读, p 是优先级的意思， p越大则越优先分配到。i index， 是指各个数据源的一个index， 此外还有个权重配置的w可以写在 p 后面 数据库本身的配置com.taobao.tddl.atom.global.${atomName}12345dbName=${dbName}dbType=mysqldbStatus=RWip=${ip}port=3306 数据库自身的配置， 与TDDL 无关， 需要让TDDL知道数据库的地址， 端口， 数据库名称等基本信息 连接池的配置com.taobao.tddl.atom.app.${appName}.${atomName}1234567userName=${userName}maxPoolSize=100minPoolSize=5idleTimeout=1testOnBorrow=1abandonedTimeout=60connectionProperties=charset=utf-8 由于TDDL底层使用Driud连接池， 因此， 这里主要用于配置 druid 连接池 数据库账号与密码指定com.taobao.tddl.atom.passwd.${dbName}.mysql.${userName}1encPasswd=${password} 此处用于配置数据库用的密码。 由于开源出来的版本好像是没有对应的加密逻辑实现的， 因此， 这里的密码并不是加密的密码，而是明文的密码。不过用户可以自行实现加密方法， 此处的接口TDDL是有留出来的。","link":"/2016/09/13/storage/tddl_config/"},{"title":"TDDL 简介及入门使用","text":"TDDL简介TDDL 是淘宝开源的一个用于访问数据库的中间件， 它集成了分库分表， 读写分离，权重调配，动态数据源配置等功能。封装 jdbc 的 DataSource给用户提供统一的基于客户端的使用。它只是一组Jar包， 并不是单独的服务，目前已经被一些公司默默使用， 因为官方至今为止没有维护这一块的开源社区， 大多数使用也停留在各个公司自己研究的阶段。 TDDL 最新的版本是个泄露的版本, 有兴趣的同学可以下载源代码研究一下。 使用方法引入依赖1234567891011121314151617181920&lt;dependency&gt; &lt;groupId&gt;com.taobao.tddl&lt;/groupId&gt; &lt;artifactId&gt;tddl-matrix&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.taobao.tddl&lt;/groupId&gt; &lt;artifactId&gt;tddl-config-diamond&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.taobao.tddl&lt;/groupId&gt; &lt;artifactId&gt;tddl-parser&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.taobao.tddl&lt;/groupId&gt; &lt;artifactId&gt;tddl-sequence&lt;/artifactId&gt; &lt;version&gt;${project.version}&lt;/version&gt;&lt;/dependency&gt; 注册bean1234567@Beanpublic DataSource getDataSource() throws TddlException { TDataSource dataSource = new TDataSource(); dataSource.setAppName(\"appName\"); dataSource.init(); return dataSource;} 完成以上两步之后，它的使用是跟普通的 Datasource 是一样的, 可以很方便的与各种ORM框架集成。 三层架构（可独立使用）：Matrix（TDataSource）matrix 是整个 datasource的入口， 用户使用的入口。它是一种标准接口的多数据源多功能的实现。这里控制了一个数据源所有的对应的数据库的信息， 下面可以持有多个 group, 而 group下面有可以有多个 atom。它也是唯一支持分库分表实现的一层， 用户一般使用的是这一层， 如果不需要其他规则和功能也可以使用这一层。 Group（TGroupDataSource）group 是逻辑数据分组的概念， 一个 group 下的数据一般都是相同的， 不同的是对数据源的配置， 不同实际数据源， 对应不同的读写属性， 优先级， 权重等 Atom（TAtomDataSource）atom 是对实际物理数据库的一种抽象。 它持有的是实际的物理数据库的信息，以及这个物理数据库所相关的 druid 连接池的配置信息。它隶属于 group。 不支持的点 不支持跨库事务 不支持很奇怪的SQL 不推荐使用跨库JOIN的功能 其他 tddl-manager 模块是一个管理TDDL数据源的界面UI tddl-server 是mysql协议的一个实现， 底层使用TDDL数据源","link":"/2016/08/13/storage/tddl/"},{"title":"tidis 笔记","text":"策略 懒删除 用线程去扫所有的key以便删除该删的 如果启用过期机制，则需要多一个key用来存过期时间 不启用过期则比较快 缺少 监控与统计信息 充足的压测信息 单测不够 key分析， 可以在扫描的时候做掉","link":"/2016/08/13/storage/tidis/"},{"title":"Golang的工程配置","text":"go module简介Go 1.11 中引入了module 管理的概念，这使得我们终于有一个官方正式支持的模块管理工具了， 但由于官方工具刚出功能还不是十分完善，易用， 很多人还摸索不出来如何使用是最科学的。 为什么要使用 go module? 更好的版本管理， 使得项目依赖的module版本都是确定以及稳定的（这是依赖管理的最核心的要求）依赖管理更简单方便dep本身不利于项目中私有仓库的包共享， dep 在拉取依赖的时候， 会把依赖放到 项目目录的vendor下面， 这样也能达到上面说的这个效果，但是dep本身对私有仓库的支持不好，不能很好的配置ssh key拉取私有仓库代码（无论如何配置它就是不用，不是不能用就是不好用）另外，dep对依赖的拉取是相对比较慢， 处理比较复杂的。dep已经进入不维护状态， 而且go module 得到了官方的支持与更新的确认 私有仓库通过git命令1git config --global url.git@github.com:.insteadof=https://github.com 修改git配置文件看下 vim ~/.gitconfig 是否有生效，否则，就手动改一下 12[url &quot;git@github.com:&quot;] insteadOf = https://github.com/ 重启terminal生效 go module的要求golang version &lt; 1.12 1GO111MODULE=on 由于官方没有默认打开 go module 的feature 因此在设置ci的时候需要手动设置一下两个环境变量, 这是唯一的代价 golang version &gt;=1.12则不需要进行任何设置 依赖管理初始化项目1go mod init mod_name 添加依赖1go get github.com/demo/xxx 删除依赖1go mod edit -droprequire github.com/demo/test 更新依赖版本1go mod edit xxx 更新与删除依赖比较难用， 建议直接修改go.mod 文件与go.sum 文件， 更简单直接 整理依赖1go mod tidy 这句命令会自动去除没用的依赖， 添加需要增加的依赖 依赖查询1go mod graph 依赖设置代理由于墙的原因， 想要去使用Go， 没有代理还是不行的 12set GOPROXY=https://goproxy.io # windows, 也可以设置环境变量export GOPROXY=https://goproxy.io # windows","link":"/2018/12/13/langs/golang/go_project/"},{"title":"Golang 简介","text":"IDE 选择vscodegoland （收费）需要装的插件列表 material theme UI m jetbrains 出品， 省去了很多的配置麻烦，而且用IDEA的用户可以无缝切换到这个的使用。 常见问题SQL查询的Rows不关闭的时候回造成阻塞的问题对此的英文解释大概意思是， 如果不关闭rows， 缓冲区就有未读完的数据， 然后就造成了连接阻塞，影响接下来的查询， 最终会导致连接池的连接全部用满。 1db.Query(..) returns a rows struct which must be closed or completely iterated (with for rows.Next()). Otherwise there is unread data on the connection stream and the connection blocks. 如果需要执行但不需要结果可以用 db.Exec(..) 1Simple solution: use db.Exec(..) instead. Does it work then? gorm 的问题 1db.Raw.Row() 如果这个row不被消费则会造成这个问题如果不想消费 1db.Exec() golang 初始化顺序","link":"/2017/09/13/langs/golang/goland/"},{"title":"Golang JSON 详解","text":"GOLANG JSON 详解JSON 简介json 是JavaScript Object Notation 的简称, 它是一种数据格式定义语言， 使用起来非常简单，层级结构也非常明确，学习成本几乎为0， 两分钟即可明白它的格式和含义，并上上手写自己的一段json, 下面是个json的实例 12345678910{ \"data\":1, mems:[ { \"name\":\"Tom\", \"age\":6, \"avatar\": null } ]} JSON 库对于golang 而言， json解析也是sdk自带的一个功能， 而且它还定义了json的一些规范，位置在 encoding/json 下面。 另外还有一个值得一提的json库是 github.com/json-iterator/go， 据说他有着非常高的性能 API 定义123456// 序列化func Marshal(v interface{}) ([]byte, error)// 序列化并且加缩进func MarshalIndent(v interface{}, prefix, indent string) ([]byte, error)// 反序列化 func Unmarshal(data []byte, v interface{}) error 使用示例1234567891011type Test struct { Content string}func testJson() { var a A // 序列化 d, err := json.Marshal(a) // 反序列化 json.Unmarshal(d, &amp;a)} JSON TAGS12345type A struct { Name string `json:\"name,omitempty\"` // name， 可为空空则忽略 Age int `json:\"age\"` Num uint64 `json:\"num,string\"` // 把num在序列化的时候序列化成string} 注意事项json html 注意事项html的encode的时候往往会有问题， 因为默认的json的marshal 方法是忽略这点的， html标签中的 &gt;, &lt;, &amp; 会被golang变掉， 如果想保持原样请用如下方式 123456789func testJson() { t := new(Test) t.Content = \"&lt;p&gt;test&lt;/p&gt;\" bf := bytes.NewBuffer([]byte{}) jsonEncoder := json.NewEncoder(bf) jsonEncoder.SetEscapeHTML(false) jsonEncoder.Encode(t) fmt.Println(bf.String())} JSON 自定义marshal在 encoding 包下有个文件叫 encoding.go 里面定义了很多接口，在反序列化的时候会被调用，如果你想像java那样通过设置 getter 和setter来改变默认的行为， 你只需要把被处理的对象实现这些接口就行了， 例如我们常见的 logrus.Level 便实现了这一接口， 可以使得 “info”字符串可以被反序列化成 uint32 1234567891011121314151617// TextMarshaler is the interface implemented by an object that can// marshal itself into a textual form.//// MarshalText encodes the receiver into UTF-8-encoded text and returns the result.type TextMarshaler interface { MarshalText() (text []byte, err error)}// TextUnmarshaler is the interface implemented by an object that can// unmarshal a textual representation of itself.//// UnmarshalText must be able to decode the form generated by MarshalText.// UnmarshalText must copy the text if it wishes to retain the text// after returning.type TextUnmarshaler interface { UnmarshalText(text []byte) error}","link":"/2019/12/02/langs/golang/json/"},{"title":"Golang 下的数据库使用","text":"golang 下使用数据库是几乎每个golang程序员必须经历过的一个环节， 我们在这里专门挑了两个常见的数据库的使用方式来进行简单的科普一下。此文可以用作使用MySQL和Postgres的笔记性的文档， 不做深入分析， 全当给大家记备。 Golang下使用 MySQL由于golang 官方的SDK中已经定义好了数据库的访问接口， 还内定了连接池连接方式等基本的数据库操作元素， 但Golang并没有实现每种数据库的访问方式。因此如果要正常使用数据库，数据库相关的开发者需要找到定义了数据库具体访问方式与协议的数据库的驱动， 以访问数据库。 下面我们就从驱动讲起。 驱动1go get github.com/go-sql-driver/mysql 基础使用创建数据库访问对象 *sql.DB1234567891011121314151617181920212223242526272829303132333435import ( \"database/sql\" \"fmt\" \"time\" _ \"github.com/go-sql-driver/mysql\")const ( USERNAME = \"root\" PASSWORD = \"*******\" NETWORK = \"tcp\" SERVER = \"localhost\" PORT = 3306 DATABASE = \"blog\")func getDb() *sql.DB { dsn := fmt.Sprintf(\"%s:%s@%s(%s:%d)/%s\",USERNAME,PASSWORD,NETWORK,SERVER,PORT,DATABASE) DB,err := sql.Open(\"mysql\",dsn) if err != nil{ fmt.Printf(\"Open mysql failed,err:%v\\n\",err) return } DB.SetConnMaxLifetime(100*time.Second) //最大连接周期，超过时间的连接就close DB.SetMaxOpenConns(100）//设置最大连接数 DB.SetMaxIdleConns(16) //设置闲置连接数 return db}var mysqlDb = getDb()// 用此函数，则为单例， 只在初始化的时候创建一次， 不会多创建无用的连接池func GetDb() { return mysqlDb} 查询数据库1234567891011121314151617181920212223242526func FetchData(db *sql.DB) { // 查询多行 rows, err := db.Query(\"SELECT * FROM user limit 3\") if err != nil { // 如果err不等于nil, 一般row 为nil， 所以你直接close 会抛出空指针异常 log.Error(err) return } // 注意这个一定要close defer rows.Close() // 取数据 for rows.Next() { var userId int var username string // 如果有多行， 可以把结果放到相应的数据结构中 sErr := rows.Scan(&amp;userId, &amp;username) } // 查询单行 row := db.QueryRow(\"SELECT username FROM user WHERE user_id = ?\", 1) var username string sErr := row.Scan(&amp;username) // deal with error and username} 执行DML1234567func DML() { // INSERT result, err := db.Exec(\"INSERT INTO user(username, user_id) VALUES(?, ?)\", username, userId) stmt, err := db.Prepare(\"INSERT INTO user(username, user_id) VALUES(?, ?)\") sr, err := stmt.Exec(username, userId) // result 和 sr 中都可以拿到自动生成的ID，以及影响的行数 } Golang下使用 Postgres SQL驱动1go get github.com/lib/pq 基础使用创建数据库访问对象 *sql.DB12345678910111213141516171819202122232425262728293031323334package mainimport ( \"database/sql\" \"fmt\" _ \"github.com/lib/pq\")const ( host = \"localhost\" port = 1234 user = \"testuser\" password = \"testpass\" dbname = \"test_db\")func getDb() *sql.DB { psqlInfo := fmt.Sprintf(\"host=%s port=%d user=%s \"+ \"password=%s dbname=%s sslmode=disable search_path=test_schema\", host, port, user, password, dbname) db, err := sql.Open(\"postgres\", psqlInfo) if err != nil { panic(err) } defer db.Close() err = db.Ping() if err != nil { panic(err) } return db}// 同样你可以再使用单例模式创建一个访问方式// 但推荐使用 sync.Once() 进行单例模式的设计 Postgres SQL中查询数据由于其他的与MySQL一样， 因此此处实例就简单一点写明： 123456func QueryPostgres(db *sql.DB) { // MySQL 使用 \"?\" Postgres 使用 $num, // 使用 ？ postgres 会报错 db.Query(\"SELECT * FROM user WHERE user_id = $1\", userId) db.QueryRow(\"SELECT * FROM user WHERE user_id = $1\", userId)} Postgres SQL 中进行DML操作12345func DML(db *sql.DB) { // 貌似必须得用stmt 直接使用Exec 会报错 stmt, err := db.Prepare(\"UPDATE user SET username= $1 WHERE user_id = $2\") stmt.exec(\"Lily\", 123)} 使用数据库的注意点golang 使用数据库的注意点都基本相同因此不再把postgres跟mysql分开讲述 合理创建连接池 合理设定连接池参数 对于一个数据库仅仅创建一个连接池 关闭该关闭的结果集主要还是提到的 12// 如果不关闭， 它会慢慢的耗尽连接池的连接， 并且让你无连接可用rows.Close()","link":"/2018/12/13/langs/golang/golang_db/"},{"title":"golang 学习笔记","text":"go-docgolang 中文文档本文主要分三部分来帮助初学者能够顺利入坑golang， 第一部分主要是讲golang语法， 第二部分主要是讲golang的工程化的东西，附带会推荐一些常用的三方包。第三部分则主要是介绍这门语言的编程思想，常见的思路。学习一门语言，主要是学习语法、工程化、以及这门语言的编程思想。附录则会讲一些工具的使用 golang 基本语法参见demo下面的， 基本10分钟看完就学会了 golang 工程化golang 工程化是教大家如何配置golang项目，如何组织代码结构，以及如何调试分析，进行单元测试与性能测试等等。工程化是一门语言离不开的话题，好的工程化与工具可以令人事半功倍。 golang 开发工具常见的golang开发工具有非常多，但有着比较好的代码补全，以及其他提高编程体验的IDE却为数不多。目前市面上比较流行的开发工具主要有如下几个，也可能有些高手手写vim插件使用vim之类的，此类的不做说明了。 goland vscode lite ide 以上三个IDE(集成开发环境) 我更倾向于使用jetbrains 的 goland, 虽然goland目前有成吨的bug, 但由于习惯问题，我还是习惯使用，大家也可以根据自己的习惯来使用相应的IDE。后面会有专门的文章对goland下如何配置golang项目进行详细说明。 golang 项目组织结构一般而言一个非库型golang项目会存在一个main.go, 用作程序的主入口。同时也会存在一些模块，跟一些配置文件。这些配置文件与模块的推荐的放置方式如下。后面也会有专题去说明我们这样放置的道理与好处。 123456module1_dirmodule2_dirothers.gomain.goconfig_filereadme dep与 go module dep 是 go module 推出之前半官方的一个依赖管理工具，它使用起来非常简单， 它的总体思想是把依赖代码集成到项目中的vendor目录去，但不需要手动管理这些依赖。这样做有一个好处：在编译期不用下载额外的依赖。但也有个坏处：会使得代码仓库变得比较庞大， 除非使用ignore 忽略这个vendor目录 go module 是golang官方推出的在 1.11版本后用来取代其他各种依赖管理工具的官方工具，它的思想跟maven的思想有点像， 与dep不同的就是它会把依赖不放入项目中去，而是管理到 GOPATH下面。这样做可以节省很多的代码仓库的空间。在进行编译的时候提前把依赖下载好，就不存在编译的时候下载依赖的问题了， 是以后的方向。 golang 生成各个平台下的可执行文件golang 生成其他平台的二进制文件是非常非常简单的，只需要进行简单的一个环境变量的设置即可生成 mac/windows/linux 等平台下的二进制文件， 二进制文件可以直接运行而不需要像java一样需要一个 jre, 这也许就是golang的一个简单哲学。 golang 单元测试与性能测试golang提供了简单的单元测试与性能测试的功能， 虽然简单却很强大，后面会专门文档来说明如何去写单元测试与性能测试。golang的单元测试与性能测试的文件需要以 _test.go结尾单测方法和性能测试方法需要用类似如下方法声名。 12345678910111213import \"testing\"// 单元测试需要以Test开头的函数名称func TestGenerateQrCode(t *testing.T) { GenerateQrCode() // t.Fail()}// 性能测试需要以Benchmark 开头的函数名称func BenchmarkGenerateQrCode(b *testing.B) { GenerateQrCode() // b.ReportAllocs()} golang 内存分析pprof golang 远程调试dlv golang 编程思想附录go fmtgo docgo mod","link":"/2017/03/13/langs/golang/index/"},{"title":"web 前端一些基本知识","text":"IDEvscode 是开发前段不二的IDE, 也是近年来维护最活跃的一个编辑器 插件 Auto Close Tag Auto Rename Tag Beautify Bracket Pair Colorizer Class autocomplete for HTML Code Runner Css peek Document this Eslint TsLint Image Preview Node.js Module Intellisense Path Intellisense快捷键 ctrl+shift+b 构建项目 F5 run F10 Step over F11 step into Shift F11 step out. Alt+Shift+F format node与nmp包安装与管理工具yarn 与npm淘宝npm 镜像cnpm1npm install -g cnpm --registry=https://registry.npm.taobao.org 别名的方式12345678910alias cnpm=&quot;npm --registry=https://registry.npm.taobao.org \\--cache=$HOME/.npm/.cache/cnpm \\--disturl=https://npm.taobao.org/dist \\--userconfig=$HOME/.cnpmrc&quot;# Or alias it in .bashrc or .zshrc$ echo &apos;\\n#alias for cnpm\\nalias cnpm=&quot;npm --registry=https://registry.npm.taobao.org \\ --cache=$HOME/.npm/.cache/cnpm \\ --disturl=https://npm.taobao.org/dist \\ --userconfig=$HOME/.cnpmrc&quot;&apos; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc 项目结构package.json文件介绍package.json 是项目的总的一个配置文件， 它定义了这个项目所需要的各种模块， 以及项目的基本配置信息。可以自动生成或者手动编写， 自动生成的方法是用node 1node init scripts 段scripts 段制定了运行脚本命令的npm命令行缩写， 比如start指定了运行npm run start 的时候所需要执行的命令 1234&quot;scripts&quot;: { &quot;dev&quot;: &quot;node build/dev-server.js&quot;, &quot;lint&quot;:&quot;eslint --ext .js,.vue src test/unit/specs&quot;} dependenciesdepencies and devDependencies 分别指定了项目在运行时候依赖的模块与项目开发的时候需要的一些模块， 它们都指向同一个对象， 用来管理各种依赖 1234567&quot;dependencies&quot;: { &quot;vue&quot;: &quot;^2.2.2&quot;, &quot;vue-router&quot;: &quot;^2.2.0&quot; }, &quot;devDependencies&quot;: { &quot;autoprefixer&quot;: &quot;^6.7.2&quot; } config 字段config 字段用于向环境变量输出值 engines 字段engines 字段主要声明node 与 npm的版本 bin 字段bin字段 主要是为了让一个可执行命令安装到系统的路径， 可以直接调用,比如，要使用hello作为命令时可以这么做： 1{ &quot;bin&quot; : { &quot;hello&quot; : &quot;./cli.js&quot; } } 这么一来，当你安装hello程序，npm会从cli.js文件创建一个到/usr/local/bin/myapp的符号链接(这使你可以直接在命令行执行hello脚本)。","link":"/2019/06/18/langs/fe/project/"},{"title":"Golang的内存分析工具pprof","text":"pprof 的使用","link":"/2017/03/13/langs/golang/memory_analyze/"},{"title":"markdown 语法笔记","text":"Markdown基本常用语法列表 你好 xx列表 There is a literal backtick (`) here. Use the printf() function. 分割线 引用 链接 百度 这是一个标题。 这是第一行列表项。 这是第二行列表项。 给出一些例子代码： return shell_exec(&quot;echo $input | $markdown_script&quot;); 图片 表格 表头1 列11 列2 居左 居中 居右 强调 被强调的文本 被强调的文本","link":"/2016/01/13/langs/grammar/markdown/"},{"title":"react+ts+webpack+antd 实战","text":"1. 安装react项目创建程序1npm install -g create-react-app yarn 2. 创建react + ts项目1create-react-app my-app --scripts-version=react-scripts-ts react-scripts-ts是一些调整的组合，使得你可以使用标准create-react-app项目流程并且加入TypeScript。现在你的项目应该是下面这个样子： 123456789my-app/├─ .gitignore├─ node_modules/├─ public/├─ src/│ └─ ...├─ package.json├─ tsconfig.json└─ tslint.json 其中： tsconfig.json包含我们项目中的TypeScript的配置信息 tslint.json是我们的代码规范工具TSLint相关的配置 package.json包含我们的依赖项，以及一些用于测试、预览、部署等的快捷命令。 public包含静态资源，比如我们要部署的HTML页面和图片。你们可以删除除了index.html以外的任何文件。 src 包含了我们TypeScript和CSS的代码。index.tsx是我们的文件的入口。 在package.json 中scripts中 分别有 start 开发命令 执行 npm run startbuild 部署命令 执行 npm run buildtest 测试命令允许Jest 3. 集成antd（不需要UI库可以跳过这里）1yarn add antd ts-import-plugin --dev 4.配置 config-overrides.js1234567891011121314151617181920212223242526272829303132333435363738394041/*jshint esversion: 6 */const tsImportPluginFactory = require(&apos;ts-import-plugin&apos;);const { getLoader} = require(&quot;react-app-rewired&quot;);const rewireLess = require(&apos;react-app-rewire-less&apos;);module.exports = function override(config) { const tsLoader = getLoader( config.module.rules, rule =&gt; rule.loader &amp;&amp; typeof rule.loader === &apos;string&apos; &amp;&amp; rule.loader.includes(&apos;ts-loader&apos;) ); tsLoader.options = { getCustomTransformers: () =&gt; ({ before: [tsImportPluginFactory({ libraryDirectory: &apos;es&apos;, libraryName: &apos;antd&apos;, style: &apos;css&apos;, })] }) }; config = rewireLess.withLoaderOptions({ modifyVars: { &quot;@primary-color&quot;: &quot;#1DA57A&quot; }, })(config, env); config.resolve = { alias: { &apos;@&apos;: path.resolve(&quot;./&quot;, &apos;src&apos;) }, extensions: [&apos;.tsx&apos;, &apos;.ts&apos;, &apos;.js&apos;, &apos;.jsx&apos;, &apos;css&apos;] }; return config;}; 5.配置 tsconfig.json1234complierOptionsu加入 &quot;paths&quot;: { &quot;@/*&quot;: [&quot;src/*&quot;] } https://blog.csdn.net/u010377383/article/details/79014405","link":"/2019/06/18/langs/fe/new_project/"},{"title":"Golang slice 与 数组详解","text":"golang slice 与数组详解对于很多人来说golang的 slice 与数组很难分清楚具体的区别，不知道什么情况下是数组什么情况下是slice，两者有什么区别，怎么 使用它们， 本文就slice与数数组的区别做出一个详细的说明。 定义slice 定义slice的定义比较灵活， 可以直接用make来定义， 也可以直接初始化好 12var a = []byte{1,2,3} // 定义一个长度为3容量为3的slice， 并初始化好数据var b = make([]int, 0, 3) // 定义一个长度为0， 容量为3的slice 数组定义数据在定义的时候，就指定了长度，且长度与容量相等 12var a = [3]byte{1, 2, 3} // 定义一个长度为3的数组, 并初始化好var b = [2]byte{} // 定义一个长度为2的数组, 初始化为 0 使用区别sliceslice 对于golang来说使用的更普遍一些，因为其灵活性非常好，可以自动扩容， 当然功能多，意味着性能相比功能单一的数组来说较低一些 1234 var a = []byte{1,2,3}fmt.Println(append(a, byte(1))) // ok, slice 可以被append, 且slice 会自动扩容 b[1] = int(a[2]) // ok, slice 也可以根据下标进行操作 len(a) // 这时长度为4, 容量为8（2倍） 数组数组一旦定义了就固定了类型使用起来也只能根据下标进行操作，不同长度的数组，其类型不同，即便其基类型相同，仍然是不同的类型 123// append(a, b) 报错， 因为数组不能进行appendb[1] = a[2] // ok 可以直接根据数组下标进行操作fmt.Println(reflect.TypeOf(a) == reflect.TypeOf(b)) // false 不类型不同, 所以也不能进行赋值或者转换 相互转换尽管如此golang提供了方便的slice与数组相互转换的机制，下面是一些示例代码 12345678910111213141516func conv() { a := [3]byte{1,2,3} // 定义一个数组 // 数组转slice b := a[:] // slice 转数组， 其实这种情况很少 c :=[3]byte{} copy(c[:], b) // 查看结果 fmt.Println(reflect.TypeOf(a)) fmt.Println(reflect.TypeOf(b)) fmt.Println(reflect.TypeOf(c)) fmt.Println(c)} 输出结果 1234[3]uint8[]uint8[3]uint8[1 2 3]","link":"/2019/12/02/langs/golang/slice/"},{"title":"yaml 高级语法笔记","text":"缩进表示层级 行内表示法数组用中括号，属性用花括号， 用逗号隔开 非行内， 数组用短横线， 属性正常 !! 强制转换为字符串 字符串 单引号和双引号都可以使用，双引号不会对特殊字符转义。 字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。 多行字符串可以使用|保留换行符，也可以使用&gt;折叠换行（去掉转换的空格）。 +表示保留文字块末尾的换行，-表示删除字符串末尾的换行 锚点&amp;和别名*，可以用来引用。 &amp;用于起别名，放在： 后面， defaults: &amp;defaults adapter: postgres host: localhost development: database: myapp_development &lt;&lt;: *defaults","link":"/2016/03/13/langs/grammar/yaml/"},{"title":"etcd的安装与配置","text":"说明我们将在 10.1.19.214:2380, 10.1.15.229:2380, 10.1.13.165:2380 三个机器上分别创建三个etcd节点以组成集群 创建用户安装比较简单， 但为了安全性起见， 推荐为etcd创建单独的用户 12sudo groupadd etcdsudo useradd -r -g etcd etcd 下载安装下载12cd /optwget https://github.com/etcd-io/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz ###安装执行如下命令下载加压并创建好制定的文件与目录，设置好软链接等 12345678910cd /opttar xpf etcd-v3.3.9-linux-amd64.tar.gzmv etcd-v3.3.9-linux-amd64 etcd-v3.3.9ln -sf /opt/etcd-v3.3.9 /opt/etcdcd etcdmkdir datamkdir walmkdir ssltouch conf.yml 配置执行如下命令 1vim conf.yml 修改内容 , 如下加粗的内容都要改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd1'# Path to the data directory.data-dir: /opt/etcd/data# Path to the dedicated wal directory.wal-dir: /opt/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 500# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.# List of comma separated URLs to listen on for client traffic.listen-client-urls: http://10.111.8.11:2379,http://127.0.0.1:2379listen-peer-urls: http://10.111.8.11:2380# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: http://10.111.8.11:2380# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: http://10.111.8.11:2379# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: etcd0=http://10.111.10.151:2380,etcd1=http://10.111.8.11:2380,etcd2=http://10.111.9.145:2380,etcd3=http://10.111.10.152:2380,etcd4=http://10.111.8.12:2380# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0#client-transport-security: # Path to the client server TLS cert file.# cert-file: #/opt/etcd/ssl/etcd.pem# Path to the client server TLS key file.# key-file: # /opt/etcd/ssl/etcd-key.pem# Enable client cert authentication.# client-cert-auth: false# Path to the client server TLS trusted CA cert file.# trusted-ca-file: # /opt/etcd/ssl/etcd-root-ca.pem# Client TLS using generated certificates # auto-tls: # false# peer-transport-security: # Path to the peer server TLS cert file.# cert-file: # /opt/etcd/ssl/etcd.pem# Path to the peer server TLS key file.# key-file: # /opt/etcd/ssl/etcd-key.pem# Enable peer client cert authentication.# client-cert-auth: false# Path to the peer server TLS trusted CA cert file.# trusted-ca-file: #/opt/etcd/ssl/etcd-root-ca.pem# Peer TLS using generated certificates. # auto-tls: # false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: \"1\" 注意： 里面关于集群的一些点， 和本机配置的一些点， 如果要配置tls/https 需要自行准备证书， 然后配置被如上注释了， 根据经验，如果配置了https， 再转到http需要清空数据才可以转， 否则是不能转的 最后的步骤更改权限使得使用etcd用户来启动etcd节点 12cd /opt/etcdchown -R etcd:etcd /opt/etcd 配置systemd 的 unit文件并加入开机启动 vim /usr/lib/systemd/system/etcd.service 并贴入如下内容 12345678910111213141516[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]User=etcdType=notifyWorkingDirectory=/opt/etcd/ExecStart=/opt/etcd/etcd --config-file=/opt/etcd/conf.ymlRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.target 验证为了方便etcdctl的使用 1ln -sf /opt/etcd/etcdctl /usr/bin/etcdctl etcd 多版本支持， 支持的有v2的API， 与V3 的API， 但V2与V3 是完全隔离的， V2 更像ZK， V3 更像KV V2 验证集群健康状况1etcdctl cluster-health 如果健康则返回如下 123member 42137edb694602d3 is healthy: got healthy result from http://10.1.15.229:2379member b813a1f117f7f288 is healthy: got healthy result from http://10.1.13.165:2379member e6992029de967f70 is healthy: got healthy result from http://10.1.19.214:2379 V3 验证集群监控在此之前需要先设置环境变量 1export ETCDCTL_API=3 验证命令如下 1etcdctl --endpoints=http://10.1.13.165:2379,http://10.1.15.229:2379,http://10.1.19.214:2379 endpoint health 如果健康则返回如下 123http://10.1.13.165:2379 is healthy: successfully committed proposal: took = 1.605107mshttp://10.1.19.214:2379 is healthy: successfully committed proposal: took = 2.097534mshttp://10.1.15.229:2379 is healthy: successfully committed proposal: took = 2.983391ms 配置推荐CPUetcd是可以利用多核的性能的 轻度使用 CPU 1QPS &lt; 2000 2-4core 中度使用 1QPS &gt; 5000 8-16core 内存轻度使用 Mem 1QPS &lt; 2000 4-8G 中度使用 1QPS &gt; 5000 16-64G 硬盘不需要做raid，raid0 就可以， 因为etcd就是高可用的， 推荐使用固态硬盘 网络需要稳定可靠的网络， 网络不行的话，容易导致可用性比较低 如果要多数据中心部署， 尽量离近一些。 Etcd的一些限制请求大小限制etcd多用于处理小的key-value对的元信息，大的请求虽然也可以工作， 但会增加延时，目前默认最大支持1MB的请求， 这个限制可以通过配置来更改 存储大小限制默认的存储限制是 2GB, 可以用 –quota-backend-bytes flag 来配置;最大支持 8GB. httpshttps 与http互转是有坑的， 可能需要用代码来数据迁移， data下面的与 wal 里面的清空， 通过调整配置可以互转怀疑是这两个地方的一些文件里记录了之前一些tls，的一些设置， 导致互转不成。","link":"/2018/11/13/storage/etcd/etcd/"},{"title":"java 笔记","text":"线程状态 高cpu 使用的代码查找1ps H -eo user,pid,ppid,tid,time,%cpu,cmd –sort=%cpu tid 是线程ID 转成十六进制去查 jstack 9002 &gt; stack.log arthas","link":"/2016/03/13/langs/java/java/"},{"title":"TypeScript 简易笔记","text":"typescript 介绍type script 是Javascript 的一种超集， 引入了JavaScript所欠缺的继承抽象与封装等 基本类型boolean1let isDone: boolean = false; number1234let decLiteral: number = 6;let hexLiteral: number = 0xf00d;let binaryLiteral: number = 0b1010;let octalLiteral: number = 0o744; string模版字符串，它可以定义多行文本和内嵌表达式。 这种字符串是被反引号包围（ `），并且以${ expr }这种形式嵌入表达式 1234let name: string = `Gene`;let age: number = 37;let sentence: string = `Hello, my name is ${ name }.I'll be ${ age + 1 } years old next month.`; array []12let list: number[] = [1, 2, 3];let list: Array&lt;number&gt; = [1, 2, 3]; Tuple [number, string]123456// Declare a tuple typelet x: [string, number];// Initialize itx = ['hello', 10]; // OK// Initialize it incorrectlyx = [10, 'hello']; // Error enum123enum Color {Red = 1, Green, Blue}let colorName: string = Color[2];console.log(colorName); // 显示'Green'因为上面代码里它的值是2 any有时候，我们会想要为那些在编程阶段还不清楚类型的变量指定一个类型。 这些值可能来自于动态的内容，比如来自用户输入或第三方代码库。 这种情况下，我们不希望类型检查器对这些值进行检查而是直接让它们通过编译阶段的检查。 那么我们可以使用 any类型来标记这些变量 12345let notSure: any = 4;notSure.ifItExists(); // okay, ifItExists might exist at runtimenotSure.toFixed(); // okay, toFixed exists (but the compiler doesn't check)let prettySure: Object = 4;prettySure.toFixed(); // Error: Property 'toFixed' doesn't exist on type 'Object'. void某种程度上来说，void类型像是与any类型相反，它表示没有任何类型。声明一个void类型的变量没有什么大用，因为你只能为它赋予undefined和null： 1234function warnUser(): void { console.log(&quot;This is my warning message&quot;);}let unusable: void = undefined; null / undefined默认情况下null和undefined是所有类型的子类型。 就是说你可以把 null和undefined赋值给number类型的变量。当你指定了–strictNullChecks标记，null和undefined只能赋值给void和它们各自 123// Not much else we can assign to these variables!let u: undefined = undefined;let n: null = null; nevernever 表示永远不存在的类型 12345678910111213// 返回never的函数必须存在无法达到的终点function error(message: string): never { throw new Error(message);}// 推断的返回值类型为neverfunction fail() { return error(&quot;Something failed&quot;);}// 返回never的函数必须存在无法达到的终点function infiniteLoop(): never { while (true) { }} objectobject 表示非原始类型，即除number, boolean, string, symbol, null, undefined之外的类型 12345declare function create(o: object | null): void;create({ prop: 0 }); // OKcreate(null); // OKcreate(42); // Errorcreate(\"string\"); // Error 类型推断1234let someValue: any = \"this is a string\";// 以下两种方法等价let strLength: number = (&lt;string&gt;someValue).length;let strLength: number = (someValue as string).length; 变量声明 let 比较正常的作用域与范围 var 奇怪的作用域与范围 const 不可修改 接口简介接口是TS 的一个重要的概念，可以用于结构类型检查. 123456function printLabel(labelledObj: { label: string }) { console.log(labelledObj.label);}let myObj = { size: 10, label: \"Size 10 Object\" };printLabel(myObj); 接口与go的差不多， 只要包含即可 123interface LabelledValue { label: string;} 可选属性带有可选属性的接口与普通的接口定义差不多，只是在可选属性名字定义的后面加一个?符号。 1234interface SquareConfig { color?: string; width?: number;} 可选属性的好处之一是可以对可能存在的属性进行预定义，好处之二是可以捕获引用了不存在的属性时的错误。 比如，我们故意将 createSquare里的color属性名拼错，就会得到一个错误提示： 123456789101112131415interface SquareConfig { color?: string; width?: number;}function createSquare(config: SquareConfig): { color: string; area: number } { let newSquare = {color: &quot;white&quot;, area: 100}; if (config.clor) { // Error: Property &apos;clor&apos; does not exist on type &apos;SquareConfig&apos; newSquare.color = config.clor; } return newSquare;}let mySquare = createSquare({color: &quot;black&quot;}); 只读属性1234567891011121314151617181920212223interface Point { readonly x: number; readonly y: number;}let p1: Point = { x: 10, y: 20 };p1.x = 5; // error!let a: number[] = [1, 2, 3, 4];let ro: ReadonlyArray&lt;number&gt; = a;ro[0] = 12; // error!ro.push(5); // error!ro.length = 100; // error!a = ro; // error!a = ro as number[];interface SquareConfig { color?: string; width?: number; [propName: string]: any;} 函数类型123456789interface SearchFunc { (source: string, subString: string): boolean;}let mySearch: SearchFunc;mySearch = function(src: string, sub: string): boolean { let result = src.search(sub); return result &gt; -1;} 可索引的类型TypeScript支持两种索引签名：字符串和数字。 可以同时使用两种类型的索引，但是数字索引的返回值必须是字符串索引返回值类型的子类型。 这是因为当使用 number来索引时，JavaScript会将它转换成string然后再去索引对象。 也就是说用 100（一个number）去索引等同于使用”100”（一个string）去索引，因此两者需要保持一致。 123456789101112131415interface StringArray { [index: number]: string;}let myArray: StringArray;myArray = [\"Bob\", \"Fred\"];let myStr: string = myArray[0];interface StringArray { [index: number]: string;}let myArray: StringArray;myArray = [\"Bob\", \"Fred\"];let myStr: string = myArray[0]; 最后，你可以将索引签名设置为只读，这样就防止了给索引赋值 12345interface ReadonlyStringArray { readonly [index: number]: string;}let myArray: ReadonlyStringArray = [\"Alice\", \"Bob\"];myArray[2] = \"Mallory\"; // error! 类类型123456789101112interface ClockInterface { currentTime: Date; setTime(d: Date);}class Clock implements ClockInterface { currentTime: Date; setTime(d: Date) { this.currentTime = d; } constructor(h: number, m: number) { }} constructor存在于类的静态部分，所以不在检查的范围内 1234567891011121314151617181920212223242526interface ClockConstructor { new (hour: number, minute: number): ClockInterface;}interface ClockInterface { tick();}function createClock(ctor: ClockConstructor, hour: number, minute: number): ClockInterface { return new ctor(hour, minute);}class DigitalClock implements ClockInterface { constructor(h: number, m: number) { } tick() { console.log(\"beep beep\"); }}class AnalogClock implements ClockInterface { constructor(h: number, m: number) { } tick() { console.log(\"tick tock\"); }}let digital = createClock(DigitalClock, 12, 17);let analog = createClock(AnalogClock, 7, 32); 继承接口1234567891011interface Shape { color: string;}interface Square extends Shape { sideLength: number;}let square = &lt;Square&gt;{};square.color = \"blue\";square.sideLength = 10; 接口继承类当接口继承了一个类类型时，它会继承类的成员但不包括其实现。 就好像接口声明了所有类中存在的成员，但并没有提供具体实现一样。 接口同样会继承到类的private和protected成员。 这意味着当你创建了一个接口继承了一个拥有私有或受保护的成员的类时，这个接口类型只能被这个类或其子类所实现 123456789101112131415161718192021222324class Control { private state: any;}interface SelectableControl extends Control { select(): void;}class Button extends Control implements SelectableControl { select() { }}class TextBox extends Control { select() { }}// 错误：“Image”类型缺少“state”属性。class Image implements SelectableControl { select() { }}class Location {} TypeScript 的类typescrpit 的类与java差不多，默认为Public， 有Protcected与private 成员， 也有抽象类，多态与继承readonly关键字将属性设置为只读的。 只读属性必须在声明时或构造函数里被初始化。我们也可以创建类的静态成员，这些属性存在于类本身上面而不是类的实例上。 12345678910class Point { x: number; y: number;}interface Point3d extends Point { z: number;}let point3d: Point3d = {x: 1, y: 2, z: 3}; 类定义会创建两个东西：类的实例类型和一个构造函数。 因为类可以创建出类型，所以你能够在允许使用接口的地方使用类。 函数函数类型 123456 // myAdd has the full function typelet myAdd = function(x: number, y: number): number { return x + y; };// The parameters `x` and `y` have the type numberlet myAdd: (baseValue: number, increment: number) =&gt; number = function(x, y) { return x + y; }; 可选参数 可选参数必须跟在必须参数后面。 如果上例我们想让first name是可选的，那么就必须调整它们的位置，把first name放在后面。 12345678910 function buildName(firstName: string, lastName?: string) { if (lastName) return firstName + \" \" + lastName; else return firstName;}let result1 = buildName(\"Bob\"); // works correctly nowlet result2 = buildName(\"Bob\", \"Adams\", \"Sr.\"); // error, too many parameterslet result3 = buildName(\"Bob\", \"Adams\"); // ah, just right 默认参数 与普通可选参数不同的是，带默认值的参数不需要放在必须参数的后面。 如果带默认值的参数出现在必须参数前面，用户必须明确的传入 undefined值来获得默认值。 12345678 function buildName(firstName = \"Will\", lastName: string) { return firstName + \" \" + lastName;}let result1 = buildName(\"Bob\"); // error, too few parameterslet result2 = buildName(\"Bob\", \"Adams\", \"Sr.\"); // error, too many parameterslet result3 = buildName(\"Bob\", \"Adams\"); // okay and returns \"Bob Adams\"let result4 = buildName(undefined, \"Adams\"); // okay and returns \"Will Adams\" 可变数量参数（剩余参数） 剩余参数会被当做个数不限的可选参数。 可以一个都没有，同样也可以有任意个。 编译器创建参数数组，名字是你在省略号（ …）后面给定的名字，你可以在函数体内使用这个数组 12345 function buildName(firstName: string, ...restOfName: string[]) { return firstName + \" \" + restOfName.join(\" \");}let buildNameFun: (fname: string, ...rest: string[]) =&gt; string = buildName; this this和箭头函数 1234567891011121314151617 let deck = { suits: [\"hearts\", \"spades\", \"clubs\", \"diamonds\"], cards: Array(52), createCardPicker: function() { return function() { let pickedCard = Math.floor(Math.random() * 52); let pickedSuit = Math.floor(pickedCard / 13); return {suit: this.suits[pickedSuit], card: pickedCard % 13}; } }}let cardPicker = deck.createCardPicker();let pickedCard = cardPicker();alert(\"card: \" + pickedCard.card + \" of \" + pickedCard.suit); 可以看到createCardPicker是个函数，并且它又返回了一个函数。 如果我们尝试运行这个程序，会发现它并没有弹出对话框而是报错了。 因为 createCardPicker返回的函数里的this被设置成了window而不是deck对象。 因为我们只是独立的调用了 cardPicker()。 顶级的非方法式调用会将 this视为window。 （注意：在严格模式下， this为undefined而不是window）。 为了解决这个问题，我们可以在函数被返回时就绑好正确的this。 这样的话，无论之后怎么使用它，都会引用绑定的‘deck’对象。 我们需要改变函数表达式来使用ECMAScript 6箭头语法。 箭头函数能保存函数创建时的 this值，而不是调用时的值： 123456789101112131415161718 let deck = { suits: [\"hearts\", \"spades\", \"clubs\", \"diamonds\"], cards: Array(52), createCardPicker: function() { // NOTE: the line below is now an arrow function, allowing us to capture 'this' right here return () =&gt; { let pickedCard = Math.floor(Math.random() * 52); let pickedSuit = Math.floor(pickedCard / 13); return {suit: this.suits[pickedSuit], card: pickedCard % 13}; } }}let cardPicker = deck.createCardPicker();let pickedCard = cardPicker();alert(\"card: \" + pickedCard.card + \" of \" + pickedCard.suit); 你可以也看到过在回调函数里的this报错，当你将一个函数传递到某个库函数里稍后会被调用时。 因为当回调被调用的时候，它们会被当成一个普通函数调用， this将为undefined。 稍做改动，你就可以通过 this参数来避免错误。首先，库函数的作者要指定 this的类型： 12345678910111213 interface UIElement { addClickListener(onclick: (this: void, e: Event) =&gt; void): void;}class Handler { info: string; onClickGood(this: void, e: Event) { // can't use this here because it's of type void! console.log('clicked!'); }}let h = new Handler();uiElement.addClickListener(h.onClickGood); 因为onClickGood指定了this类型为void，因此传递addClickListener是合法的。 当然了，这也意味着不能使用 this.info. 如果你两者都想要，你不得不使用箭头函数了： 1234 class Handler { info: string; onClickGood = (e: Event) =&gt; { this.info = e.message }} 这是可行的因为箭头函数不会捕获this，所以你总是可以把它们传给期望this: void的函数。 缺点是每个 Handler对象都会创建一个箭头函数。 另一方面，方法只会被创建一次，添加到 Handler的原型链上。 它们在不同 Handler对象间是共享的。 泛型 12345 function identity&lt;T&gt;(arg: T): T { return arg;}let output = identity&lt;string&gt;(\"myString\"); // 显式声明let output = identity(\"myString\"); // 自动推断 使用泛型变量1234567891011121314function loggingIdentity&lt;T&gt;(arg: T): T { console.log(arg.length); // Error: T doesn't have .length return arg;}function loggingIdentity&lt;T&gt;(arg: T[]): T[] { console.log(arg.length); // Array has a .length, so no more error return arg;}function loggingIdentity&lt;T&gt;(arg: Array&lt;T&gt;): Array&lt;T&gt; { console.log(arg.length); // Array has a .length, so no more error return arg;} 泛型类型泛型接口123456789interface GenericIdentityFn { &lt;T&gt;(arg: T): T;}function identity&lt;T&gt;(arg: T): T { return arg;}let myIdentity: GenericIdentityFn = identity; 泛型类12345678class GenericNumber&lt;T&gt; { zeroValue: T; add: (x: T, y: T) =&gt; T;}let myGenericNumber = new GenericNumber&lt;number&gt;();myGenericNumber.zeroValue = 0;myGenericNumber.add = function(x, y) { return x + y; }; 1234567891011121314151617181920212223242526class BeeKeeper { hasMask: boolean;}class ZooKeeper { nametag: string;}class Animal { numLegs: number;}class Bee extends Animal { keeper: BeeKeeper;}class Lion extends Animal { keeper: ZooKeeper;}function createInstance&lt;A extends Animal&gt;(c: new () =&gt; A): A { return new c();}createInstance(Lion).keeper.nametag; // typechecks!createInstance(Bee).keeper.hasMask; // typechecks! 交叉类型（Intersection Types）12345678910111213141516171819202122232425262728function extend&lt;T, U&gt;(first: T, second: U): T &amp; U { let result = &lt;T &amp; U&gt;{}; for (let id in first) { (&lt;any&gt;result)[id] = (&lt;any&gt;first)[id]; } for (let id in second) { if (!result.hasOwnProperty(id)) { // 两个类型如果这个没有就去另外一个 (&lt;any&gt;result)[id] = (&lt;any&gt;second)[id]; } } return result;}class Person { constructor(public name: string) { }}interface Loggable { log(): void;}class ConsoleLogger implements Loggable { log() { // ... }}var jim = extend(new Person(\"Jim\"), new ConsoleLogger());var n = jim.name;jim.log(); 联合类型（Union Types）联合类型与交叉类型很有关联，但是使用上却完全不同。 偶尔你会遇到这种情况，一个代码库希望传入 number或 string类型的参数。 例如下面的函数： 12345678910111213141516/** * Takes a string and adds \"padding\" to the left. * If 'padding' is a string, then 'padding' is appended to the left side. * If 'padding' is a number, then that number of spaces is added to the left side. */function padLeft(value: string, padding: any) { if (typeof padding === \"number\") { return Array(padding + 1).join(\" \") + value; } if (typeof padding === \"string\") { return padding + value; } throw new Error(`Expected string or number, got '${padding}'.`);}padLeft(\"Hello world\", 4); // returns \" Hello world\" 导入导出export defaultimport xxx from 1234567class ZipCodeValidator implements StringValidator { isAcceptable(s: string) { return s.length === 5 &amp;&amp; numberRegexp.test(s); }}export { ZipCodeValidator };export { ZipCodeValidator as mainValidator }; 命名空间三斜线三斜线指令是包含单个XML标签的单行注释。 注释的内容会做为编译器指令使用。 三斜线指令仅可放在包含它的文件的最顶端。 一个三斜线指令的前面只能出现单行或多行注释，这包括其它的三斜线指令。 如果它们出现在一个语句或声明之后，那么它们会被当做普通的单行注释，并且不具有特殊的涵义。 1/// &lt;reference no-default-lib=\"true\"/&gt; 这个指令把一个文件标记成默认库。 你会在 lib.d.ts文件和它不同的变体的顶端看到这个注释。 模块导入相对 vs. 非相对模块导入 相对导入是以/，./或../开头的。 下面是一些例子： 123import Entry from \"./components/Entry\";import { DefaultHeaders } from \"../constants/http\";import \"/mod\"; 相对导入的模块是相对于导入它的文件进行解析的。 因此 /root/src/folder/A.ts文件里的import { b } from “./moduleB”会使用下面的查找流程： /root/src/folder/moduleB.ts /root/src/folder/moduleB.d.ts所有其它形式的导入被当作非相对的。 下面是一些例子：12import * as $ from \"jQuery\";import { Component } from \"@angular/core\"; 有一个对moduleB的非相对导入import { b } from “moduleB”，它是在/root/src/folder/A.ts文件里，会以如下的方式来定位”moduleB”： 12345678/root/src/folder/moduleB.ts/root/src/folder/moduleB.d.ts/root/src/moduleB.ts/root/src/moduleB.d.ts/root/moduleB.ts/root/moduleB.d.ts/moduleB.ts/moduleB.d.ts TypeScript如何解析模块TypeScript是模仿Node.js运行时的解析策略来在编译阶段定位模块定义文件。 因此，TypeScript在Node解析逻辑基础上增加了TypeScript源文件的扩展名（ .ts，.tsx和.d.ts）。 同时，TypeScript在 package.json里使用字段”types”来表示类似”main”的意义 - 编译器会使用它来找到要使用的”main”定义文件。","link":"/2019/05/18/langs/fe/typescript/"},{"title":"java 开发环境的配置","text":"JDKIDE工具：maven框架自动项目生成工具 initializr spring mvcspring bootorm mybatis","link":"/2016/03/13/langs/java/develop/"},{"title":"简谈MySQL报警","text":"报警报警时我们保证稳定性的最后一道坎， 如果进入了报警的阶段，应用离Down掉就不远了。 报警也是我们保障稳定性的一个重要的环节。对于报警的设置是比较考究的， 下面主要会分析一下报警的几个方面。 报警的及时性 ★★★★★对于报警，笔者认为最应该优先保障的就是及时性， 试想一个应用已经被用户反馈挂了的情况下， 这个时候就算不用报警，开发者应该也已经知道了。 这个时候报警存在的意义就是0了。因此报警最重要的一个特性就是要能及时地反馈问题，让应用开发者能够在用户发现之前发现问题，处理问题和解决问题。 报警的准确性 ★★★★☆很多人会碰到收到的报警太多的情况， 也有很多人会碰到收不到报警的情况。这两种情况基本上来讲都是不太正常的， 当然不排除后者是在应用非常良好的情况下，有一堆其他措施能够保证不需要走到报警这个环节。对于过多的报警，很多人往往容易麻木， 这样就容易造成人们心理的一种懈怠， 反正之前报警什么事情也不会发生， 现在报警应该也不会发生什么事情，一旦到了关键时候，警也报了，但没有相应的人员去处理，最终酿成应用挂掉影响用户的惨剧。 所以报警不光是要能及时， 也要能准确， 能够区分报警的优先级别，用不同的方式来警告开发者， 并正确反馈情况。 报警的渠道和高可用 ★★★★☆说到报警的高可用， 除了报警服务本身的架构之外，本文更关注的是多渠道。因为任何渠道都有阻塞或者down掉的可能性， 而且不同的渠道的触达及时性不同。一般而言，电话报警的触达及时性是最高的，有电话大家往往能够及时看到。 其次，短信也是一个报警经常选择的手段， 因为短信也是会在比较短的时间内可以被开发者察觉到， 而且手机是随身携带的，不管在上班还是下班都能相对及时的收到。第三开发者往往会使用一些聊天软件如企业微信与钉钉， 这些软件在开发者使用电脑的时候可以比较及时的收到，因为性格不同， 一般开发者下班不太会关注这类软件的通知与消息，因此这类软件的有效性只能排到第三。最后一个也是一个很常见的手段：邮件。 邮件对于大家来说都不陌生， 但邮件的实时性一般会比较差一点， 但邮件有一个好处：可以沉淀比较多的内容，一起发送。 不同的渠道的作用与及时性也不一样，一般报警会选择多渠道结合， 科学合理的利用与选择这些渠道。 报警的科学性与智能性 ★★★☆☆最后一点也是不太好做的一点， 但一旦做好就会让人感觉比较舒服。试想一下，利用机器学习的方式，回顾监控需要关注的各个指标，以及当时的处理情况，通过与往期数据的结合分析， 最终做出决定是否要报警。 这类服务一般的研发成本也比较高， 但它确实可以让人感觉比较有用，比较舒服。 如果做的好， 可以在各项指标都还正常的情况下，会更早的报警，更早的反馈问题， 可以留给开发者更多的时间处理问题，这在极大程度上能减少损失；另外这类报警一般也可以比较人性化一点： 比如可以根据报警的优先级以及时间段决定是否去烦扰正在熟睡中的开发者。总之这个领域可以做的比较多，但做的好往往会要求比较高的投入与比较精致的设计。","link":"/2016/08/13/storage/mysql/alert/"},{"title":"索引优化","text":"简介索引是存储引擎用于快速定位记录的数据结构, 索引的优化对于性能优化是最有效的手段，良好的索引会让数据库的查询性能得到成百倍甚至千倍的提升，不好的索引也会导致数据库查询效率急剧下降。我们应该学会如何正确使用索引，来优化我们的查询，提高对索引的认知，避免MySQL 索引的误区。下面主要分索引类型、索引策略、索引使用的原则与经验来介绍MySQL索引。 ##索引类型 Normal 普通索引最常见的索引， 一般都用此索引. Unique 唯一索引需保证此索引对应的数据没有重复项，可以用此项来保证一些约束条件，防止重复数据 ###Full Text 全文索引FULLTEXT 用于搜索很长一篇文章的时候，效果最好。但总体来讲，效率肯定不高， 使用的时候需要用分词工具，来转换成全文索引支持的格式方可正常使用，用在比较短的文本，如果就一两行字的，普通的 INDEX 也可以。 索引方法B-TreeB tree 索引是我们最常见，最常用的索引，在InnoDb 里，它的实际实现是基于B+ Tree的。B-tree 索引可以用于范围取值，精确匹配， JOIN, GROUP BY, ORDER BY, 应用范围非常广泛， 下面是一些例子：全值匹配 如匹配 username, email匹配列前缀 如匹配email 的 @前面的部分匹配范围值 匹配数字类型, 日期类型， 比如 user.age between 12 and 20精确匹配某一列并者范围匹配另外一列 如 user.name =‘John’ AND user.age between 12 and 20 HashHash索引，基于Hash表实现，只有精确匹配所有列的查询才有效实现：基于Hash 表实现，对所索引的列计算hash值，并索引。优点： 结构紧凑，对于精确匹配所有列的查询，非常高效。缺点：只能精确匹配所有列的值， 部分类型不适用于范围查找, Order By等， 应用范围比较窄。INNODB 是不支持不是unique 的字段上加 Hash索引的。实测，如果类型是Normal 把type 改成Hash 是没用的 索引可以大大减少服务器所需要扫描的数据数量，帮助服务器避免排序和临时表，将随机IO变成顺序IO但索引不是万能的解决方案，不是越多越好，不恰当的索引会引起性能的下降，对于数据量表的小不如全表扫描好 索引策略单列索引单列索引是指在某一个列上建立的索引，是最常见的一种索引，一般用在一些区分性比较大的列上，比如username, 某个关联的ID 联合索引联合索引是在多个列上一起建立的索引， 也是我们常见的一种索引，当有多个索引做相交的时候，通常意味着需要一个联合索引，而不是每列都建立一个索引。但联合索引的作用范围尤其要注意，下图是一个基本的说明 下图是一个实际使用的例子 聚簇索引聚簇索引不是一种索引类型，而是一种数据存储方式。InnoDb 的实现是在一个数据结构中保存了B-tree 索引与数据行。在使用得当的情况下可以大大提高SELECT效率，但也有很大的弊端， 一张表只能有一个聚簇索引， 聚簇索引不应该包含经常修改的列候选列： 主键，JOIN的列， 不常修改的列 GROUP BY 与ORDER BY 中用的列语法： 1CREATE CLUSTER INDEX xxxx 覆盖索引覆盖索引不是一种索引，而是一种索引建立的方式。简单的来说， 你需要查的数据都在，索引所在的列内， 这样就是覆盖索引。索引的条目数，往往远小于数据行数，如果只读取索引不读取数据行，会极大的提高性能, 因为索引是按照顺序排列的，所以IO消费会少的多，以下情况覆盖索引有可能无法起作用： 没有任何索引能够覆盖这个查询 语句中有用了 LIKE 之类的操作下面则是一个使用覆盖索引的例子， 注意Extra字段里面会不一样 原则与经验 不能一味的乱建索引，索引与查询优化相结合 索引要尽量找选择性比较高的列 避免无用索引与重复索引 索引应建立在经常被查询的列上 可预期的小型表不应建立索引 避免选择大型数据类型的列做为索引 要在经常用作过滤器的列上建索引 在经常Group By Order By JOIN 的列上建索引","link":"/2016/07/13/storage/mysql/index/"},{"title":"常见的MySQL客户端","text":"驱动/程序连接java如果你是maven项目，则只需要引入以下依赖 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt;&lt;/dependency&gt; 如果不是， 可以下载Jar包直接使用 go1go get -u github.com/go-sql-driver/mysql 对于go项目直接引入依赖即可 其他由于作者我不熟悉其他语言， 不在此赘述其他语言的客户端驱动是怎么样的，但是对于大部分语言来说都有比较知名的驱动。 ##桌面客户端 MySQL Workbench (Official)MySQL 是官方的MySQL客户端，它是开源免费的，有开源社区跟Oracle的支持，功能也是比较的完善，易用性比Navicat 稍差，功能比较强大，它自带了非常多的功能，比如数据迁移，可视化Explain， Dashboard, 及常见的查询， schema 设计等等。本文将以次工具来展示MySQL的许多功能 Navicat for MySQLNavicat是一款商业软件，其功能是非常的强大，UI与易用性也是屈指可数的，是一个被广泛盗版的软件 PhpMyAdmin一个基于Web的SQL客户端 DBForge Studio它是一个商业软件，目前存在免费版跟收费版本，收费版本功能更多一些，界面跟以前的某些版本的Visual Studio类似，功能应该也比较强大 HeidiSQL使用Pascal写的一个SQL客户端工具，它有着非常强大的功能跟非常垃圾的用户使用体验。 Sequel ProMac下的一个MySQL 客户端， 列出来只是给Mac用户多一个选择。 Eclipse与Idea 插件这个是与自己的开发环境IDE集成的一种MySQL工具， 它往往提供的功能比较简单，通常包括语句查询、Schema建立与删除、自动补全等功能。","link":"/2016/07/14/storage/mysql/clients/"},{"title":"使用Explain了解SQL语句的执行计划","text":"语法与含义语义上讲EXPLAIN 、DESCRIBE 、 DESC 是一样的但习惯上用法不太一样， DESC 经常用于描述一个表或者列的信息EXPLAIN经常用于描述一个查询执行计划 1234explain select 1;desc select 1;describe select 1;explain format=json select 1; explain_typeEXTENDED -&gt; 显示额外信息(filtered info)PARTITIONS -&gt; 显示所用的分区 (目前没用到)FORMAT -&gt; 显示结果的格式 (JSON/TRADITIONAL, 分表表示用JSON或者表格形式) Explain支持 SELECT/INSERT/UPDATE/DELETE/REPLACE 等语句 Explain 结果集中需要特别注意的列 类型 说明 ALL 全表扫描 INDEX 使用覆盖索引，且全表扫描的时候 RANGE 使用索引并且是在索引范围内查找的时候 REF 在查询中直接用索引匹配单个值的时候 EQ_REF 用索引查找，而且HIT到index的时候 CONST, SYSTEM MYSQL 可以把有些查询转换成常量比如用主键查找 一行记录 NULL 如SELECT 1 等不需要表的查询 对于上表而言， 越往下， 效率越高 KeyKey 是指MySQL最终决定回用哪个索引到这个执行计划中， 一般有没有用到索引及用到的是哪个索引就看此列。 RefREF 记录了 在Key列记录中的索引中所用的列，或常量 Rows 与 FilteredROWS列记录了查找需要查找的元素，所需要扫描的行数FILTERED的列显示的是针对表里面符合某个条件的记录的百分比所做的一个悲观的估算 Extra USING INDEX是不是用了覆盖索引 USING WHEREMYSQL会在检索行后根据WHERE条件进行过滤 USING TEMPORARY在对查询结果进行排序的时候是否用了临时表 USING FILE SORTMYSQL对查询结果进行排序，不用索引排序 MySQL Visual ExplainVisual explain 是MySQL Workbench 带的一项功能，其他工具很少会有此项功能，它可以比较直观的告诉你整个查询的执行计划，及查取过程，是分析SQL语句不可或缺的利器。在 Visual Explain中从蓝到红的效率是递减的，越偏向红效率就越低，越偏向蓝效率就越高。下图是个例子","link":"/2016/07/13/storage/mysql/explain/"},{"title":"Centos7 安装配置 mysql5.6","text":"安装123456wget http://dev.mysql.com/get/mysql57-community-release-el7-8.noarch.rpmsudo yum -y localinstall mysql57-community-release-el7-8.noarch.rpmsudo yum -y install mysql-community-serversudo systemctl start mysqldsudo systemctl enable mysqldsudo systemctl daemon-reload 找到 root 密码1vim /var/log/mysqld.log 更改root密码1set password=PASSWOR('djaskd'); 更改数据文件及binlog 的位置（统一用）12345systemctl stop mysqldmkdir -p /db/mysqlmv /var/lib/mysql /db/mysql/mv /db/mysql/mysql /db/mysql/datachown -R mysql:mysql /db/mysql my.cnf 配置123456789101112131415161718192021222324252627282930313233343536[client]port = 3306socket = /var/run/mysqld/mysqld.sock[mysqld_safe]socket = /var/run/mysqld/mysqld.socknice = 0[mysqld]user = mysqlpid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockport = 3306basedir = /usrdatadir = /db/mysql/datatmpdir = /tmpskip-external-lockingskip-name-resolvedefault-storage-engine=INNODBcharacter-set-server=utf8collation-server=utf8_general_cilower_case_table_names=1bind-address = 0.0.0.0max_allowed_packet = 16Mthread_stack = 192Kthread_cache_size = 8max_connections = 1000query_cache_limit = 1Mquery_cache_size = 16Mlog_error = /var/log/mysqld.logserver-id = 1binlog_format = ROWlog_bin = /db/mysql/mysql-bin.logexpire_logs_days = 10max_binlog_size = 500M 启动mysql server1systemctl start mysqld ubuntu 问题我想，你一定是从seo/seo.html” target=”_blank”&gt;搜索引擎搜索这个标题进来的！你一定是想改变mysql默认安装的数据目录！你已经修改了my.cnf中的datadir的值首先是查看数据库日志mysqld started[Warning] Can’t create test file xxx.lower-test[Warning] Can’t create test file xxx.lower-test/usr/libexec/mysqld: Can’t change dir to ‘/xxx’ (Errcode: 13)[ERROR] Aborting 你已经chown和chmod了数次新数据目录或者其父路径的属主和权限你无数次地试图service mysql start，或者 /etc/init.d/mysql start，以及mysql_install_db！恭喜你看见这篇文章，我在被系统坑了几个小时之后，找到了解决的方法。这个原因有二，其中任意的一个原因都会造成你被系统告知这个warning。如果你不是一个专业的linux系统安全工程师，或者你只是个PHP程序员，并没有对系统安全有深入的研究，你就不会太容易找到它的答案。第一，selinux，记得当年念书时，字符界面安装redhat（很古老的操作系统么。。。）的时候，有这么一个选项，通常大家都听取前辈的建议，改变默认值以不安装它。但如果你恰好要操作的这台机器开着selinux，它确实能够使你的mysql无法在新目标位置进行mysql_install_db的操作，并爆出标题所示的警告。一个简单的解决办法是使用命令暂时关闭selinux，以便让你的操作可以继续下去 1setenforce 0 但最好使用一个永久方法，以便在重启后继续不要这货。修改/etc/selinux/config文件中设置SELINUX=disabled ，然后重启或等待下次重启。第二，apparmor，这个坑爹货和selinux一样的坑爹，它也对mysql所能使用的目录权限做了限制在 /etc/apparmor.d/usr.sbin.mysqld 这个文件中，有这两行，规定了mysql使用的数据文件路径权限 12/var/lib/mysql/ r,/var/lib/mysql/** rwk, 你一定看到了，/var/lib/mysql/就是之前mysql安装的数据文件默认路径，apparmor控制这里mysqld可以使用的目录的权限我想把数据文件移动到/data/mysql下，那么为了使mysqld可以使用/data/mysql这个目录，照上面那两条，增加下面这两条就可以了 12/data/mysql/ r,/data/mysql/** rwk, 重启apparmor，/etc/inid.d/apparmor restart之后，就可以顺利地干你想干的事儿了！","link":"/2016/05/13/storage/mysql/install/"},{"title":"Mysql 的监控","text":"MySQL监控的必要性对于MySQL的稳定性而言，一个合理的监控是十分必要的， 监控是提前预知问题并解决问题的最后一道坎， 过了这道坎，服务或者应用基本就是在挂掉的边缘了。为了未雨绸缪，大部分比较知名的公司都回具备相对健全的监控体系。 关于监控项 MySQL的监控项有非常多， 多大几十上百项， 这些项中的大部分都是在日常应用场景中是使用不到的， 也不能帮助大家定位MySQL的监控问题。对于日常或者生产中所遇到的问题基本上都可以由下文中的常见的几项指标来定位并排查出来。 当然对于这些监控项， 不同的监控项的重要程度也不同， 因为MySQL的第一要义在于存储数据，因此对于MySQL而言，IOPS对于MySQL而言是最重要的指标。此外，很多指标也都是相互影响的，比如慢查询80%的情况下是因为IO过多，而非CPU占用过多。 健康度健康是一个相对的概念，就如一个人是不是好看， 你只能用相对量来形容，而不是绝对量。但一直以来，对于MySQL 的健康度的计算方法其实是很少有人总结的，因此本文希望能把这个健康度这个概念总结出来，并给出一定科学的依据。 一个健康度的计算规则是否合理的唯一标准是它是否能真实的反映MySQL服务的健康状况，举例子来说，假设一个MySQL服务的健康度为50分， 那么这个MySQL中运行中必然会存在可以优化的地方，比如存在慢查询，连接数稍微偏多，在这种情况下虽然服务能正常服务， 但请求到达一定量的时候就可能使得这个健康度变为0分，直接导致服务挂掉。 对于一个很健康的Mysql服务，来讲， 它应该是各项指标在合理的范围内， 不存在潜在的问题。 健康度的计算也很考究， 因为你要真实反馈问题，能够通过一个值来判断整体的情况， 那么必须就得存在一个科学的计算方法， 由于各项指标的重要程度不一， 因此我们需要对各项指标加上一个权重， 但仅仅加权重还是不够的， 因为往往某一项指标也能决定服务的存活与否， 因此我们还需要有另外一个概念 checkpoint, 在不同的checkpiont各项指标的权重应该都不一样。比如CPU在到达100%的 checkpoint的时候它的权重就应该提高， 因此引发健康度分数下降到10分以内， 甚至更低。因此我们要加的这个权重是 动态的， 根据指标的重要程度以及checkpoint 进行相应的变化。 监控指标上文讲完健康度这一概念之后， 我们知道，要想完善一个监控， 必须要对我们的监控指标项目有比较清楚的了解。下面主要简单介绍一下各个参数的意义。 IOPS由于mysql是为了存储数据而存在的， 所有的查询直接或者间接的也是在操作存储， 而存储的硬件基础是磁盘，磁盘的一个重要的监控指标就是IO。 对于MySQL而言相关的指标就是IOPS. 顾名思义， IOPS是每秒的IO次数。 对于不同类型的磁盘的IOPS的支持情况不同， 一般的机械硬盘支持的IOPS为100左右， 像云服务提供商阿里云ECS的磁盘支持的IOPS为300左右， 一般固态硬盘支持的IOPS会有3000到5000， 对于RDS而言， IOPS是标明了总量的。 IO主要是用户的查询从磁盘存取数据来用的， 所有的查询经过解析后都回最终涉及到IO操作，IO操作的多少也决定了查询的快慢，以及每秒能满足的查询数量（QPS） 因为对于不同的存储介质而言QPS是不一样的，因此我们关注IOPS的使用率会更科学一点。 CPUCPU一直以来都是各种类型的服务一项比较重要的资源， 对于MySQL而言，CPU主要用来对查询进行解析，优化， 以及聚合分析等等，有非常多的地方会使用到CPU， 当一个服务的CPU占用过高的时候必然会影响到它正常提供服务的能力， 对于MySQL而言，各种规范都是不推荐讲耗费CPU的计算放在MySQL端来做的。但要支持那么多的查询以及其他的东西往往还是要消费CPU资源， 当CPU计算一个查询所涉及到的任务的时候也必然会影响到其他任务的计算。 一般而言，CPU使用率在60%以内都还算健康的状态。最合理的使用范围是30%~60% QPSQPS (Query Per Seond)即每秒的查询数量，对于很多服务，都存在这么一个概念，如普通的业务服务每秒能支持多少次订单生成请求。QPS的高低与服务器资源的占用是正相关的。过高的QPS会导致其他资源如CPU或者IOPS等资源的占用过高。而MySQL 对于QPS的支持不是确定的， 要根据具体运行的查询状况而定， 对于慢查询而言，一般运行一个或者两个就能占用非常多的CPU和IO资源， 对于效率比较高的查询（如按主键指定查询的）， 这类查询消费的CPU与IO资源相比而言非常小， 因此这个服务就可以支持更高的QPS 慢查询慢查询是各类企业经常会遇到的一些问题， 慢查询的多少与数据库实例的健康状况也直接相关。虽然慢查询的数量与查询的时长是比较间接的指标， 但它也是不可忽略的一部分。一般优秀的程序员都会控制自己的查询时长在50ms之内， 并且考虑到随着数据量增长这样的查询是不是能够保持住这个时间量级。业界也很少有对慢查询确定非常明确的标准。不同企业，要求高低不同， 一般而言我推荐将超过100-200ms的查询列入为慢查询。 慢查询是必须要优化的对象， 如果不优化，后期会不得不为慢查询付出额外的惨痛的代价。 活跃连接数活跃连接是指在某一时刻，MySQL 与客户端之间在处理着查询请求的连接， 它可以直接反应这个MySQL服务的繁忙程度， 一般而言活跃连接越多， QPS也会越高， CPU与IO占用的也相应越多。 连接数对于mysql 的连接而言， 有活跃的，自然也有不活跃的， 这些活跃的和不活跃的连接加起来基本上就是所有的连接了。因为MySQL Service 在服务端都回设置 max_connection_num 这个参数，这个参数决定了允许所有连向这个MySQL服务的总的连接数。如果超出这个数字， MySQL服务端就会拒绝连接，客户端一般会报出 get connection failed的错误。日常环境中，不管是MySQL 还是Redis或者一些其他的知名服务， 就有连接池这个概念，连接池是为了提高连接的复用率， 降低连接的创建与销毁所消耗的资源而存在的。连接池的设置的恰当与否也往往决定了一个MySQL服务的连接的使用情况。 如果一个MySQL服务中存在大量Sleep的连接，就要考虑连接池设置的调整了。 内存内存对于很多服务来讲也是非常重要的一个资源， 对于MySQL而言相对于其他的指标而言没有那么重要，因为MySQL的内存使用率，是应用程序无法直接决定的，Mysql会在很多地方使用到内存，如查询缓存， 连接缓存， 查询解析等等， 有很多一部分是为了加快查询的速度， 也有很多部分是为了维持MySQL的自身功能。一般而言MySQL服务的内存使用分配由DBA或者云服务提供商根据业界BP已经提前设置好了，是不需要更改的。 但对于内存使用的异常情况也是需要关注的， 因为它会影响到服务的正常使用。 网络IN/OUT网络IO情况主要包括两个方面， 一个是入流量，一个是出流量, 不管什么应用， 当网络带宽被占满的时候都会出现问题。因此这个也是一个重要指标， 而指标中主要关注的还是比率。 百分比监控项对于监控项与权重的说法如下： 监控项 Check Point 权重 描述 IOPS使用率 &lt;30% 0 健康 30%~60% 1 正常 60%~80% 5 偏高 80%~95% 50 非常高 &gt;95% 90 致命 CPU使用率 &lt;30% 0 健康 30%~60% 1 一般 60%~80% 5 偏高 80%~95% 70 非常高 &gt;95% 90 致命 连接使用率 &lt;40% 0 健康 40%~60% 5 正常 60%~80% 30 偏高 80%~95% 50 非常高 &gt;95% 70 急需优化 内存使用率 &lt;70% 0 健康 70%~90% 10 偏高 &gt;90% 20 急需优化 网络出口带宽使用率 &lt;60% 0 正常 60~90% 10 偏高 &gt;90% 30 急需优化 其他监控项活跃连接数，慢查询与QPS三项根据配置的不同，支持的也不同， 通常情况下如下来分活跃连接就是客户端正在使用的连接，这类数据一般需要结合历史情况和现状来看， 如果一个数据库服务的活跃连接突然陡增，比历史同期要高好几倍（排除大促等情况），那肯定是不正常的， 有可能是应用端使用的问题， 也有可能是来自外部的攻击， 这个时候就要做相应的报警，以应对突发情况。 对于慢查询，任何公司的态度都是始终如一的， 不管如何，慢查询都是不应该出现在线上的OLTP数据库里的，因为数据库出问题最多的原因也就是慢查询， 70%左右的问题都是跟慢查询直接相关的， 慢查询可以导致CPU/IOPS等的使用剧增，往往不需要几个慢查询就能就能拖垮一个数据库。因此对于慢查询的监控也是十分必要的。但通常情况下慢查询应该也要区分对待， 否则出现的频率会非常大，容易出现报警太多别人不关注的情况。我这里推荐一下查询的区分与等级。 &lt;=50ms 正常查询，一般不需要进行优化 &gt; 50ms &amp;&amp; &lt;=100ms 稍微慢一点的查询，如果能优化可以想办法优化， 但不适宜比较大规模的调用，支持的QPS会比较低 &gt; 100ms &amp;&amp; &lt;= 300ms 一般的还好的慢查询，这类慢查询往往出现在一些用了索引的统计任务里，就算有一般也应该加上缓存， 由任务定时刷新， 后期应该考虑迁移到其他解决方案上 &gt; 300ms &lt;= 2000ms 一般的慢查询， 对于这类慢查询， 如果出现就要尽快优化掉， 而且一定要控制触发这类查询的入口，不能让用户或者攻击者可以出发这类查询。 一旦留有口子， 数据库很容易被攻击挂掉。 &gt; 2000ms 这类查询不应该出现，出现之后应该紧急优化上线。 一般很多应用开发者会经常问DBA的一个问题是这个配置的数据库能支持多少QPS， 其实这个是一个非常错误的问法。因为对于查询快慢， 数据库的支持情况是肯定不同的， 如果应用开发者只根据主键进行查询， 那一般可以支持比较高的QPS， 而且不需要很高的配置， 但如果都是慢查询， 可能支持的QPS也就只有个位数了， 所以大家一般问这类问题的时候，DBA给的值一般是按照经验，然后假定应用的查询还可以的情况下，给出的一个相对保守的值。","link":"/2016/04/13/storage/mysql/monitoring/"},{"title":"Mysql 主从同步相关的知识","text":"同步时mysql -&gt; backup 123456mkdir -p /backup/mysqltar -izxvf /backup/hins1851643_data_20170301022021.tar.gz -C /backup/mysql/cp /home/wenwen/backup-my.cnf /backup/mysql/backup-my.cnfinnobackupex --user=root --password --defaults-file=/etc/mysql/my.cnf --apply-log /backup/mysqlchown -R mysql.mysql /var/lib/mysqlchown -R mysql.mysql /backup/mysql replicate.conf 1234567891011121314151617181920212223242526272829303132333435[mysqld]server-id = 2## same on the slavelog-bin = mysql-bininnodb_buffer_pool_size=512Minnodb_flush_log_at_trx_commit = 1sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLESlog_bin_trust_function_creators=1sync_binlog = 1#binlog-format='ROW'relay-log=mysqld-relay-binread_only=1master-info-repository=filerelay-log-info_repository=filebinlog-format=ROWinnodb_checksum_algorithm=innodbinnodb_data_file_path=ibdata1:200M:autoextendinnodb_log_files_in_group=2innodb_log_file_size=524288000#relay_log_info_repository = TABLE#master_info_repository = TABLErelay_log_recovery = on### replicate settingsreplicate-ignore-db=performance_schemareplicate-ignore-db=mysqlreplicate-do-db=fenshua123#replicate-rewrite-db=\"maindata-&gt;diydata\" 1CHANGE MASTER TO MASTER_HOST='10.1.8.160', MASTER_USER='rdsinner', MASTER_PORT = 3306, MASTER_LOG_POS=120, MASTER_PASSWORD='the_password', MASTER_LOG_FILE='mysql-bin.002458'; 主从同步 SLAVE 切换命令： CHANGE MASTER TO MASTER_HOST=’10.1.6.111’, MASTER_USER=’fenshuaprod’, MASTER_LOG_POS=120, MASTER_PASSWORD=’TUzu4515’, MASTER_LOG_FILE=’mysql-bin.000001’, relay-log=mysqld-relay-bin; Master 重启在先， Slave重启在后，即可保持复制关系， 如果Slave 重启在先， 则需要在Slave 上手动 start slave；才可以维持复制关系 MySql 支持引入其他配置文件， 用 !include /filepath 即可，但注意引入的文件要标明在那个section 下面，否则很容易就会抛异常 检查relay_log_info_repository是否修改成功。 show variables where variable_name in (‘relay_log_info_repository’,’master_info_repository’); 设置表只读 lock table t_depart_info read; 设置表名忽略大小写 lower_case_table_names=1 设置库只读read-only sudo innobackupex –user=root –password –defaults-file=/data/backup-my.cnf –tables-file=/data/site.cnf –ibbackup=xtrabackup_56 –apply-log /data/mysql 1．在ECS服务器上安装MySQL，详细步骤可以参考如下：http://www.centoscn.com/mysql/2014/0924/3833.html一些关键注意点：a.数据库的版本至少为5.6.16及以上b.需要在my.cnf中配置的一些关键参数： 123456789server-id ###Slave配置需要master-info-repository=file### Slave配置需要relay-log-info_repository=file### Slave配置需要binlog-format=ROW### Slave配置需要gtid-mode=on###开启GTID需要enforce-gtid-consistency=true###开启GTID需要innodb_data_file_path=ibdata1:200M:autoextend###使用RDS的物理备份中的backup-my.cnf参数innodb_log_files_in_group=2###使用RDS的物理备份中的backup-my.cnf参数innodb_log_file_size=524288000###使用RDS的物理备份中的backup-my.cnf参数 2.MySQL安装好后，可以使用RDS提供的物理备份文件恢复到本地MySQL中，可以参考：http://help.aliyun.com/knowledge_detail/5973700.html?spm=5176.7114037.1996646101.1.7qe3ot&amp;pos=1注意：需要将备份解压后的文件backup-my.cnf中的三个参数加到启动文件中去 123innodb_checksum_algorithm=innodbinnodb_data_file_path=ibdata1:200M:autoextendinnodb_log_files_in_group=2 3.数据库启动后，开始设置本地数据库与RDS的同步关系a．reset slave;####用于重置本地MySQL的复制关系，这一步操作有可能报错： 12mysql&gt; reset slave;ERROR 1794 (HY000): Slave is not configured or failed to initialize properly. You must at least set –server-id to enable either a master or a slave. Additional error messages can be found in the MySQL error log. 原因是由于RDS的备份文件中包含了RDS的主从复制关系，需要把这些主从复制关系清理掉，清理方法： 123truncate table slave_relay_log_info;truncate table mysql.slave_master_info;truncate table mysql.slave_worker_info; 然后重启MySQL；b.SET @@GLOBAL.GTID_PURGED=’818795a2-8aa8-11e5-95b1:1-289,8da7b8ab-8aa8-11e5-95b1:1-75′;打开备份解压文件可以看到文件xtrabackup_slave_info，其中第一行就是我们需要在本地MySQL执行的命令，他表示在备份结束时刻RDS当前GTID值’c. 1234change master tomaster_host=’gtid1.mysql.rds.aliyuncs.com’,master_user=’qianyi’,master_port=3306,master_password=’qianyi’,master_auto_position=1; 设置本地MySQL与RDS的复制关系，账户qianyi是在RDS控制系统中添加（注意：同步账户不要以repl开头）；4．测试同步关系是否正常，可以在本地MySQL执行show slave status\\G查看同步状态，同时可以在RDS中插入测试一些数据，或者重启实例，观察同步情况： 123456789101112131415161718192021222324252627mysql&gt; show slave status\\G;Slave_IO_State: Queueing master event to the relay logMaster_Host: gtid1.mysql.rds.aliyuncs.comMaster_User: qianyiMaster_Port: 3306Connect_Retry: 60Master_Log_File: mysql-bin.000007Read_Master_Log_Pos: 625757Relay_Log_File: slave-relay.000002Relay_Log_Pos: 2793Relay_Master_Log_File: mysql-bin.000007 Slave_IO_Running: Yes Slave_SQL_Running: YesExec_Master_Log_Pos: 612921Relay_Log_Space: 15829 Seconds_Behind_Master: 57133Master_SSL_Verify_Server_Cert: NoMaster_Server_Id: 2319282016Master_UUID: 818795a2-8aa8-11e5-95b1-6c92bf20cfcfMaster_Info_File: /data/work/mysql/data3001/mysql/master.infoSQL_Delay: 0SQL_Remaining_Delay: NULLSlave_SQL_Running_State: Reading event from the relay logMaster_Retry_Count: 86400818795a2-8aa8-11e5-95b1-6c92bf20cfcf:17754-17811Executed_Gtid_Set: 818795a2-8aa8-11e5-95b1-6c92bf20cfcf:1-17761Auto_Position: 1 5.做好监控，由于采用MySQL的原生复制，所以可能会导致本地MySQL与RDS的复制出现中断，可以定时去探测 Slave_IO_Running和 Slave_SQL_Running两个状态值是否为yes，同时也需要关注本地MySQL与RDS的延迟： Seconds_Behind_Master。 删除slave_worker_info内容 1234mysql&gt; set global sync_binlog=20 ;Query OK, 0 rows affected (0.00 sec)mysql&gt; set global innodb_flush_log_at_trx_commit=2;Query OK, 0 rows affected (0.00 sec)innodb_flush_log_at_trx_commit 如果innodb_flush_log_at_trx_commit设置为0，log buffer将每秒一次地写入log file中，并且log file的flush(刷到磁盘)操作同时进行.该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。如果innodb_flush_log_at_trx_commit设置为1，每次事务提交时MySQL都会把log buffer的数据写入log file，并且flush(刷到磁盘)中去.如果innodb_flush_log_at_trx_commit设置为2，每次事务提交时MySQL都会把log buffer的数据写入log file.但是flush(刷到磁盘)操作并不会同时进行。该模式下,MySQL会每秒执行一次 flush(刷到磁盘)操作。注意：由于进程调度策略问题,这个“每秒执行一次 flush(刷到磁盘)操作”并不是保证100%的“每秒”。sync_binlogsync_binlog 的默认值是0，像操作系统刷其他文件的机制一样，MySQL不会同步到磁盘中去而是依赖操作系统来刷新binary log。当sync_binlog =N (N&gt;0) ，MySQL 在每写 N次 二进制日志binary log时，会使用fdatasync()函数将它的写二进制日志binary log同步到磁盘中去。注意:如果启用了autocommit，那么每一个语句statement就会有一次写操作；否则每个事务对应一个写操作。而且mysql服务默认是autocommit打开的修改参数后，slave2,3也一样可以跟上slave1的速度了","link":"/2016/04/13/storage/mysql/master_slave/"},{"title":"Mysql 如何找回Root密码","text":"1 修改 /etc/my.cnf加上 12[mysqld]skip-grant-tables 2. 重启mysql1service mysql restart 3. 更改root密码1234mysql&gt; USE mysql ; mysql&gt; UPDATE user SET Password = password ( 'new-password' ) WHERE User = 'root' ; mysql&gt; flush privileges ; mysql&gt; quit 如果版本高于5.6 则是如下字段存着密码 1authentication_string 4.将MySQL的登录设置修改回来5. 重启mysql","link":"/2019/05/13/storage/mysql/passwd/"},{"title":"Mysql 其他杂记","text":"事务查询1SELECT @@tx_isolation 12345678910111213SELECT r.`trx_id` waiting_trx_id, r.`trx_mysql_thread_id` waiting_thread, r.`trx_query` waiting_query, b.`trx_id` bolcking_trx_id, b.`trx_mysql_thread_id` blocking_thread, b.`trx_query` block_query FROM information_schema.`INNODB_LOCK_WAITS` w INNER JOIN information_schema.`INNODB_TRX` b ON b.`trx_id` = w.`blocking_trx_id` INNER JOIN information_schema.`INNODB_TRX` r ON r.`trx_id` = w.`requesting_trx_id` ; 查询外键12345678910111213select concat('alter table ',table_name,' drop foreign key ',constraint_name,';') from information_schema.key_column_usagewhere constraint_schema = 'dbname' and referenced_table_name = 'tbName';SELECT concat('alter table ',table_schema,'.',table_name,' DROP FOREIGN KEY ',constraint_name,';')FROM information_schema.table_constraintsWHERE constraint_type='FOREIGN KEY'AND table_schema='dbname';SELECT TABLE_NAME FROM information_schema.`TABLES`WHERE TABLE_TYPE=\"BASE TABLE\"AND TABLE_SCHEMA=\"dbname\" 原因是由于RDS的备份文件中包含了RDS的主从复制关系，需要把这些主从复制关系清理掉，清理方法： 123truncate table slave_relay_log_info;truncate table mysql.slave_master_info; Mysql DUMP 表1mysqldump -u root -h 127.0.0.1 dbName msgattach &gt; msgattach.sql MySQL 客户端从表现上来看分两种： 客户端执行完query之后， 直接返回， 并开始用rows.next 去取数据， 其中很快 如果在客户端取数据的时候打上断点， 服务端发送完数据之后， 客户端只能取到部分数据， 不能取到全部数据， 怀疑客户端有一个缓冲区，不断接收服务端的数据并刷新 如果客户端收到数据后，不打断点， 则可以获取全部数据（前提是建立在客户端执行速度比较快的情况下） 底层代码里面来看， mysql服务端发送数据给客户端的时候， 客户端会把数据存在一个 默认4096大小的buffer 里面， 从这个buffer里面再读到rows里面如果buffer里的数据没有被及时消费掉， 那么连接上面传送过来的数据会丢失掉。 MySQL 本身的问题对于数据量传输不完的时候有个write_timeout, 出现错误之后， 会返回EOF 相关的Error 12[mysql] 2019/04/25 14:31:27 packets.go:72: unexpected EOF[mysql] 2019/04/25 14:31:27 packets.go:393: busy buffer 实用脚本Mysql 查看表的外键1234select TABLE_NAME,COLUMN_NAME,CONSTRAINT_NAME, REFERENCED_TABLE_NAME,REFERENCED_COLUMN_NAME from INFORMATION_SCHEMA.KEY_COLUMN_USAGE WHERE CONSTRAINT_NAME != 'PRIMARY' AND REFERENCED_TABLE_NAME IS NOT NULL 测试稳定情况123for i in `seq 1 100`; do mysql -u username -h host -P 3306 -ppassword -e \"use dbName; select askid from ask limit 1\"; echo \"==============$i\"; sleep 1;done","link":"/2019/05/13/storage/mysql/mysql_other/"},{"title":"使用docker运行MySQL服务","text":"为啥要使用docker运行MySQL熟悉docker和k8s的人都知道， 我们使用容器化技术，是为了方便我们运行某个服务，我们用docker 运行Mysql， 并不是因为mysql在docker比在物理机或者kvm上运行的更好，配置起来更简单。我们使用docker 主要还是因为，我们在不关注太多MySQL本身的东西的时候， 单纯想快速简单的启动一个MySQL服务的时候能够做到，分钟级别即可完成。这相对于传统级别的从安装到配置动则半小时到几个小时的工作量来说，已经非常简单方便了。 怎么使用Docker 去运行一个MySQL服务在运行之前，你首先要装docker， 安装docker非常简单，只需要一路下一步下一步就可以完成， linux或者mac野只需要一些安装命令即可搞定。当然安装docker可以不仅仅用于MySQL用途， 也可以用来它方便其他任何你想需要运行的服务的部署和运行。 下载镜像1docker pull mysql:5.6 这里mysql是镜像名称， 5.6 是mysql的版本。 一半这些镜像都默认是比较官方维护的， 在docker没有进行特殊设置的情况下可以信任。 运行1docker run -p 3306:3306 --name mysql -v /opt/docker_v/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 -d imageID 上述命令有如下解释： -p 3306:3306 是指把容器的3306端口映射到本地的3306端口 --name 制定容器的名称, 我们下次要操作的时候可以制定名称即可， 如 docker start mysql -v 把本地文件夹与docker文件夹进行一个映射 -e 传入容器一些环境变量， 这里传入的是MySQL运行的时候需要的root密码 -d 后台方式运行 常见的问题 docker 守护进程没有开启 解决方案： 开启docker后台进程 如 service docker start或者 systemctl start docker.service mysql 运行的端口过小导致的没有权限 解决方案：请在大于1024 的端口上运行MySQL 系统环境是 Windows 10 Pro，Docker 版本 18.03.1-ce，电脑开机之后第一次运行 docker run 的时候会遇到这个错误 12C:\\Program Files\\Docker\\Docker\\Resources\\bin\\docker.exe: Error response from daemon: driver failed programming external connectivity on endpoint app (36577729ce7d4d2dddefb7fddd32521ea66958cf824138804b02ffb3c98452f3): Error starting userland proxy: mkdir /port/tcp:0.0.0.0:3306:tcp:172.17.0.2:3306: input/output error. 解决方案： 重启docker服务","link":"/2019/07/02/storage/mysql/mysql_docker/"},{"title":"Mysql 简介","text":"前言简介MySQL是一个关系型数据库管理系统，目前属于Oracle 旗下产品， 开源分支为MariaDB，它是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的免费 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件。MySQL是一种关系数据库管理系统，它所使用的 SQL 语言是用于访问数据库的最常用标准化语言, 关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。 内容概要本材料介绍内容涵盖如下几点 MySQL简单介绍及常见工具 MySQL Schema设计 MySQL 索引 Explain 的理解与使用 MySQL规范与最佳实践声明本材料不会对具体SQL语法进行讲解， 也不会涉及到MySQL运维及分库分表之类的知识，具体相关知识可以自行百度或者看书。本文针对的读者主要在于使用MySQL的研发及想要了解MySQL的研发，仅仅涉及到工作中要经常遇到的部分","link":"/2016/03/18/storage/mysql/preface/"},{"title":"Mysql 闪回方案设计","text":"mysql 数据闪回背景数据闪回属于数据恢复的一个范畴, 常见的数据恢复的方法有很多， 但都大同小异。主要有以下两种方案： 进行变更之前把变更前的数据存储下来用于恢复 基于全量备份+ binlog增量备份的方式 这两种方案，私下里操作最多的往往是第一种， 因为第一种比较小作坊， 容易操作， 但操作也相对零碎， 人工化， 容易产生失误。第二种是业界许多解决方案的标准， 例如RDS的备份恢复克隆等功能就是按照第二种方案去做的， 但第二种有三个问题， 第一，备份数据比较庞大， 比较多，恢复时间非常久， 第二， 需要有一整套的体系支持（包括不仅限于服务器、运维、等）第三，第二种需要比较大的开发量。需要对binlog 进行订阅解析以及合理的安排， 对备份策略进行合理的程序化定制。 很多企业在发展到一定阶段之后必定会对线上数据变更进行相应的管控， 建立好相应的变更管控的流程。 但这个流程也非万能的， 数据被误操作的概率还是存在， 当数据被误操作后，如何快速恢复到操作前的一个状态， 成了本文主要关注的一个话题。 闪回工具现状mysqlbinlog工具配合sed、awk该方式先将binlog解析成类SQL的文本，然后使用sed、awk把类SQL文本转换成真正的SQL。 优点：当SQL中字段类型比较简单时，可以快速生成需要的SQL，且编程门槛也比较低。 缺点：当SQL中字段类型比较复杂时，尤其是字段中的文本包含HTML代码，用awk、sed等工具时，就需要考虑极其复杂的转义等情况，出错概率很大。 给数据库源码打patch。该方式扩展了mysqlbinlog的功能，增加Flashback选项。 优点：复用了MySQL Server层中binlog解析等代码，一旦稳定之后，无须关心复杂的字段类型，且效率较高。 缺点：在修改前，需要对MySQL的复制代码结构和细节需要较深的了解。版本比较敏感，在MySQL 5.6上做的patch，基本不能用于MySQL 5.7的回滚操作。升级困难，因为patch的代码是分布在MySQL的各个文件和函数中，一旦MySQL代码改变，特别是复制层的重构，升级的难度不亚于完全重新写一个。 使用业界提供的解析binlog的库，然后进行SQL构造，其优秀代表是binlog2sql。 优点：使用业界成熟的库，因此稳定性较好，且上手难度较低。 缺点：效率往往较低，且实现上受制于binlog库提供的功能。 上述几种实现方式，主要是提供的过滤选项较少，比如不能提供基于SQL类型的过滤，需要回滚一个delete语句，导致在回滚时，需要结合awk、sed等工具进行筛选。 开源方案总体来讲如下的两种方案其实是一个原理，其中一个程序化了， 产品化了而已。 美团的 MyFlash MyFlash 是由美团点评公司技术工程部开发维护的一个回滚DML操作的工具。该工具通过解析v4版本的binlog，完成回滚操作。相对已有的回滚工具，其增加了更多的过滤选项，让回滚更加容易。 无需把binlog解析成文本，再进行转换。 提供原生的基于库、表、SQL类型、位置、时间等多种过滤方式。 支持MySQL多个版本。 对于数据库的代码重构不敏感，利于升级。 自主掌控binlog解析，提供尽可能灵活的方式。 闪回工具分析 以上业界的一些闪回方案，基本上都是基于Binlog 去倒腾的， 有一些明显的限制 不能针对某条语句(tansaction)进行回滚操作 需要维护binlog, 建立binlog 相关的逻辑体系 人工变更与应用变更不能真正分开(闪回是针对某个表， 某个类型，在某一段时间内的全部数据) 使用起来不方便， 对于普通研发来说成本还是蛮高的 … 闪回工具的方案为了解决以上所有问题， 主要考虑以下两种方案（两种都是原创） 基于SQL重写与备份的方案这种方案的整个大的流程如下： 变更前备份： 在实际执行语句前，通过对SQL语句的重写，来达到保存变更前状态的一个目的。 变更执行： 进行实际的变更操作 备份恢复： 利用备份的数据进行恢复（一般是自动生成rollback的SQL语句进行执行） 上述环节中， 最重的一部分工作就是变更前的备份，其中工作量最大的一部分就是进行SQL的重写，这有可能涉及到AST语法树的部分， 当然对于比较简单的SQL语句， 也用不着AST语法树。这里面最大的工作量就是SQL语句的重写， 利用重写的SQL进行原数据查询， 再将变更前的数据备份下来。 整体的一个流程如下： 存在的问题： 备份与实际写入之间是存在时间间隔的， 如果这个时间间隔内存在其他变数，则也只能恢复到备份的时候的状态。 基于binlog 映射SQL语句的方案对于每一个DML操作， binlog 总是忠实的记录了DML操作的详情，我们可以利用这个详情用于事后恢复数据。但binlog 并未将语句与binlog一一对应起来， 如果要做，可能需要花费的代价比较大，浪费很多的存储空间。由于我们的需求是仅对我们的变更进行回滚，不对其他同类型同时间的变更进行回滚， 所以，我们需要有能力将binlog与我们的变更进行一一对应。整体的操作流程如下： 从图中可以看到， 变更的执行跟数据的备份恢复其实是分开的， 变更执行只是变更执行，不用去卡很多时间点相关的概念， 也不同担心备份的跟实际变更的数据会不一致。避免了上面方案的一个时间差的问题，而且不需要进行SQL语句的重写与反查数据库。 为了能做变更与binlog 的唯一对应， 我们需要对变更执行这个步骤做一些手脚， 加入一个唯一对应的标志。由于binlog 是会忠实记录变更的， 因此我们可以给每个变更生成一个唯一标志， 并把这个唯一标志体现在binlog 里面， 这样我们就实现了变更与binlog的对应。有了这层对应关系我们就可以轻松的对这个变更进行数据的恢复， 从而不影响其他同时间同类型的变更。 数据备份模块也是需要投入开发的一个模块他主要涉及到以下工作： binlog 名称与位点的确定 slave 伪装 binlog 接收与解析 定位变更的binlog 由对应的binlog生成相应的备份 总结就开发量上来讲， 两种方案都是差不多的， 整个周期至少都需要2-3周左右的开发量，我们显然应该采取第二种方案， 因为第二种方案与业界许多方案有比较多的共通之处， 也是比较通用靠谱的一种方案， 而且避免了查询间隔带来的数据不一致问题。而且它与现有生态融合比较好。","link":"/2017/03/18/storage/mysql/roll_back/"},{"title":"MySQL 使用规范","text":"MySQL 使用规范目前我们数据库使用缺乏一个统一的使用规范，这篇规范是我参考阿里的部分的BP以及赶集网的SQL 军规结合酷家乐自身的情况制定的。 约定 数据库操作与使用一定要按照规范进行 所有的变更必须提前4小时提交申请，进过审批后才能执行操作 全网变更必须经过线下测试，线上小规模验证后，才能全网推送 数据订正和数据提取必须经过团队leader审核通过后才能进行操作； 禁止未经正式审批进行查阅，变更，传播，移动线上数据； 禁止对无关人员提供系统登录和发布权限； 重大变更（数据库停机，扩容，迁移）必须团队review 新发布的任何sql都必须进过严格的审核，添加上必要的索引 线上的操作务必谨慎，必须在测试环境中完全验证后才能到线上执行，同时需要必要的数据备份 禁止在非变更窗口执行变更， 添加索引或者添加字段的操作， 都需要在业务的低峰期进行 禁止在程序端显式加锁 统一字符集为UTF8, 排序规则为 UTF-generic-ci 统一命名规范 表名一律用大小写，schema设计避免关键字（MYSQL 关键字列表），索引使用同一前缀 index_ + colname 用下划线分割核心 尽量不在数据库做运算， 把MySQL作为一个存储组件 在设计表之前就把表将来的使用场景考虑清楚，做好相应准备 控制单表数据量 保持表身段苗条 平衡范式使用，不冗余非必要字段 拒绝3B： BIG SQL) 大事务 (BIG Transaction) 大批量 (BIG Batch)Schema设计 INNODB 引擎是你的最佳选择 用好数值字段类型 将字符转化为数字， 如时间采用bigint, IP 采用 Unsigned Int， Enum 转为tinyint 比较小的表，通常情况下数据量小于5000的表，在未来的预期内也不会增加到比较大的，不用建立索引。 避免使用NULL字段 不在在数据库里存图片 谨慎合理添加索引， 不要每个查询的字段都建立索引 字符字段必须建前缀索引 不在索引列做运算 自增列或全局ID做INNODB主键 尽量不用外键 自增主键是你的最佳选择 新建表时总是设计添加冗余字段SQL 语句类 SQL语句尽可能简单 保持事务(连接)短小 尽可能避免使用SP/TRIG/FUNC 尽量不用 SELECT * 改写OR语句 避免负向查询和% 前缀模糊查询 减少COUNT(*) LIMIT的高效分页 可以用where pk &gt; 之后limit num 避免单次数据库操作涉及行数过多的查询，此类查询没有加limit的都应该warning 尽量不使用UNION， 在代码里面实现Union 分解联接保证高幵发 GROUP BY 去除排序 order by,确认排序字段是否用到索引，避免filesort 在Update 操作的时候尽量不用 SELECT， 避免造成在此类操作的时候锁表 同数据类型的列值比较 Load data导数据 打散大批量更新尽量不用 INSERT … SELEC SQL 中 JOIN不超过两张表 Know Your SQL!","link":"/2016/02/13/storage/mysql/standards/"},{"title":"redis 简谈redis监控","text":"简介由于redis是一个内存型的数据库，关注的点势必跟MySQL等非内存型的数据库相差比较多， 因此有必要单独对redis的监控关注项进行梳理。此外由于redis的单线程模型， 以及速度要求非常高，因此对于redis的监控需要因事制宜。 监控项内存由于redis是内存型数据库，因此对于内存的需求是最大的需求， 监控项里面最重要的也是对于内存的监控，一般出问题，大概率是内存满掉的问题。一般内存满了之后会导致一系列的问题，比如逐出了不该逐出的key、写不进数据、 超时阻塞等问题。 对于内存的监控是至关重要的。一般一个合理的范围是在 30% 到70%之间。而超过了80%就需要报警和升级了。 12345678910111213141516171810.1.9.164:7300&gt; info memory# Memoryused_memory:108476200used_memory_human:103.45Mused_memory_rss:129523712used_memory_rss_human:123.52Mused_memory_peak:109672760used_memory_peak_human:104.59Mtotal_system_memory:16658898944total_system_memory_human:15.51Gused_memory_lua:37888used_memory_lua_human:37.00Kmaxmemory:4294967296maxmemory_human:4.00Gmaxmemory_policy:volatile-lrumem_fragmentation_ratio:1.19mem_allocator:jemalloc-4.0.310.1.9.164:7300&gt; 大Key， 慢查询另外一个比较常见的导致线上问题的一大因素是对于redis的不合理的使用，如 大key与慢查询一般而言redis的key与数据越大， 导致的查询时长就越长， 加上redis本身是单线程的模型，因此这类查询往往会影响到其他查询的正常进行， 对大key的控制一方面可以通过中间件或者proxy 等手段来截断或者拒绝， 一方面也是需要使用一定的手段如 redis的 slowlog get命令来查看。有了能获取的方法，那自然就可以对这些东西进行监控并合理的进行报警。 一般来讲redis模型中， 还是以尽量不出现慢查询为宜， 一旦出现慢查询就应该立即报警。 123456789101112131410.1.9.164:7300&gt; slowlog get1) 1) (integer) 155 2) (integer) 1545025300 3) (integer) 19422 4) 1) &quot;COMMAND&quot;2) 1) (integer) 154 2) (integer) 1544754674 3) (integer) 11571 4) 1) &quot;scan&quot; 2) &quot;26005&quot; 3) &quot;MATCH&quot; 4) &quot;*&quot; 5) &quot;COUNT&quot; 6) &quot;10000&quot; 连接数redis在与客户端进行通信也是维护了连接的， 这些连接用来处理服务端与redis server之间的命令发送与数据发送等等 另外redis的最大连接数是有上限的 可以通过命令 config get maxclients来取得， 也可以使用info clients来看连接的使用情况，一般而言，连接数只要使用不满其实是不会出现太多的问题的， 但一定用满了，则会导致应用端错误。 因此对于连接数的监控也是以比率为标准比较好， 建议合理的范围是 &lt; 60%, 对于超过80%的情况应给予报警 123456789101112127.0.0.1:6379&gt; info clients#Clientsconnected_clients:621client_longest_output_list:0client_biggest_input_buf:0blocked_clients:0127.0.0.1:6379&gt;127.0.0.1:6379&gt; CONFIG GET maxclients ##1) &quot;maxclients&quot; ##2) &quot;10000&quot;127.0.0.1:6379&gt; CPUCPU对于redis来说也是比较重要的需要关注的一个项， 一般跟QPS直接相关， QPS越高，CPU使用的也就越高。对于redis服务来讲， CPU 跟其他服务的要求差别也不大， 不正常区间是 &lt;70% 超过80%则需要报警。 123456710.1.9.164:7300&gt; info cpu# CPUused_cpu_sys:88088.66used_cpu_user:39641.02used_cpu_sys_children:40.78used_cpu_user_children:131.6810.1.9.164:7300&gt; QPSQPS 即每秒钟Redis接受的请求数量，不同的Key的大小支持的QPS对于Redis是不同的， 因此单一的对redis的QPS进行监控可能意义不大。如果要做redis QPS的监控应该与历史同期相比较，是否有剧增等情况。 还有一个可以参照点就是经验值， 一般这个配置的redis可以承受多少的QPS。 通过经验值与历史对比两个标准来决定是否需要报警。一般是历史同期的两倍，且比高峰期高的时候，是应该报警的。 QPS 可以通过计算两个时间间隔内执行的总命令数量来计算出来 1234567891011121314151617181920212210.1.9.164:7300&gt; info stats# Statstotal_connections_received:215774total_commands_processed:2980658450instantaneous_ops_per_sec:50total_net_input_bytes:1135673367887total_net_output_bytes:594951090244instantaneous_input_kbps:30.66instantaneous_output_kbps:13.65rejected_connections:0sync_full:0sync_partial_ok:0sync_partial_err:0expired_keys:161591evicted_keys:0keyspace_hits:112552664keyspace_misses:809125690pubsub_channels:0pubsub_patterns:0latest_fork_usec:3349migrate_cached_sockets:010.1.9.164:7300&gt; 网络IO网络IO就是redis在处理命令的时候占用的网络的流入与流出的量， 一般而言， 网络IO是较少关注的一个点， 但在网络达到带宽的上限的时候我们还是应该重视起来， 因为这个时候其他的东西是会阻塞的， 一般推荐的报警值为超过网络带宽的 80%","link":"/2017/12/13/storage/redis/monitoring/"},{"title":"MySQL Schema 设计","text":"Schema 简介说到使用MySQL我们接触最早的就是Schema设计，俗称建表，这个小节主要介绍MySQL Schema的设计方法与一些基本的使用原则。 Schema 设计在用户应用设计前期是非常重要的，一般情况下它会影响到业务以后的健康程度，以及其他业务代码的设计，Schema一旦设计成型并投入使用，当数据量达到一定程度的时候将会对索引的要求会越来越高，变更Schema也将会花费更多的代价，应用设计之前，认真正确的设计Schema是非常有必要的。 下面将主要分Schema的数据类型，索引与外键，使用原则，默认值等来介绍Schema的设计，在此之前，下面是一个最常见的一个schema设计语句。 1234567891011121314CREATE TABLE `userinfo` (`id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键' ,`name` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT '名字' ,`gender` bit NULL DEFAULT NULL COMMENT '性别 0 男 1 女 NULL 未知' ,`height` float UNSIGNED NULL DEFAULT NULL COMMENT '身高， 单位m' ,`account_id` int UNSIGNED NOT NULL COMMENT '账户ID' ,`created` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '记录创建时间' ,`last_modfied` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '记录最后修改时间' ,PRIMARY KEY (`id`),INDEX `idx_name` (`name`) USING BTREE COMMENT '用于按姓名查找',UNIQUE INDEX `idx_account_id` (`account_id`) USING BTREE COMMENT '用于对应某个账户的唯一ID',INDEX `idx_heigt_gender` (`height`, `gender`) USING BTREE COMMENT '用户筛选用户特征')DEFAULT CHARACTER SET=utf8 COLLATE=utf8_general_ci; 数据类型整形数据： TINYINT, SMALLINT, MEDIUMINT, INT BIGINT整形的数据类型一旦类型确定长度也就确定了，所以之前文章里面给定的那个长度是不起实际作用的，可以不指定长度只指定类型。下表是各种整形所对应的长度| 数据类型 | 位数 | 范围 ||———–|—-|———————————————————-|| TINYINT | 8 | -2^7 到 2^7-1 (-128-127) || SMALLINT | 16 | -2^15到2^15-1 (-32768-32767) || MEDIUMINT | 24 | -2^23到2^23-1 (-8388608-8388607) || INT | 32 | -2^31到2^31-1 (-2147483648-2147483647) || BIGINT | 64 | -2^63到2^63-1 (-9223372036854775808-9223372036854775807) |应用场景：INT 通常用作一个实体表的主键部分，比如用户表的用户IDTINYINT 通常用作枚举类型的数据，需要很小长度比如 status/type/deleted通常会根据一个数据将来可能的最大量来选择类型数据， 一般使用前会做一个预估来选择数据类型。 实数型数据： DECIMAL， FLOAT， DOUBLEDECIMAL 可以用于存储比BIGINT 更大的整数，计算方法MYSQL自行实现FLOAT与DOUBLE 都可以指定精度应用场景：实数类型通常应用与需求精度非常高的字段可能的改进方法：在存储面积，身高等需要精度低的领域，在最大值可预估的时候，可以用整形来存取 字符串类型 VARCHAR CHAR TEXT BLOBVARCHAR可变长字符串， 需要额外字节来存储当前的长度, 用于存储，改动不太频繁，且长度变化范围比较大的字符串CHAR 固定长字符串 用于存储相对固定的东西，如密码的MD5，以及经常要改变的列TEXT与BLOB 分别是指用字符存储与二进制的存取可能的改进：如果一个列的字符串的数量基本是确定的， 比如性别： Male Female Unknown 只有固定的几种String 或者是可以遇见的几种，尽量可以使用tinyint 来代替常用函数 123CHAR_LENGTSTR_CMPSTR_TO_DATE 日期类型 TIMESTAMP, DATETIMEMYSQL支持毫秒级别的时间， MariaDb 支持微秒级的时间值DATETIME (1001-9999) 时区无关 使8Byte的存储空间TIMESTAMP（1970-2038）时区相关 使用4Byte的存储空间常用函数 123456# 把UNIX时间戳转成TIMESTAMPUNIX_TIMESTAMP() /FROM_UNIXTIME() DATE()DATE_ADDDATE_SUBDATE_FORMAT 枚举类型与集合类型此类型仅作列出， 不作实际使用， 一般这些类型可以用TINYINT 等值来代替，不建议用这些类型 默认值NULLNULL 是最常见的默认值，一切默认值都可以设置为NULL， 但不是所有默认值都要设置为NULL， 要根据情况具体设定 Empty String空字符串，是很多char, varchar, text 的默认值的选项，如果你的应用程序没有处理NULL值的时候可以把默认值设置为这个避免空指针异常， 但一般情况下不希望MySQL给你做这类的检测。 CURRENT_TIMESTAMP当前时间， 一般用于TIMESTAMP的默认值，它在使用跟理论上均优于创建TRIGGER来设置默认值 索引与外键索引与外键是建立Schema 永远不能避开的话题，你可以避开外键，但你可能会想到它，但你不能避开索引。这里不会对索引进行详细的表述，具体的描述在后面小节中会详细道来。在这里你只需要记住一点，建立表的时候，一定要根据查询建立索引，最好不要建立外键，除非非常必要。 Schema设计误区太多的列太多的列需要更大的维护成本， 对与SELECT * 的效率来说也更低 太多的JOINJOIN过多，会导致查询复杂，结果更难判断，查询效率变低 泛滥的外键与索引索引与外键并不越多越好， 每个外键与索引都需要额外的维护成本 随意性建表在很小的数据量的情况下， 通常存在一个汇总表，或者设置表里面比较合适] 范式与反范式 设计模式 优点 缺点 范式 数据量小，单实体表查询效率高 需要做很多的JOIN 反范式 反范式把冗余信息都放在一张表里面，可以很好的避免JOIN 数据量的增大，导致查询的速度，稍受影响； 更新须保证冗余信息的更新 两者要结合实际情况，可以混合使用，但一般不推荐过多的JOIN，SQL语句JOIN不能超过两张表。 Schema 设计原则简单就好，更小的通常更好越简单的Schema所需要的存储空间就越小，对于MySQL而言需要的存储空间， IO， 运算等资源就越少，MySQL支持的QPS就越高 尽量避免大量NULL值的列如果一个列有80%甚至90%以上的行都是NULL，那么这个属性值，很可能可以采取其他方法来实现，比如新建一张表，用ID去关联 不使用外键外键在MySQL维护与使用中始终是一个坑，大家尽量在建表的时候避免使用外键。 合理建立索引索引要根据查询而建， 并不是越多越好，索引多有可能会引起MySQL不能正确使用索引，MySQL维护索引的空间变大等等 写好表注释与写代码一样一个良好的习惯就是所有MySQL的字段，及索引，表等都要有详尽的注释，注释不会占用多余空间，最好的注释是大家都能看懂的注释。","link":"/2016/03/13/storage/mysql/schema/"},{"title":"redis 使用规范","text":"key/value规范 所有的key 应该有合理的业务前缀 key 不应该包含特殊字符，应尽量使用常见的字母，数字， 下划线等组合 key 的大小应该保证在16kb 之内 value 的大小应该保证在1M之内， 拒绝bigkey(防止网卡流量、慢查询) string类型控制在10KB以内，hash、list、set、zset元素个数不要超过5000。 控制key的生命周期，redis不是垃圾桶, 能过期的key一定要设置过期时间 数据类型使用注意 hash 不推荐使用 hkeys 命令除非hash表的尺寸非常小， 否则推荐使用scan set/zset/list 需要遍历整个列表也尽量使用scan与 lindex等命令 非字符串的bigkey，不要使用del删除，使用hscan、sscan、zscan方式渐进式删除，同时要注意防止bigkey过期时间自动删除问题(例如一个200万的zset设置1小时过期，会触发del操作，造成阻塞，而且该操作不会不出现在慢查询中(latency可查)) 选择适合的数据类型, 注意节省内存和性能之间的平衡 命令使用 O(N)命令关注N的数量 1例如hgetall、lrange、smembers、zrange、sinter等并非不能使用，但是需要明确N的值。有遍历的需求可以使用hscan、sscan、zscan代替。 禁用命令 1禁止线上使用keys、flushall、flushdb等，通过redis的rename机制禁掉命令，或者使用scan的方式渐进式处理。 不推荐使用select, 新业务禁止使用 select 用批量操作提高效率, 但要注意控制一次批量操作的元素个数(例如500以内，实际也和元素字节数有关)。 Redis事务功能较弱，不建议过多使用 必要情况下使用monitor命令时，要注意不要长时间使用 客户端使用 避免多个应用使用一个Redis实例, 不相干的业务拆分，公共数据做服务化 使用带有连接池的数据库，可以有效控制连接，同时提高效率 高并发下建议客户端添加熔断功能(例如netflix hystrix) 设置合理的密码 根据自身业务类型，选好maxmemory-policy(最大内存淘汰策略)，设置好过期时间 123456volatile-lru: 即超过最大内存后，在过期键中使用lru算法进行key的剔除，保证不过期数据不被删除，但是可能会出现OOM问题allkeys-lru：根据LRU算法删除键，不管数据有没有设置超时属性，直到腾出足够空间为止。allkeys-random：随机删除所有键，直到腾出足够空间为止。volatile-random:随机删除过期键，直到腾出足够空间为止。volatile-ttl：根据键值对象的ttl属性，删除最近将要过期数据。如果没有，回退到noeviction策略。noeviction：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息&quot;(error) OOM command not allowed when used memory&quot;，此时Redis只响应读操作。","link":"/2017/12/14/storage/redis/redis_standards/"},{"title":"redis 内存分析工具 &#96;rma4go&#96;","text":"redis 简介redis是一个很有名的内存型数据库，这里不做详细介绍。而rma4go (redis memory analyzer for golang) 是一个redis的内存分析工具，这个工具的主要作用是针对运行时期的redis进行内存的分析，统计redis中key的分布情况， 各种数据类型的使用情况，key的size，大key的数量及分布, key的过期状况分布等一些有助于定位redis使用问题的工具，希望这能够给应用开发者提供便利排查生产中所遇到的实际问题。 rma4go的应用场景redis是目前很流行的一个内存型数据库，很多企业都在使用。 但由于业界并没有很多对于redis使用上的规范，或者是有一些规范并没有被很好的遵循， 存在很多redis使用上的问题，我这边就列举一些例子： redis 存用满了, 不知道key的分布情况，不知道来源于那个应用 redis 被block了，不知道什么原因导致的block，是哪个应用里的什么key的操作导致的 想迁移redis数据，或者调整一些设置，但不知道要不要对redis里的数据进行保留，以及不知道什么业务在使用等 redis的key的过期情况不明朗， 不知道哪些东西可以删除或者调整其实上面的一些问题是我随便列举出来的一些，并不是所有的存在的问题，相信也有很多其他场景同样会用到这样的一个redis内存分析工具rma4go rma4go的具体功能数据维度对于key的分析我们这个工具会提供如下几个维度的数据： key的数量分布维度 key的过期分布维度 key的类型分布维度 key对应的的数据的大小分布维度 key的前缀分布维度 慢key与大key的维度 当然以后如果发现有更好的纬度也会添加进去，目前先以这几个纬度为主 数据类型设计12345678910111213141516171819202122232425262728type RedisStat struct { All KeyStat `json:\"all\"` String KeyStat `json:\"string\"` Hash KeyStat `json:\"hash\"` Set KeyStat `json:\"set\"` List KeyStat `json:\"list\"` ZSet KeyStat `json:\"zset\"` Other KeyStat `json:\"other\"` BigKeys KeyStat `json:\"bigKeys\"`}// distributions of keys of all prefixestype Distribution struct { KeyPattern string `json:\"pattern\"` Metrics}// basic metrics of a group of keytype Metrics struct { KeyCount int64 `json:\"keyCount\"` KeySize int64 `json:\"keySize\"` DataSize int64 `json:\"dataSize\"` KeyNeverExpire int64 `json:\"neverExpire\"` ExpireInHour int64 `json:\"expireInHour\"` // &gt;= 0h &lt; 1h ExpireInDay int64 `json:\"expireInDay\"` // &gt;= 1h &lt; 24h ExpireInWeek int64 `json:\"expireInWeek\"` // &gt;= 1d &lt; 7d ExpireOutWeek int64 `json:\"expireOutWeek\"` // &gt;= 7d} 实现细节key元信息1234567type KeyMeta struct { Key string KeySize int64 DataSize int64 Ttl int64 Type string} 众所周知， redis里的所有的数据基本都是由key的， 也是根据key进行操作的，那么对redis里的key进行分析我们必须要记录下来这个key的信息才可以做到， 我们能记录的信息正如以上结构中的一样， key本身， key的大小， 数据的大小， 过期时间以及key的类型。这些信息是我们对key进行分析的一个基础信息，都可以通过一些简单的redis命令就可以取到。 遍历redis所有key要对一个redis进行完整的key分析， 我们就需要有办法能够访问到所有key的源信息， 所幸redis提供了 scan这么一种方式可以比较轻量的遍历所有的key，访问到相应的key的元信息。这样对于redis而言， 进行在线key分析的时候造成的压力也不会非常大，当然key分析不能再QPS高峰期进行， 需要在redis资源余量允许的情况下进行分析。 另外由于redis本身的一个内存清理机制，有25%的过期占用可以在分析key的时候被清理掉， 因此这个分析工具同时兼具了清理一部分内存的作用， 如果redis里面存在过期的而且存在于内存里面的key的话。 对记录的信息进行分析与汇总有了遍历所有key的方法， 又有了元数据， 剩下的事情就是把这些数据进行聚合汇总， 这个主要是一个算法上的工作，最难的部分要数这个key聚合的部分了， 这里面有很多取舍， 由于作者我本人不是专攻算法的， 而且没有找到合适的库， 因此只能动手自己想了一种方式。 基本的思路是： 压缩的算法 对于每个新的key的元信息， 添加到老的key分析对象里去 对这个key从后往前缩短， 去除尾部，看是否已经包含这个key的统计信息，如果包含， 则把key的信息累加上去， 如果不包含则创建一个新的纪录。 当记录的个数添加到一定数量的时候， 对对象的个数进行一次压缩 压缩的算法也是从字符串的末尾往字符串首部进行压缩 当压缩不能增加这个pattern 的key的个数的时候使用原来的key（压缩前的key） 当压缩可以增加这个pattern的key的个数的时候，进行key的合并，把pattern设置成压缩后的pattern 当记录的条数超过指定的条数就循环往复，直到压缩到小于指定的条数为止 如果对于key的最小长度（就算再压缩也要保留一两位）有要求， 有一些压缩到字符串的最小长度的参数可以进行调整与设置， 进行一定的取舍。 直到scan完毕 代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687const ( defaultSize = 128 compactNum = 30 maxLeftNum = 150 minKeyLenLower = 2 minKeyLen = 5)func (stat *KeyStat) compact() { distMap := stat.Distribution tmpMap := make(map[string][]string, defaultSize) shrinkTo := compactNum for k := range distMap { compactedKey := k if orgks, ok := tmpMap[compactedKey]; ok { orgks = append(orgks, k) tmpMap[compactedKey] = orgks } else { ks := make([]string, 0, defaultSize) ks = append(ks, k) tmpMap[compactedKey] = ks } } shrinkTo-- for (len(tmpMap) &gt; compactNum &amp;&amp; shrinkTo &gt;= minKeyLen) || (len(tmpMap) &gt; maxLeftNum &amp;&amp; shrinkTo &gt;= minKeyLenLower) { tnMap := make(map[string][]string, defaultSize) for k := range tmpMap { // shrink if len(k) &gt; shrinkTo { compactedKey := k[0:shrinkTo] if oik, ok := tnMap[compactedKey]; ok { oik = append(oik, tmpMap[k]...) tnMap[compactedKey] = oik } else { ks := make([]string, 0, defaultSize) ks = append(ks, tmpMap[k]...) tnMap[compactedKey] = ks } } else { tnMap[k] = tmpMap[k] } } // 如果此次shrink 没有使得这个集合的元素数量增加， 就使用原来的key for k := range tmpMap { if len(k) &gt; shrinkTo { ck := k[0:shrinkTo] if len(tnMap[ck]) == len(tmpMap[k]) &amp;&amp; len(tnMap[ck]) &gt; 1 { x := make([]string, 0, defaultSize) tnMap[k] = append(x, tnMap[ck]...) delete(tnMap, ck) } } } tmpMap = tnMap shrinkTo -- } dists := make(map[string]Distribution, defaultSize) for k, v := range tmpMap { if len(v) &gt; 1 { var nd Distribution for _, dk := range v { d := distMap[dk] nd.KeyPattern = k + \"*\" nd.KeyCount += d.KeyCount nd.KeySize += d.KeySize nd.DataSize += d.DataSize nd.ExpireInHour += d.ExpireInHour nd.ExpireInWeek += d.ExpireInWeek nd.ExpireInDay += d.ExpireInDay nd.ExpireOutWeek += d.ExpireOutWeek nd.KeyNeverExpire += d.KeyNeverExpire } dists[k] = nd } else { for _, dk := range v { nd := distMap[dk] nd.KeyPattern = dk + \"*\" dists[dk] = nd } } } stat.Distribution = dists} 在线key分析的github项目rma4go这是一个我已经写好的项目， 它使用起来非常简单 构建方法 构建之前请确保golang sdk 已经安装， 并且版本 &gt;=1.11.0 请确保已经具备翻墙的环境， 因为它要下载一些依赖，可能来自墙外翻墙方法如下 123456// linux/osxexport http_proxy=somehost:portexport https_proxy=somehost:port// windowsset http_proxy=somehost:portset https_proxy=somehost:port 构建 123git clone git@github.com:winjeg/rma4go.gitcd rma4gogo build . 使用方法用法如下：rma4go -h 1234567891011121314rma4go usage:rma4go -r some_host -p 6379 -a password -d 0====================================================== -H string address of a redis (default &quot;localhost&quot;) -a string password/auth of the redis -d int db of the redis to analyze -h help content -p int port of the redis (default 6379) -r string address of a redis (default &quot;localhost&quot;) 示例输出12345678910111213141516171819202122232425262728293031323334353637383940all keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |string keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |list keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |hash keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |set keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |zset keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |other keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |big keys statistics| PATTERN | KEY NUM | KEY SIZE | DATA SIZE | EXPIRE IN HOUR | EXPIRE IN DAY | EXPIRE IN WEEK | EXPIRE OUT WEEK | NEVER EXPIRE ||---------|---------|----------|-----------|----------------|---------------|----------------|-----------------|--------------|| total | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | rendered by markdowntotal count 4004 all keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE TOP_TEN_NEW_XXXXXXXX* 1 20 1529 0 0 0 0 1 XXXXXXXXXXXXXX_STATISTICS_MIGRATION_LIST* 1 40 7692832 0 0 0 0 1 time-root:* 23 272 299 0 0 0 0 23 DS_AXXXXXXXX_CORRECT* 2 45 46 0 0 0 0 2 time-2* 761 7528 9893 0 0 0 0 761 time-level:* 537 8461 6981 0 0 0 0 537 time-9* 102 901 1326 0 0 0 0 102 time-7* 153 1372 1989 0 0 0 0 153 DS_MAGIC_SUCC_2017-06-22* 1 24 415 0 0 0 0 1 tersssss* 5 124 0 0 0 0 0 5 appoint_abcdefg_msgid* 1 21 0 0 0 0 0 1 BUSSINESSXXXXXXX_STATISTICS_NEED_CALC_RECENT* 1 44 1 0 0 0 0 1 switch_abcd_abcde* 3 69 3 0 0 0 0 3 abcdeferCounter_201* 3 78 0 0 0 0 0 3 diy1234567flag* 1 14 1 0 0 0 0 1 DS_PRXXBCD_LIST* 1 15 17208 0 0 0 0 1 time-4* 133 1194 1729 0 0 0 0 133 datastatistics_switch_version0* 1 30 1 0 0 0 0 1 register_count_2_201* 592 15984 640 0 0 0 0 592 canVisitNewabcdef1234PageLevels* 1 31 0 0 0 0 0 1 YOUR_WEEK_VITALITY_INFO* 1 23 75782 0 0 0 0 1 time-8* 101 894 1313 0 0 0 0 101 EXPERTS_APPOINT_INFO_MAP* 1 24 0 0 0 0 0 1 time-3* 130 1215 1690 0 0 0 0 130 time-1* 943 9456 12259 0 0 0 0 943 time-64* 87 781 1131 0 0 0 0 87 time-5* 168 1516 2184 0 0 0 0 168 total 4004 53422 7832490 0 0 0 0 4004 string keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE BUSSINESSXXXXXXX_STATISTICS_NEED_CALC_RECENT* 1 44 1 0 0 0 0 1 time-5* 130 1174 1690 0 0 0 0 130 datastatistics_switch_version0* 1 30 1 0 0 0 0 1 time-7* 39 348 507 0 0 0 0 39 time-level:* 567 8939 7371 0 0 0 0 567 diy1234567flag* 1 14 1 0 0 0 0 1 switch_abcd_abcde* 3 69 3 0 0 0 0 3 time-2* 598 5918 7774 0 0 0 0 598 time-6* 125 1118 1625 0 0 0 0 125 time-4* 136 1225 1768 0 0 0 0 136 time-8* 72 636 936 0 0 0 0 72 time-1* 1176 11814 15288 0 0 0 0 1176 time-9* 100 880 1300 0 0 0 0 100 time-root:* 23 272 299 0 0 0 0 23 register_count_2_201* 592 15984 640 0 0 0 0 592 DS_AXXXXXXXX_CORRECT* 1 20 20 0 0 0 0 1 TOP_TEN_NEW_tersssss* 1 20 1529 0 0 0 0 1 time-3* 202 1925 2626 0 0 0 0 202 total 3989 53042 46253 0 0 0 0 3989 list keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE XXXXXXXXXXXXXX_STATISTICS_MIGRATION_LIST* 1 40 7692832 0 0 0 0 1 DS_MAGIC_SUCC_2017-06-22* 1 24 415 0 0 0 0 1 DS_PRXXBCD_LIST* 1 15 17208 0 0 0 0 1 total 3 79 7710455 0 0 0 0 3 hash keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE tersssss_action_prepage_new* 1 27 0 0 0 0 0 1 YOUR_WEEK_VITALITY_INFO* 1 23 75782 0 0 0 0 1 EXPERTS_APPOINT_INFO_MAP* 1 24 0 0 0 0 0 1 abcdeferCounter_2017-06-11* 1 26 0 0 0 0 0 1 tersssssHardTaskCounter* 1 23 0 0 0 0 0 1 abcdeferCounter_2018-04-27* 1 26 0 0 0 0 0 1 abcdeferCounter_2017-09-01* 1 26 0 0 0 0 0 1 tersssssEasyTaskCounter* 1 23 0 0 0 0 0 1 total 8 198 75782 0 0 0 0 8 set keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE tersssss_bind_phone_phone* 1 25 0 0 0 0 0 1 appoint_abcdefg_msgid* 1 21 0 0 0 0 0 1 canVisitNewabcdef1234PageLevels* 1 31 0 0 0 0 0 1 tersssss_bind_phone_userid* 1 26 0 0 0 0 0 1 total 4 103 0 0 0 0 0 4 zset keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE total 0 0 0 0 0 0 0 0 other keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE total 0 0 0 0 0 0 0 0 big keys statistics PATTERN KEY NUM KEY SIZE DATA SIZE EXPIRE IN HOUR EXPIRE IN DAY EXPIRE IN WEEK EXPIRE OUT WEEK NEVER EXPIRE XXXXXXXXXXXXXX_STATISTICS_MIGRATION_LIST* 1 40 7692832 0 0 0 0 1 total 1 40 7692832 0 0 0 0 1 作为依赖使用获取方法如下： 1go get github.com/winjeg/rma4go 使用方法如下： 123456789101112131415func testFunc() { h := \"localhost\" a := \"\" p := 6379 cli := client.BuildRedisClient(client.ConnInfo{ Host: h, Auth: a, Port: p, }, cmder.GetDb()) stat := analyzer.ScanAllKeys(cli) // print in command line stat.Print() // the object is ready to use} github 维护(主要阵地) 欢迎其他开发者加入 欢迎提issue 反馈问题 欢迎任何有意义的建议 另外欢迎star，不建议fork，建议直接提交PR ; )","link":"/2018/11/13/storage/redis/mem_analysis/"},{"title":"Git提交规范","text":"## Git 提交规范 Git是使用最广的代码管理工具，版本控制工具，也是大家最熟知的，如果不了解Git是什么以及怎么使用的，请参考临近的一些文档。本文档主要讨论Git提交代码的一些推荐规范。 一、分支选择一个分支一般对应一个比较明确的版本， 大家需要在这个分支上开发，继承各种功能点， 是许多Commit的集合。为了能够更好的适应Git的特点与企业级的分支管理策略， 分支的命名就显得尤为重要。 比如Gitlab可以根据分支名称特点进行设置权限级别， 有些CICD工具根据分支工具进行约束部署行为， 所以合理的分支名称，是一个合格的代码开发者的基本素质要求。推荐的分支名称 feature 功能点分支 release/production 发布分支 test/benchmark 测试分支 比较典型的用法如：feature/weixin_register 作为微信注册的一个功能点， feature/email_register 作为邮件注册的一个功能点。而上线的时候可以用 release/user_register 作为用户注册的功能，集成之前的微信注册与邮件注册的功能点。 而在分支保护的时候，我们也可以轻松的将 release/* 设置为保护分支，仅仅允许固定的工具或者固定的人去提交和merge, 这样就能很好的控制线上在运行的代码的质量。 二、标签选择git tag 也是一个比较重要的功能，往往用作一个比较长周期的，例如中间件的迭代，如java代码， 可以用 git tag 与maven版本号保持一致， 可以很方便的回溯代码。这里推荐的tag命名方式为：vx.x.x 其中 v 代表版本的意思， 第一个 x 代表大版本号， 第二个代表小版本， 第三个代表小的修订版本, 这种命名方式，对于一些语言，如golang，就比较友好，golang是根据 tag来读取软件的版本的。 三、Commit 规范 一个Commit只做一件事情这是为了可以在出现问题的情况下可以随时对不同commit进行操作，且同时最大程度的降低对其他地方造成的影响。同时这也是非常知名的一些仓库的一些普遍做法，如 linux kernel的维护方式就是这样的。 commit （标签）commit 标签是为了更好的识别与分类commit的内容， 更好的组织commit本身. 常见的commit标签如下： bugfix 如 bugfix:fix user name not long enough problem. doc 如 doc: update user related api doc improvement 如 improvement: change the implementation of the algrithm reduce exec time to 1/10 hotfix 用于紧急修复 task 任务 feature 功能特点","link":"/2020/09/14/apps/git_recommend/"},{"title":"输入法之双拼","text":"双拼输入法是什么在了解双拼输入法的时候我们先了解下一拼音输入法，目前所有的拼音输入法的基础都是拼音， 根据一个字的读音来确定这个字怎么在键盘上输入。如: 你好 为 “nihao”， 这就是拼音输入法。 顾名思义， 双拼输入法就是打一个字（拼音）的时候最多最少都是需要至少按下两次键盘， 他是拼音输入法的一种。每种双拼输入法，都是使用一定的码表的， 如果您使用双拼输入法, 必然会涉及到码表的选择，如果你少输入 双拼 这个词语， 那么按照自然码码表， 你输入的内容为：udpn， 如果使用全拼则输入的内容为： shuangpin 相较于全拼，双拼输入法只需要键入4次，而全拼则需要键入9次， 这就是双拼的明显优势。 双拼相较于全拼的优劣势凡事都有两面性，虽然双拼较晚被发现，并且使用，但是双拼本身并非毫无缺点，双拼本身会具备所有拼音输入法的缺点，就是对于重音字而言，都是可能需要经过翻页才能找到字本身的，而且一般都有这种候选项这种概念，要用户自己去选择是第几个候选项的字。相对而言五笔编码可以唯一确定一个字，就不需要选择候选项这一步了。 由于双拼出现时机较晚， 因此，它的出现肯定是为了解决全拼中存在的一些既定问题而出现的，下面我们来详细谈一下双拼与全拼的优劣势。 相较于全拼的优势 双拼整体而言，每个单独的拼音只需要最多键入两次，对于平均情况而言可以减少至少 50%的键入， 这样可以大大提高打字速度。 双拼输入法打字节奏明朗，每个字都是单独的两次按键，比较顺手节奏感好。 相较于大部分人不会双拼而言，学会双拼，别人可以减少对你隐私的窥探，常用词频率不会被不用双拼的人监测到。 相较于全拼的劣势 全拼输入法基本上是所有人入门使用手机或者电脑都使用的输入法, 学习成本极低，而双拼输入法则有一定的学习成本 全拼输入法有超级简拼，而双拼输入法则没有，但超级简拼应用的场景似乎有限。 1总体而言， 双拼是优于全拼的，如果您对打字速度有追求，又不想投入太多精力去学习其他输入法的话，双拼无疑是您最好的选择。 双拼的规则双拼的拆拼音规则双拼双拼， 自然每个拼音只需要按两次键，双拼输入法的主要规则是声母+韵母，即声母按一次键，韵母按一次键盘。 这跟我们在小学的时候学习的拼音规则是一模一样的。所以对于一般人而言，记忆这个规则应该是没有代价的。但是这不意味着在享受双拼输入的同时，是没有代价的。我们知道键盘上的键几乎覆盖了所有的声母，维度三个声母是没有覆盖的，即 zh, ch, sh。这三个生母在键盘上都对应着双键， 如果再加上韵母至少会按下三次键， 这显然是不符合双拼之道的。所以，这三个字母肯定都有相应的对应关系， 只需要一次按键即可按出三个声母中的任意一个。韵母与声母一样在字母数超过1的时候， 必然需要把这个整体的韵母对应到其中一个键上去。 比如 装 这个词语，它的全拼是 zhuang, 其中装的声母是zh 韵母是uang, 这显然是都超过一个字母的， 所以 zh 一定在某个键上， uang, 也一定在某个键上，这些生母与韵母对应的键的对应关系的表， 我们称之为码表。通常而言，流行的有十多种码表， 你可以选择其中的一种去使用。当然至少你遵守这个规则，很多输入法都可以支持你自定义自己的码表。 常见的双拼的码表对于双拼而言，并没有组织来定义一个统一的码表， 这也是为什么双拼码表方案有很多的原因，选择码表的时候， 大家可以根据自己的喜好以及记忆的难易程度，来确定自己选择哪个码表，码表之间并没有优劣势而言， 下面是一些常见的码表： 1. 小鹤双拼 2. 搜狗双拼 3. 微软双拼 4. 自然码 总结学习双拼一定要下决心，自从决定的那一刻起，自己的手机、电脑上的输入法一定要调整成双拼的方式， 这样，您最快可以在半小时内掌握，最慢也就三天就掌握了。如果您不痛下决心， 那么您是不可能轻松就学会的， 在信息时代这是一门非常有用的技能， 尤其你身为一个中国人，这个就更有用了。 对于大部分人而言，全拼输入肯定是能满足需求的， 如果你觉得自己还有一定的输入速度的追求， 还想在键入上偷个懒， 而有不想付出太多的精力去学习输入法的情况下。双拼是您最好的选择。","link":"/2020/07/04/others/shuangpin/"},{"title":"Golang的代码风格","text":"一些废话（Some useless words）想必能看到我博客的人，已经对golang有一定的了解了 gofmt 一统天下gofmt 是目前golang里面用的最多的用来格式化代码风格的一个命令行工具， 很多知名项目都用它来保证自己的最基本的代码风格与官方和社区推荐的风格一致。使用如下命令就可以轻松格式化一个目录下的所有文件： 1gofmt -w . gofmt 可以解决的问题 gofmt 不能解决的问题 天下之外其实标准gofmt有很多代码风格没有规定的地方比如以下几种场景： 常量命名风格 变量命名风格 函数命名风格 对象字段使用风格 值传递，还是指针传递 据库对象： 可空字段，使用指针，用来表示NULL 对于不可空字段不使用指针类型， 用来表示这个字段一定有值 注释风格golang 与某Java不同， 不喜欢多行注释，纵观Golang SDK以及 一些非常文明的项目，大部分注释均为单行注释， 虽然golang 支持以下两种注释类型。 1234567// 单行注释var code = 0/*多行注释 */const someVeryNastyThing = -1 特殊指令在golang注释里面可以写一些特殊指令， 这个时候编译器就会处理这些指令，而不是仅仅当做注释， 这在很多场景下非常有用。","link":"/2020/05/30/langs/golang/code_style/"},{"title":"JWT 基本原理笔记","text":"jwt 简介： jwt 是json web token 的缩写， 主要用来做用户授权或者session登录的事情。 123public void test() {}","link":"/2020/10/23/base/jwt/"}],"tags":[{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"multi-media","slug":"multi-media","link":"/tags/multi-media/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"tidb","slug":"tidb","link":"/tags/tidb/"},{"name":"notes","slug":"notes","link":"/tags/notes/"},{"name":"database","slug":"database","link":"/tags/database/"},{"name":"devops","slug":"devops","link":"/tags/devops/"},{"name":"cicd","slug":"cicd","link":"/tags/cicd/"},{"name":"ftp","slug":"ftp","link":"/tags/ftp/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"container","slug":"container","link":"/tags/container/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"bigtable","slug":"bigtable","link":"/tags/bigtable/"},{"name":"ldap","slug":"ldap","link":"/tags/ldap/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"mongodb","slug":"mongodb","link":"/tags/mongodb/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"biz","slug":"biz","link":"/tags/biz/"},{"name":"openapi","slug":"openapi","link":"/tags/openapi/"},{"name":"rocksdb","slug":"rocksdb","link":"/tags/rocksdb/"},{"name":"middleware","slug":"middleware","link":"/tags/middleware/"},{"name":"tidis","slug":"tidis","link":"/tags/tidis/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"lang","slug":"lang","link":"/tags/lang/"},{"name":"npm","slug":"npm","link":"/tags/npm/"},{"name":"project","slug":"project","link":"/tags/project/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"gramma","slug":"gramma","link":"/tags/gramma/"},{"name":"react","slug":"react","link":"/tags/react/"},{"name":"antd","slug":"antd","link":"/tags/antd/"},{"name":"yaml","slug":"yaml","link":"/tags/yaml/"},{"name":"etcd","slug":"etcd","link":"/tags/etcd/"},{"name":"索引","slug":"索引","link":"/tags/%E7%B4%A2%E5%BC%95/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"standards","slug":"standards","link":"/tags/standards/"},{"name":"input","slug":"input","link":"/tags/input/"}],"categories":[{"name":"other","slug":"other","link":"/categories/other/"},{"name":"os","slug":"os","link":"/categories/os/"},{"name":"tools","slug":"tools","link":"/categories/tools/"},{"name":"storage","slug":"storage","link":"/categories/storage/"},{"name":"linux","slug":"os/linux","link":"/categories/os/linux/"},{"name":"devops","slug":"devops","link":"/categories/devops/"},{"name":"application","slug":"application","link":"/categories/application/"},{"name":"container","slug":"container","link":"/categories/container/"},{"name":"database","slug":"storage/database","link":"/categories/storage/database/"},{"name":"docker","slug":"container/docker","link":"/categories/container/docker/"},{"name":"bigtable","slug":"storage/bigtable","link":"/categories/storage/bigtable/"},{"name":"biz","slug":"biz","link":"/categories/biz/"},{"name":"k8s","slug":"container/k8s","link":"/categories/container/k8s/"},{"name":"mongodb","slug":"storage/mongodb","link":"/categories/storage/mongodb/"},{"name":"rocksdb","slug":"storage/rocksdb","link":"/categories/storage/rocksdb/"},{"name":"middleware","slug":"storage/middleware","link":"/categories/storage/middleware/"},{"name":"redis","slug":"storage/redis","link":"/categories/storage/redis/"},{"name":"lang","slug":"lang","link":"/categories/lang/"},{"name":"gramma","slug":"gramma","link":"/categories/gramma/"},{"name":"etcd","slug":"storage/etcd","link":"/categories/storage/etcd/"}]}